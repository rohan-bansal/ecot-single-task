2025-06-07 22:22:59.306561: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.306560: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.306564: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.306617: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.306618: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.306620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.307449: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.307447: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.307451: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.308495: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.308550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.309373: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.310703: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.310725: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.311214: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.311661: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:22:59.311661: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:22:59.311683: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:22:59.313662: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:22:59.314646: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:22:59.657261: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.657327: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.658046: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.662141: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:22:59.695016: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.695046: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.695750: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.699674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:22:59.721885: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.721927: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.722910: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.726959: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:23:00.442460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-07 22:23:00.442460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-07 22:23:00.442474: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-07 22:23:00.442499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-07 22:23:00.442499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-07 22:23:00.451037: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-07 22:23:00.454659: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-07 22:23:00.507825: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.73s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.69s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.13s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  8.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.52s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.71s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00,  9.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.29s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00,  9.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.10s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  8.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.80s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  8.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  8.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  6.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.74s/it]
2025-06-07 22:25:43.912706: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:44.354617: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:45.094321: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:45.526036: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:45.629774: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:46.094275: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:46.164846: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:46.631010: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:46.746054: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:47.222667: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:47.474249: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:47.571765: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:47.688662: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:47.965653: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:48.031083: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:48.156087: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:59.127487: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:26:00.445532: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:26:01.136965: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:26:02.101188: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:26:02.199092: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:26:03.137994: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:26:03.304606: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:26:03.667122: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
wandb: Currently logged in as: solace to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /srv/rl2-lab/flash7/rbansal66/embodied-CoT/runs/prism-dinosiglip-224px+mx-libero-90+n1+b16+x7/wandb/run-20250607_222610-ez8jqye1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prism-dinosiglip-224px+mx-libero-90+n1+b16+x7
wandb: ⭐️ View project at https://wandb.ai/solace/ecot_reproduce_libero
wandb: 🚀 View run at https://wandb.ai/solace/ecot_reproduce_libero/runs/ez8jqye1
=>> [Epoch 000] Global Step 000000 =>> LR :: 0.000000:   0%|          | 0/200 [00:00<?, ?it/s]=>> [Epoch 000] Global Step 000000 =>> LR :: 0.000000:   0%|          | 1/200 [00:33<1:52:26, 33.90s/it]=>> [Epoch 000] Global Step 000001 =>> LR :: 0.000020 - Loss :: 2.0336:   0%|          | 1/200 [00:33<1:52:26, 33.90s/it]=>> [Epoch 000] Global Step 000001 =>> LR :: 0.000020 - Loss :: 2.0336:   1%|          | 2/200 [01:01<1:38:43, 29.92s/it]=>> [Epoch 000] Global Step 000002 =>> LR :: 0.000020 - Loss :: 1.2111:   1%|          | 2/200 [01:01<1:38:43, 29.92s/it]=>> [Epoch 000] Global Step 000002 =>> LR :: 0.000020 - Loss :: 1.2111:   2%|▏         | 3/200 [01:29<1:35:41, 29.14s/it]=>> [Epoch 000] Global Step 000003 =>> LR :: 0.000020 - Loss :: 0.6832:   2%|▏         | 3/200 [01:29<1:35:41, 29.14s/it]=>> [Epoch 000] Global Step 000003 =>> LR :: 0.000020 - Loss :: 0.6832:   2%|▏         | 4/200 [01:56<1:33:08, 28.51s/it]=>> [Epoch 000] Global Step 000004 =>> LR :: 0.000020 - Loss :: 0.5582:   2%|▏         | 4/200 [01:56<1:33:08, 28.51s/it]=>> [Epoch 000] Global Step 000004 =>> LR :: 0.000020 - Loss :: 0.5582:   2%|▎         | 5/200 [02:24<1:32:07, 28.34s/it]=>> [Epoch 000] Global Step 000005 =>> LR :: 0.000020 - Loss :: 0.4810:   2%|▎         | 5/200 [02:24<1:32:07, 28.34s/it]=>> [Epoch 000] Global Step 000005 =>> LR :: 0.000020 - Loss :: 0.4810:   3%|▎         | 6/200 [02:52<1:31:01, 28.15s/it]=>> [Epoch 000] Global Step 000006 =>> LR :: 0.000020 - Loss :: 0.4492:   3%|▎         | 6/200 [02:52<1:31:01, 28.15s/it]=>> [Epoch 000] Global Step 000006 =>> LR :: 0.000020 - Loss :: 0.4492:   4%|▎         | 7/200 [03:20<1:29:55, 27.96s/it]=>> [Epoch 000] Global Step 000007 =>> LR :: 0.000020 - Loss :: 0.4772:   4%|▎         | 7/200 [03:20<1:29:55, 27.96s/it]=>> [Epoch 000] Global Step 000007 =>> LR :: 0.000020 - Loss :: 0.4772:   4%|▍         | 8/200 [03:47<1:29:11, 27.87s/it]=>> [Epoch 000] Global Step 000008 =>> LR :: 0.000020 - Loss :: 0.4691:   4%|▍         | 8/200 [03:47<1:29:11, 27.87s/it]=>> [Epoch 000] Global Step 000008 =>> LR :: 0.000020 - Loss :: 0.4691:   4%|▍         | 9/200 [04:15<1:28:51, 27.91s/it]=>> [Epoch 000] Global Step 000009 =>> LR :: 0.000020 - Loss :: 0.3981:   4%|▍         | 9/200 [04:15<1:28:51, 27.91s/it]=>> [Epoch 000] Global Step 000009 =>> LR :: 0.000020 - Loss :: 0.3981:   5%|▌         | 10/200 [04:43<1:28:00, 27.79s/it]=>> [Epoch 000] Global Step 000010 =>> LR :: 0.000020 - Loss :: 0.3722:   5%|▌         | 10/200 [04:43<1:28:00, 27.79s/it]=>> [Epoch 000] Global Step 000010 =>> LR :: 0.000020 - Loss :: 0.3722:   6%|▌         | 11/200 [05:10<1:27:04, 27.64s/it]=>> [Epoch 000] Global Step 000011 =>> LR :: 0.000020 - Loss :: 0.3420:   6%|▌         | 11/200 [05:10<1:27:04, 27.64s/it]=>> [Epoch 000] Global Step 000011 =>> LR :: 0.000020 - Loss :: 0.3420:   6%|▌         | 12/200 [05:38<1:26:38, 27.65s/it]=>> [Epoch 000] Global Step 000012 =>> LR :: 0.000020 - Loss :: 0.3449:   6%|▌         | 12/200 [05:38<1:26:38, 27.65s/it]=>> [Epoch 000] Global Step 000012 =>> LR :: 0.000020 - Loss :: 0.3449:   6%|▋         | 13/200 [06:06<1:26:15, 27.68s/it]=>> [Epoch 000] Global Step 000013 =>> LR :: 0.000020 - Loss :: 0.3300:   6%|▋         | 13/200 [06:06<1:26:15, 27.68s/it]=>> [Epoch 000] Global Step 000013 =>> LR :: 0.000020 - Loss :: 0.3300:   7%|▋         | 14/200 [06:34<1:26:00, 27.74s/it]=>> [Epoch 000] Global Step 000014 =>> LR :: 0.000020 - Loss :: 0.3248:   7%|▋         | 14/200 [06:34<1:26:00, 27.74s/it]=>> [Epoch 000] Global Step 000014 =>> LR :: 0.000020 - Loss :: 0.3248:   8%|▊         | 15/200 [07:01<1:25:19, 27.67s/it]=>> [Epoch 000] Global Step 000015 =>> LR :: 0.000020 - Loss :: 0.3065:   8%|▊         | 15/200 [07:01<1:25:19, 27.67s/it]=>> [Epoch 000] Global Step 000015 =>> LR :: 0.000020 - Loss :: 0.3065:   8%|▊         | 16/200 [07:29<1:24:53, 27.68s/it]=>> [Epoch 000] Global Step 000016 =>> LR :: 0.000020 - Loss :: 0.3097:   8%|▊         | 16/200 [07:29<1:24:53, 27.68s/it]=>> [Epoch 000] Global Step 000016 =>> LR :: 0.000020 - Loss :: 0.3097:   8%|▊         | 17/200 [07:56<1:24:17, 27.64s/it]=>> [Epoch 000] Global Step 000017 =>> LR :: 0.000020 - Loss :: 0.2970:   8%|▊         | 17/200 [07:56<1:24:17, 27.64s/it]=>> [Epoch 000] Global Step 000017 =>> LR :: 0.000020 - Loss :: 0.2970:   9%|▉         | 18/200 [08:24<1:23:55, 27.67s/it]=>> [Epoch 000] Global Step 000018 =>> LR :: 0.000020 - Loss :: 0.3085:   9%|▉         | 18/200 [08:24<1:23:55, 27.67s/it]=>> [Epoch 000] Global Step 000018 =>> LR :: 0.000020 - Loss :: 0.3085:  10%|▉         | 19/200 [08:51<1:23:18, 27.61s/it]=>> [Epoch 000] Global Step 000019 =>> LR :: 0.000020 - Loss :: 0.2859:  10%|▉         | 19/200 [08:51<1:23:18, 27.61s/it]=>> [Epoch 000] Global Step 000019 =>> LR :: 0.000020 - Loss :: 0.2859:  10%|█         | 20/200 [09:19<1:22:47, 27.60s/it]=>> [Epoch 000] Global Step 000020 =>> LR :: 0.000020 - Loss :: 0.3502:  10%|█         | 20/200 [09:19<1:22:47, 27.60s/it]=>> [Epoch 000] Global Step 000020 =>> LR :: 0.000020 - Loss :: 0.3502:  10%|█         | 21/200 [09:46<1:22:12, 27.56s/it]=>> [Epoch 000] Global Step 000021 =>> LR :: 0.000020 - Loss :: 0.3033:  10%|█         | 21/200 [09:46<1:22:12, 27.56s/it]=>> [Epoch 000] Global Step 000021 =>> LR :: 0.000020 - Loss :: 0.3033:  11%|█         | 22/200 [10:14<1:21:44, 27.55s/it]=>> [Epoch 000] Global Step 000022 =>> LR :: 0.000020 - Loss :: 0.2864:  11%|█         | 22/200 [10:14<1:21:44, 27.55s/it]=>> [Epoch 000] Global Step 000022 =>> LR :: 0.000020 - Loss :: 0.2864:  12%|█▏        | 23/200 [10:42<1:21:22, 27.58s/it]=>> [Epoch 000] Global Step 000023 =>> LR :: 0.000020 - Loss :: 0.2974:  12%|█▏        | 23/200 [10:42<1:21:22, 27.58s/it]=>> [Epoch 000] Global Step 000023 =>> LR :: 0.000020 - Loss :: 0.2974:  12%|█▏        | 24/200 [11:10<1:21:07, 27.66s/it]=>> [Epoch 000] Global Step 000024 =>> LR :: 0.000020 - Loss :: 0.2868:  12%|█▏        | 24/200 [11:10<1:21:07, 27.66s/it]=>> [Epoch 000] Global Step 000024 =>> LR :: 0.000020 - Loss :: 0.2868:  12%|█▎        | 25/200 [11:37<1:20:52, 27.73s/it]=>> [Epoch 000] Global Step 000025 =>> LR :: 0.000020 - Loss :: 0.2492:  12%|█▎        | 25/200 [11:37<1:20:52, 27.73s/it]=>> [Epoch 000] Global Step 000025 =>> LR :: 0.000020 - Loss :: 0.2492:  13%|█▎        | 26/200 [12:05<1:20:13, 27.67s/it]=>> [Epoch 000] Global Step 000026 =>> LR :: 0.000020 - Loss :: 0.2519:  13%|█▎        | 26/200 [12:05<1:20:13, 27.67s/it]=>> [Epoch 000] Global Step 000026 =>> LR :: 0.000020 - Loss :: 0.2519:  14%|█▎        | 27/200 [12:33<1:19:41, 27.64s/it]=>> [Epoch 000] Global Step 000027 =>> LR :: 0.000020 - Loss :: 0.2232:  14%|█▎        | 27/200 [12:33<1:19:41, 27.64s/it]=>> [Epoch 000] Global Step 000027 =>> LR :: 0.000020 - Loss :: 0.2232:  14%|█▍        | 28/200 [13:00<1:18:58, 27.55s/it]=>> [Epoch 000] Global Step 000028 =>> LR :: 0.000020 - Loss :: 0.2502:  14%|█▍        | 28/200 [13:00<1:18:58, 27.55s/it]=>> [Epoch 000] Global Step 000028 =>> LR :: 0.000020 - Loss :: 0.2502:  14%|█▍        | 29/200 [13:27<1:18:22, 27.50s/it]=>> [Epoch 000] Global Step 000029 =>> LR :: 0.000020 - Loss :: 0.2547:  14%|█▍        | 29/200 [13:27<1:18:22, 27.50s/it]=>> [Epoch 000] Global Step 000029 =>> LR :: 0.000020 - Loss :: 0.2547:  15%|█▌        | 30/200 [13:55<1:17:59, 27.52s/it]=>> [Epoch 000] Global Step 000030 =>> LR :: 0.000020 - Loss :: 0.2498:  15%|█▌        | 30/200 [13:55<1:17:59, 27.52s/it]=>> [Epoch 000] Global Step 000030 =>> LR :: 0.000020 - Loss :: 0.2498:  16%|█▌        | 31/200 [14:22<1:17:17, 27.44s/it]=>> [Epoch 000] Global Step 000031 =>> LR :: 0.000020 - Loss :: 0.2377:  16%|█▌        | 31/200 [14:22<1:17:17, 27.44s/it]=>> [Epoch 000] Global Step 000031 =>> LR :: 0.000020 - Loss :: 0.2377:  16%|█▌        | 32/200 [14:49<1:16:31, 27.33s/it]=>> [Epoch 000] Global Step 000032 =>> LR :: 0.000020 - Loss :: 0.2511:  16%|█▌        | 32/200 [14:49<1:16:31, 27.33s/it]=>> [Epoch 000] Global Step 000032 =>> LR :: 0.000020 - Loss :: 0.2511:  16%|█▋        | 33/200 [15:17<1:16:22, 27.44s/it]=>> [Epoch 000] Global Step 000033 =>> LR :: 0.000020 - Loss :: 0.2450:  16%|█▋        | 33/200 [15:17<1:16:22, 27.44s/it]=>> [Epoch 000] Global Step 000033 =>> LR :: 0.000020 - Loss :: 0.2450:  17%|█▋        | 34/200 [15:45<1:16:07, 27.52s/it]=>> [Epoch 000] Global Step 000034 =>> LR :: 0.000020 - Loss :: 0.2430:  17%|█▋        | 34/200 [15:45<1:16:07, 27.52s/it]=>> [Epoch 000] Global Step 000034 =>> LR :: 0.000020 - Loss :: 0.2430:  18%|█▊        | 35/200 [16:12<1:15:33, 27.48s/it]=>> [Epoch 000] Global Step 000035 =>> LR :: 0.000020 - Loss :: 0.2315:  18%|█▊        | 35/200 [16:12<1:15:33, 27.48s/it]=>> [Epoch 000] Global Step 000035 =>> LR :: 0.000020 - Loss :: 0.2315:  18%|█▊        | 36/200 [16:40<1:15:13, 27.52s/it]=>> [Epoch 000] Global Step 000036 =>> LR :: 0.000020 - Loss :: 0.2672:  18%|█▊        | 36/200 [16:40<1:15:13, 27.52s/it]=>> [Epoch 000] Global Step 000036 =>> LR :: 0.000020 - Loss :: 0.2672:  18%|█▊        | 37/200 [17:07<1:14:52, 27.56s/it]=>> [Epoch 000] Global Step 000037 =>> LR :: 0.000020 - Loss :: 0.2330:  18%|█▊        | 37/200 [17:07<1:14:52, 27.56s/it]=>> [Epoch 000] Global Step 000037 =>> LR :: 0.000020 - Loss :: 0.2330:  19%|█▉        | 38/200 [17:35<1:14:17, 27.52s/it]=>> [Epoch 000] Global Step 000038 =>> LR :: 0.000020 - Loss :: 0.2406:  19%|█▉        | 38/200 [17:35<1:14:17, 27.52s/it]=>> [Epoch 000] Global Step 000038 =>> LR :: 0.000020 - Loss :: 0.2406:  20%|█▉        | 39/200 [18:02<1:13:36, 27.43s/it]=>> [Epoch 000] Global Step 000039 =>> LR :: 0.000020 - Loss :: 0.2737:  20%|█▉        | 39/200 [18:02<1:13:36, 27.43s/it]=>> [Epoch 000] Global Step 000039 =>> LR :: 0.000020 - Loss :: 0.2737:  20%|██        | 40/200 [18:29<1:13:19, 27.50s/it]=>> [Epoch 000] Global Step 000040 =>> LR :: 0.000020 - Loss :: 0.2245:  20%|██        | 40/200 [18:29<1:13:19, 27.50s/it]=>> [Epoch 000] Global Step 000040 =>> LR :: 0.000020 - Loss :: 0.2245:  20%|██        | 41/200 [18:57<1:13:10, 27.61s/it]=>> [Epoch 000] Global Step 000041 =>> LR :: 0.000020 - Loss :: 0.2332:  20%|██        | 41/200 [18:57<1:13:10, 27.61s/it]=>> [Epoch 000] Global Step 000041 =>> LR :: 0.000020 - Loss :: 0.2332:  21%|██        | 42/200 [19:25<1:12:41, 27.60s/it]=>> [Epoch 000] Global Step 000042 =>> LR :: 0.000020 - Loss :: 0.2156:  21%|██        | 42/200 [19:25<1:12:41, 27.60s/it]=>> [Epoch 000] Global Step 000042 =>> LR :: 0.000020 - Loss :: 0.2156:  22%|██▏       | 43/200 [19:53<1:12:18, 27.64s/it]=>> [Epoch 000] Global Step 000043 =>> LR :: 0.000020 - Loss :: 0.2419:  22%|██▏       | 43/200 [19:53<1:12:18, 27.64s/it]=>> [Epoch 000] Global Step 000043 =>> LR :: 0.000020 - Loss :: 0.2419:  22%|██▏       | 44/200 [20:20<1:11:49, 27.62s/it]=>> [Epoch 000] Global Step 000044 =>> LR :: 0.000020 - Loss :: 0.2347:  22%|██▏       | 44/200 [20:20<1:11:49, 27.62s/it]=>> [Epoch 000] Global Step 000044 =>> LR :: 0.000020 - Loss :: 0.2347:  22%|██▎       | 45/200 [20:48<1:11:22, 27.63s/it]=>> [Epoch 000] Global Step 000045 =>> LR :: 0.000020 - Loss :: 0.2347:  22%|██▎       | 45/200 [20:48<1:11:22, 27.63s/it]=>> [Epoch 000] Global Step 000045 =>> LR :: 0.000020 - Loss :: 0.2347:  23%|██▎       | 46/200 [21:17<1:11:42, 27.94s/it]=>> [Epoch 000] Global Step 000046 =>> LR :: 0.000020 - Loss :: 0.2176:  23%|██▎       | 46/200 [21:17<1:11:42, 27.94s/it]=>> [Epoch 000] Global Step 000046 =>> LR :: 0.000020 - Loss :: 0.2176:  24%|██▎       | 47/200 [21:44<1:10:58, 27.83s/it]=>> [Epoch 000] Global Step 000047 =>> LR :: 0.000020 - Loss :: 0.2014:  24%|██▎       | 47/200 [21:44<1:10:58, 27.83s/it]=>> [Epoch 000] Global Step 000047 =>> LR :: 0.000020 - Loss :: 0.2014:  24%|██▍       | 48/200 [22:12<1:10:21, 27.77s/it]=>> [Epoch 000] Global Step 000048 =>> LR :: 0.000020 - Loss :: 0.2257:  24%|██▍       | 48/200 [22:12<1:10:21, 27.77s/it]=>> [Epoch 000] Global Step 000048 =>> LR :: 0.000020 - Loss :: 0.2257:  24%|██▍       | 49/200 [22:39<1:09:27, 27.60s/it]=>> [Epoch 000] Global Step 000049 =>> LR :: 0.000020 - Loss :: 0.2367:  24%|██▍       | 49/200 [22:39<1:09:27, 27.60s/it]=>> [Epoch 000] Global Step 000049 =>> LR :: 0.000020 - Loss :: 0.2367:  25%|██▌       | 50/200 [23:06<1:08:46, 27.51s/it]=>> [Epoch 000] Global Step 000050 =>> LR :: 0.000020 - Loss :: 0.2169:  25%|██▌       | 50/200 [23:06<1:08:46, 27.51s/it]=>> [Epoch 000] Global Step 000050 =>> LR :: 0.000020 - Loss :: 0.2169:  26%|██▌       | 51/200 [23:34<1:08:25, 27.55s/it]=>> [Epoch 000] Global Step 000051 =>> LR :: 0.000020 - Loss :: 0.1997:  26%|██▌       | 51/200 [23:34<1:08:25, 27.55s/it]=>> [Epoch 000] Global Step 000051 =>> LR :: 0.000020 - Loss :: 0.1997:  26%|██▌       | 52/200 [24:01<1:07:47, 27.49s/it]=>> [Epoch 000] Global Step 000052 =>> LR :: 0.000020 - Loss :: 0.2044:  26%|██▌       | 52/200 [24:01<1:07:47, 27.49s/it]=>> [Epoch 000] Global Step 000052 =>> LR :: 0.000020 - Loss :: 0.2044:  26%|██▋       | 53/200 [24:29<1:07:26, 27.53s/it]=>> [Epoch 000] Global Step 000053 =>> LR :: 0.000020 - Loss :: 0.2109:  26%|██▋       | 53/200 [24:29<1:07:26, 27.53s/it]=>> [Epoch 000] Global Step 000053 =>> LR :: 0.000020 - Loss :: 0.2109:  27%|██▋       | 54/200 [24:57<1:07:05, 27.57s/it]=>> [Epoch 000] Global Step 000054 =>> LR :: 0.000020 - Loss :: 0.1993:  27%|██▋       | 54/200 [24:57<1:07:05, 27.57s/it]=>> [Epoch 000] Global Step 000054 =>> LR :: 0.000020 - Loss :: 0.1993:  28%|██▊       | 55/200 [25:24<1:06:31, 27.53s/it]=>> [Epoch 000] Global Step 000055 =>> LR :: 0.000020 - Loss :: 0.1994:  28%|██▊       | 55/200 [25:24<1:06:31, 27.53s/it]=>> [Epoch 000] Global Step 000055 =>> LR :: 0.000020 - Loss :: 0.1994:  28%|██▊       | 56/200 [25:51<1:06:03, 27.52s/it]=>> [Epoch 000] Global Step 000056 =>> LR :: 0.000020 - Loss :: 0.1874:  28%|██▊       | 56/200 [25:51<1:06:03, 27.52s/it]=>> [Epoch 000] Global Step 000056 =>> LR :: 0.000020 - Loss :: 0.1874:  28%|██▊       | 57/200 [26:19<1:05:39, 27.55s/it]=>> [Epoch 000] Global Step 000057 =>> LR :: 0.000020 - Loss :: 0.1785:  28%|██▊       | 57/200 [26:19<1:05:39, 27.55s/it]=>> [Epoch 000] Global Step 000057 =>> LR :: 0.000020 - Loss :: 0.1785:  29%|██▉       | 58/200 [26:47<1:05:09, 27.53s/it]=>> [Epoch 000] Global Step 000058 =>> LR :: 0.000020 - Loss :: 0.1746:  29%|██▉       | 58/200 [26:47<1:05:09, 27.53s/it]=>> [Epoch 000] Global Step 000058 =>> LR :: 0.000020 - Loss :: 0.1746:  30%|██▉       | 59/200 [27:14<1:04:51, 27.60s/it]=>> [Epoch 000] Global Step 000059 =>> LR :: 0.000020 - Loss :: 0.1846:  30%|██▉       | 59/200 [27:14<1:04:51, 27.60s/it]=>> [Epoch 000] Global Step 000059 =>> LR :: 0.000020 - Loss :: 0.1846:  30%|███       | 60/200 [27:42<1:04:08, 27.49s/it]=>> [Epoch 000] Global Step 000060 =>> LR :: 0.000020 - Loss :: 0.2153:  30%|███       | 60/200 [27:42<1:04:08, 27.49s/it]=>> [Epoch 000] Global Step 000060 =>> LR :: 0.000020 - Loss :: 0.2153:  30%|███       | 61/200 [28:09<1:03:34, 27.45s/it]=>> [Epoch 000] Global Step 000061 =>> LR :: 0.000020 - Loss :: 0.2024:  30%|███       | 61/200 [28:09<1:03:34, 27.45s/it]=>> [Epoch 000] Global Step 000061 =>> LR :: 0.000020 - Loss :: 0.2024:  31%|███       | 62/200 [28:37<1:03:17, 27.52s/it]=>> [Epoch 000] Global Step 000062 =>> LR :: 0.000020 - Loss :: 0.1985:  31%|███       | 62/200 [28:37<1:03:17, 27.52s/it]=>> [Epoch 000] Global Step 000062 =>> LR :: 0.000020 - Loss :: 0.1985:  32%|███▏      | 63/200 [29:04<1:02:39, 27.44s/it]=>> [Epoch 000] Global Step 000063 =>> LR :: 0.000020 - Loss :: 0.1802:  32%|███▏      | 63/200 [29:04<1:02:39, 27.44s/it]=>> [Epoch 000] Global Step 000063 =>> LR :: 0.000020 - Loss :: 0.1802:  32%|███▏      | 64/200 [29:32<1:02:21, 27.51s/it]=>> [Epoch 000] Global Step 000064 =>> LR :: 0.000020 - Loss :: 0.1579:  32%|███▏      | 64/200 [29:32<1:02:21, 27.51s/it]=>> [Epoch 000] Global Step 000064 =>> LR :: 0.000020 - Loss :: 0.1579:  32%|███▎      | 65/200 [29:59<1:01:56, 27.53s/it]=>> [Epoch 000] Global Step 000065 =>> LR :: 0.000020 - Loss :: 0.1559:  32%|███▎      | 65/200 [29:59<1:01:56, 27.53s/it]=>> [Epoch 000] Global Step 000065 =>> LR :: 0.000020 - Loss :: 0.1559:  33%|███▎      | 66/200 [30:27<1:01:28, 27.52s/it]=>> [Epoch 000] Global Step 000066 =>> LR :: 0.000020 - Loss :: 0.1735:  33%|███▎      | 66/200 [30:27<1:01:28, 27.52s/it]=>> [Epoch 000] Global Step 000066 =>> LR :: 0.000020 - Loss :: 0.1735:  34%|███▎      | 67/200 [30:55<1:01:14, 27.63s/it]=>> [Epoch 000] Global Step 000067 =>> LR :: 0.000020 - Loss :: 0.1586:  34%|███▎      | 67/200 [30:55<1:01:14, 27.63s/it]=>> [Epoch 000] Global Step 000067 =>> LR :: 0.000020 - Loss :: 0.1586:  34%|███▍      | 68/200 [31:22<1:00:50, 27.65s/it]=>> [Epoch 000] Global Step 000068 =>> LR :: 0.000020 - Loss :: 0.1729:  34%|███▍      | 68/200 [31:22<1:00:50, 27.65s/it]=>> [Epoch 000] Global Step 000068 =>> LR :: 0.000020 - Loss :: 0.1729:  34%|███▍      | 69/200 [31:50<1:00:18, 27.62s/it]=>> [Epoch 000] Global Step 000069 =>> LR :: 0.000020 - Loss :: 0.1500:  34%|███▍      | 69/200 [31:50<1:00:18, 27.62s/it]=>> [Epoch 000] Global Step 000069 =>> LR :: 0.000020 - Loss :: 0.1500:  35%|███▌      | 70/200 [32:17<59:42, 27.56s/it]  =>> [Epoch 000] Global Step 000070 =>> LR :: 0.000020 - Loss :: 0.1687:  35%|███▌      | 70/200 [32:17<59:42, 27.56s/it]=>> [Epoch 000] Global Step 000070 =>> LR :: 0.000020 - Loss :: 0.1687:  36%|███▌      | 71/200 [32:45<59:11, 27.53s/it]=>> [Epoch 000] Global Step 000071 =>> LR :: 0.000020 - Loss :: 0.1635:  36%|███▌      | 71/200 [32:45<59:11, 27.53s/it]=>> [Epoch 000] Global Step 000071 =>> LR :: 0.000020 - Loss :: 0.1635:  36%|███▌      | 72/200 [33:12<58:40, 27.50s/it]=>> [Epoch 000] Global Step 000072 =>> LR :: 0.000020 - Loss :: 0.1589:  36%|███▌      | 72/200 [33:12<58:40, 27.50s/it]=>> [Epoch 000] Global Step 000072 =>> LR :: 0.000020 - Loss :: 0.1589:  36%|███▋      | 73/200 [33:39<58:07, 27.46s/it]=>> [Epoch 000] Global Step 000073 =>> LR :: 0.000020 - Loss :: 0.1796:  36%|███▋      | 73/200 [33:39<58:07, 27.46s/it]=>> [Epoch 000] Global Step 000073 =>> LR :: 0.000020 - Loss :: 0.1796:  37%|███▋      | 74/200 [34:07<57:41, 27.47s/it]=>> [Epoch 000] Global Step 000074 =>> LR :: 0.000020 - Loss :: 0.1732:  37%|███▋      | 74/200 [34:07<57:41, 27.47s/it]=>> [Epoch 000] Global Step 000074 =>> LR :: 0.000020 - Loss :: 0.1732:  38%|███▊      | 75/200 [34:35<57:20, 27.53s/it]=>> [Epoch 000] Global Step 000075 =>> LR :: 0.000020 - Loss :: 0.1641:  38%|███▊      | 75/200 [34:35<57:20, 27.53s/it]=>> [Epoch 000] Global Step 000075 =>> LR :: 0.000020 - Loss :: 0.1641:  38%|███▊      | 76/200 [35:02<56:56, 27.55s/it]=>> [Epoch 000] Global Step 000076 =>> LR :: 0.000020 - Loss :: 0.1709:  38%|███▊      | 76/200 [35:02<56:56, 27.55s/it]=>> [Epoch 000] Global Step 000076 =>> LR :: 0.000020 - Loss :: 0.1709:  38%|███▊      | 77/200 [35:30<56:20, 27.49s/it]=>> [Epoch 000] Global Step 000077 =>> LR :: 0.000020 - Loss :: 0.1689:  38%|███▊      | 77/200 [35:30<56:20, 27.49s/it]=>> [Epoch 000] Global Step 000077 =>> LR :: 0.000020 - Loss :: 0.1689:  39%|███▉      | 78/200 [35:57<56:02, 27.56s/it]=>> [Epoch 000] Global Step 000078 =>> LR :: 0.000020 - Loss :: 0.1812:  39%|███▉      | 78/200 [35:57<56:02, 27.56s/it]=>> [Epoch 000] Global Step 000078 =>> LR :: 0.000020 - Loss :: 0.1812:  40%|███▉      | 79/200 [36:25<55:27, 27.50s/it]=>> [Epoch 000] Global Step 000079 =>> LR :: 0.000020 - Loss :: 0.1615:  40%|███▉      | 79/200 [36:25<55:27, 27.50s/it]=>> [Epoch 000] Global Step 000079 =>> LR :: 0.000020 - Loss :: 0.1615:  40%|████      | 80/200 [36:52<54:57, 27.48s/it]=>> [Epoch 000] Global Step 000080 =>> LR :: 0.000020 - Loss :: 0.1686:  40%|████      | 80/200 [36:52<54:57, 27.48s/it]=>> [Epoch 000] Global Step 000080 =>> LR :: 0.000020 - Loss :: 0.1686:  40%|████      | 81/200 [37:20<54:31, 27.49s/it]=>> [Epoch 000] Global Step 000081 =>> LR :: 0.000020 - Loss :: 0.1390:  40%|████      | 81/200 [37:20<54:31, 27.49s/it]=>> [Epoch 000] Global Step 000081 =>> LR :: 0.000020 - Loss :: 0.1390:  41%|████      | 82/200 [37:47<53:58, 27.44s/it]=>> [Epoch 000] Global Step 000082 =>> LR :: 0.000020 - Loss :: 0.1586:  41%|████      | 82/200 [37:47<53:58, 27.44s/it]=>> [Epoch 000] Global Step 000082 =>> LR :: 0.000020 - Loss :: 0.1586:  42%|████▏     | 83/200 [38:14<53:33, 27.46s/it]=>> [Epoch 000] Global Step 000083 =>> LR :: 0.000020 - Loss :: 0.1454:  42%|████▏     | 83/200 [38:14<53:33, 27.46s/it]=>> [Epoch 000] Global Step 000083 =>> LR :: 0.000020 - Loss :: 0.1454:  42%|████▏     | 84/200 [38:42<53:22, 27.61s/it]=>> [Epoch 000] Global Step 000084 =>> LR :: 0.000020 - Loss :: 0.1470:  42%|████▏     | 84/200 [38:42<53:22, 27.61s/it]=>> [Epoch 000] Global Step 000084 =>> LR :: 0.000020 - Loss :: 0.1470:  42%|████▎     | 85/200 [39:10<53:02, 27.68s/it]=>> [Epoch 000] Global Step 000085 =>> LR :: 0.000020 - Loss :: 0.1361:  42%|████▎     | 85/200 [39:10<53:02, 27.68s/it]=>> [Epoch 000] Global Step 000085 =>> LR :: 0.000020 - Loss :: 0.1361:  43%|████▎     | 86/200 [39:38<52:42, 27.74s/it]=>> [Epoch 000] Global Step 000086 =>> LR :: 0.000020 - Loss :: 0.1292:  43%|████▎     | 86/200 [39:38<52:42, 27.74s/it]=>> [Epoch 000] Global Step 000086 =>> LR :: 0.000020 - Loss :: 0.1292:  44%|████▎     | 87/200 [40:06<52:10, 27.70s/it]=>> [Epoch 000] Global Step 000087 =>> LR :: 0.000020 - Loss :: 0.1444:  44%|████▎     | 87/200 [40:06<52:10, 27.70s/it]=>> [Epoch 000] Global Step 000087 =>> LR :: 0.000020 - Loss :: 0.1444:  44%|████▍     | 88/200 [40:33<51:43, 27.71s/it]=>> [Epoch 000] Global Step 000088 =>> LR :: 0.000020 - Loss :: 0.1393:  44%|████▍     | 88/200 [40:33<51:43, 27.71s/it]=>> [Epoch 000] Global Step 000088 =>> LR :: 0.000020 - Loss :: 0.1393:  44%|████▍     | 89/200 [41:01<51:21, 27.76s/it]=>> [Epoch 000] Global Step 000089 =>> LR :: 0.000020 - Loss :: 0.1277:  44%|████▍     | 89/200 [41:01<51:21, 27.76s/it]=>> [Epoch 000] Global Step 000089 =>> LR :: 0.000020 - Loss :: 0.1277:  45%|████▌     | 90/200 [41:29<50:50, 27.73s/it]=>> [Epoch 000] Global Step 000090 =>> LR :: 0.000020 - Loss :: 0.1315:  45%|████▌     | 90/200 [41:29<50:50, 27.73s/it]=>> [Epoch 000] Global Step 000090 =>> LR :: 0.000020 - Loss :: 0.1315:  46%|████▌     | 91/200 [41:57<50:24, 27.75s/it]=>> [Epoch 000] Global Step 000091 =>> LR :: 0.000020 - Loss :: 0.1466:  46%|████▌     | 91/200 [41:57<50:24, 27.75s/it]=>> [Epoch 000] Global Step 000091 =>> LR :: 0.000020 - Loss :: 0.1466:  46%|████▌     | 92/200 [42:24<49:49, 27.68s/it]=>> [Epoch 000] Global Step 000092 =>> LR :: 0.000020 - Loss :: 0.1505:  46%|████▌     | 92/200 [42:24<49:49, 27.68s/it]=>> [Epoch 000] Global Step 000092 =>> LR :: 0.000020 - Loss :: 0.1505:  46%|████▋     | 93/200 [42:52<49:22, 27.69s/it]=>> [Epoch 000] Global Step 000093 =>> LR :: 0.000020 - Loss :: 0.1345:  46%|████▋     | 93/200 [42:52<49:22, 27.69s/it]=>> [Epoch 000] Global Step 000093 =>> LR :: 0.000020 - Loss :: 0.1345:  47%|████▋     | 94/200 [43:20<48:57, 27.71s/it]=>> [Epoch 000] Global Step 000094 =>> LR :: 0.000020 - Loss :: 0.1286:  47%|████▋     | 94/200 [43:20<48:57, 27.71s/it]=>> [Epoch 000] Global Step 000094 =>> LR :: 0.000020 - Loss :: 0.1286:  48%|████▊     | 95/200 [43:47<48:27, 27.69s/it]=>> [Epoch 000] Global Step 000095 =>> LR :: 0.000020 - Loss :: 0.1491:  48%|████▊     | 95/200 [43:47<48:27, 27.69s/it]=>> [Epoch 000] Global Step 000095 =>> LR :: 0.000020 - Loss :: 0.1491:  48%|████▊     | 96/200 [44:15<48:00, 27.70s/it]=>> [Epoch 000] Global Step 000096 =>> LR :: 0.000020 - Loss :: 0.1402:  48%|████▊     | 96/200 [44:15<48:00, 27.70s/it]=>> [Epoch 000] Global Step 000096 =>> LR :: 0.000020 - Loss :: 0.1402:  48%|████▊     | 97/200 [44:42<47:18, 27.56s/it]=>> [Epoch 000] Global Step 000097 =>> LR :: 0.000020 - Loss :: 0.1380:  48%|████▊     | 97/200 [44:42<47:18, 27.56s/it]=>> [Epoch 000] Global Step 000097 =>> LR :: 0.000020 - Loss :: 0.1380:  49%|████▉     | 98/200 [45:10<46:50, 27.56s/it]=>> [Epoch 000] Global Step 000098 =>> LR :: 0.000020 - Loss :: 0.1364:  49%|████▉     | 98/200 [45:10<46:50, 27.56s/it]=>> [Epoch 000] Global Step 000098 =>> LR :: 0.000020 - Loss :: 0.1364:  50%|████▉     | 99/200 [45:38<46:38, 27.70s/it]=>> [Epoch 000] Global Step 000099 =>> LR :: 0.000020 - Loss :: 0.1452:  50%|████▉     | 99/200 [45:38<46:38, 27.70s/it]=>> [Epoch 000] Global Step 000099 =>> LR :: 0.000020 - Loss :: 0.1452:  50%|█████     | 100/200 [46:06<46:10, 27.70s/it]=>> [Epoch 000] Global Step 000100 =>> LR :: 0.000020 - Loss :: 0.1433:  50%|█████     | 100/200 [46:06<46:10, 27.70s/it]=>> [Epoch 000] Global Step 000100 =>> LR :: 0.000020 - Loss :: 0.1433:  50%|█████     | 101/200 [46:38<47:50, 28.99s/it]=>> [Epoch 000] Global Step 000101 =>> LR :: 0.000020 - Loss :: 0.1382:  50%|█████     | 101/200 [46:38<47:50, 28.99s/it]=>> [Epoch 000] Global Step 000101 =>> LR :: 0.000020 - Loss :: 0.1382:  51%|█████     | 102/200 [47:05<46:22, 28.39s/it]=>> [Epoch 000] Global Step 000102 =>> LR :: 0.000020 - Loss :: 0.1287:  51%|█████     | 102/200 [47:05<46:22, 28.39s/it]=>> [Epoch 000] Global Step 000102 =>> LR :: 0.000020 - Loss :: 0.1287:  52%|█████▏    | 103/200 [47:32<45:31, 28.16s/it]=>> [Epoch 000] Global Step 000103 =>> LR :: 0.000020 - Loss :: 0.1279:  52%|█████▏    | 103/200 [47:32<45:31, 28.16s/it]=>> [Epoch 000] Global Step 000103 =>> LR :: 0.000020 - Loss :: 0.1279:  52%|█████▏    | 104/200 [48:00<44:43, 27.96s/it]=>> [Epoch 000] Global Step 000104 =>> LR :: 0.000020 - Loss :: 0.1334:  52%|█████▏    | 104/200 [48:00<44:43, 27.96s/it]=>> [Epoch 000] Global Step 000104 =>> LR :: 0.000020 - Loss :: 0.1334:  52%|█████▎    | 105/200 [48:27<44:03, 27.83s/it]=>> [Epoch 000] Global Step 000105 =>> LR :: 0.000020 - Loss :: 0.1375:  52%|█████▎    | 105/200 [48:27<44:03, 27.83s/it]=>> [Epoch 000] Global Step 000105 =>> LR :: 0.000020 - Loss :: 0.1375:  53%|█████▎    | 106/200 [48:55<43:43, 27.91s/it]=>> [Epoch 000] Global Step 000106 =>> LR :: 0.000020 - Loss :: 0.1287:  53%|█████▎    | 106/200 [48:55<43:43, 27.91s/it]=>> [Epoch 000] Global Step 000106 =>> LR :: 0.000020 - Loss :: 0.1287:  54%|█████▎    | 107/200 [49:23<43:11, 27.86s/it]=>> [Epoch 000] Global Step 000107 =>> LR :: 0.000020 - Loss :: 0.1268:  54%|█████▎    | 107/200 [49:23<43:11, 27.86s/it]=>> [Epoch 000] Global Step 000107 =>> LR :: 0.000020 - Loss :: 0.1268:  54%|█████▍    | 108/200 [49:51<42:37, 27.80s/it]=>> [Epoch 000] Global Step 000108 =>> LR :: 0.000020 - Loss :: 0.1198:  54%|█████▍    | 108/200 [49:51<42:37, 27.80s/it]=>> [Epoch 000] Global Step 000108 =>> LR :: 0.000020 - Loss :: 0.1198:  55%|█████▍    | 109/200 [50:18<42:05, 27.76s/it]=>> [Epoch 000] Global Step 000109 =>> LR :: 0.000020 - Loss :: 0.1184:  55%|█████▍    | 109/200 [50:18<42:05, 27.76s/it]=>> [Epoch 000] Global Step 000109 =>> LR :: 0.000020 - Loss :: 0.1184:  55%|█████▌    | 110/200 [50:46<41:39, 27.77s/it]=>> [Epoch 000] Global Step 000110 =>> LR :: 0.000020 - Loss :: 0.1320:  55%|█████▌    | 110/200 [50:46<41:39, 27.77s/it]=>> [Epoch 000] Global Step 000110 =>> LR :: 0.000020 - Loss :: 0.1320:  56%|█████▌    | 111/200 [51:14<40:58, 27.63s/it]=>> [Epoch 000] Global Step 000111 =>> LR :: 0.000020 - Loss :: 0.1270:  56%|█████▌    | 111/200 [51:14<40:58, 27.63s/it]=>> [Epoch 000] Global Step 000111 =>> LR :: 0.000020 - Loss :: 0.1270:  56%|█████▌    | 112/200 [51:41<40:30, 27.62s/it]=>> [Epoch 000] Global Step 000112 =>> LR :: 0.000020 - Loss :: 0.1304:  56%|█████▌    | 112/200 [51:41<40:30, 27.62s/it]=>> [Epoch 000] Global Step 000112 =>> LR :: 0.000020 - Loss :: 0.1304:  56%|█████▋    | 113/200 [52:09<40:06, 27.66s/it]=>> [Epoch 000] Global Step 000113 =>> LR :: 0.000020 - Loss :: 0.1146:  56%|█████▋    | 113/200 [52:09<40:06, 27.66s/it]=>> [Epoch 000] Global Step 000113 =>> LR :: 0.000020 - Loss :: 0.1146:  57%|█████▋    | 114/200 [52:36<39:31, 27.58s/it]=>> [Epoch 000] Global Step 000114 =>> LR :: 0.000020 - Loss :: 0.1274:  57%|█████▋    | 114/200 [52:36<39:31, 27.58s/it]=>> [Epoch 000] Global Step 000114 =>> LR :: 0.000020 - Loss :: 0.1274:  57%|█████▊    | 115/200 [53:04<39:01, 27.55s/it]=>> [Epoch 000] Global Step 000115 =>> LR :: 0.000020 - Loss :: 0.1194:  57%|█████▊    | 115/200 [53:04<39:01, 27.55s/it]=>> [Epoch 000] Global Step 000115 =>> LR :: 0.000020 - Loss :: 0.1194:  58%|█████▊    | 116/200 [53:32<38:38, 27.61s/it]=>> [Epoch 000] Global Step 000116 =>> LR :: 0.000020 - Loss :: 0.1224:  58%|█████▊    | 116/200 [53:32<38:38, 27.61s/it]=>> [Epoch 000] Global Step 000116 =>> LR :: 0.000020 - Loss :: 0.1224:  58%|█████▊    | 117/200 [53:59<38:05, 27.54s/it]=>> [Epoch 000] Global Step 000117 =>> LR :: 0.000020 - Loss :: 0.1160:  58%|█████▊    | 117/200 [53:59<38:05, 27.54s/it]=>> [Epoch 000] Global Step 000117 =>> LR :: 0.000020 - Loss :: 0.1160:  59%|█████▉    | 118/200 [54:27<37:40, 27.57s/it]=>> [Epoch 000] Global Step 000118 =>> LR :: 0.000020 - Loss :: 0.1245:  59%|█████▉    | 118/200 [54:27<37:40, 27.57s/it]=>> [Epoch 000] Global Step 000118 =>> LR :: 0.000020 - Loss :: 0.1245:  60%|█████▉    | 119/200 [54:54<37:14, 27.58s/it]=>> [Epoch 000] Global Step 000119 =>> LR :: 0.000020 - Loss :: 0.1113:  60%|█████▉    | 119/200 [54:54<37:14, 27.58s/it]=>> [Epoch 000] Global Step 000119 =>> LR :: 0.000020 - Loss :: 0.1113:  60%|██████    | 120/200 [55:21<36:39, 27.49s/it]=>> [Epoch 000] Global Step 000120 =>> LR :: 0.000020 - Loss :: 0.1215:  60%|██████    | 120/200 [55:21<36:39, 27.49s/it]=>> [Epoch 000] Global Step 000120 =>> LR :: 0.000020 - Loss :: 0.1215:  60%|██████    | 121/200 [55:49<36:14, 27.52s/it]=>> [Epoch 000] Global Step 000121 =>> LR :: 0.000020 - Loss :: 0.1164:  60%|██████    | 121/200 [55:49<36:14, 27.52s/it]=>> [Epoch 000] Global Step 000121 =>> LR :: 0.000020 - Loss :: 0.1164:  61%|██████    | 122/200 [56:16<35:43, 27.48s/it]=>> [Epoch 000] Global Step 000122 =>> LR :: 0.000020 - Loss :: 0.1214:  61%|██████    | 122/200 [56:16<35:43, 27.48s/it]=>> [Epoch 000] Global Step 000122 =>> LR :: 0.000020 - Loss :: 0.1214:  62%|██████▏   | 123/200 [56:44<35:16, 27.48s/it]=>> [Epoch 000] Global Step 000123 =>> LR :: 0.000020 - Loss :: 0.1135:  62%|██████▏   | 123/200 [56:44<35:16, 27.48s/it]=>> [Epoch 000] Global Step 000123 =>> LR :: 0.000020 - Loss :: 0.1135:  62%|██████▏   | 124/200 [57:12<34:55, 27.58s/it]=>> [Epoch 000] Global Step 000124 =>> LR :: 0.000020 - Loss :: 0.1139:  62%|██████▏   | 124/200 [57:12<34:55, 27.58s/it]=>> [Epoch 000] Global Step 000124 =>> LR :: 0.000020 - Loss :: 0.1139:  62%|██████▎   | 125/200 [57:39<34:24, 27.52s/it]=>> [Epoch 000] Global Step 000125 =>> LR :: 0.000020 - Loss :: 0.1119:  62%|██████▎   | 125/200 [57:39<34:24, 27.52s/it]=>> [Epoch 000] Global Step 000125 =>> LR :: 0.000020 - Loss :: 0.1119:  63%|██████▎   | 126/200 [58:07<33:55, 27.51s/it]=>> [Epoch 000] Global Step 000126 =>> LR :: 0.000020 - Loss :: 0.1083:  63%|██████▎   | 126/200 [58:07<33:55, 27.51s/it]=>> [Epoch 000] Global Step 000126 =>> LR :: 0.000020 - Loss :: 0.1083:  64%|██████▎   | 127/200 [58:34<33:36, 27.62s/it]=>> [Epoch 000] Global Step 000127 =>> LR :: 0.000020 - Loss :: 0.1023:  64%|██████▎   | 127/200 [58:34<33:36, 27.62s/it]=>> [Epoch 000] Global Step 000127 =>> LR :: 0.000020 - Loss :: 0.1023:  64%|██████▍   | 128/200 [59:02<33:13, 27.69s/it]=>> [Epoch 000] Global Step 000128 =>> LR :: 0.000020 - Loss :: 0.0932:  64%|██████▍   | 128/200 [59:02<33:13, 27.69s/it]=>> [Epoch 000] Global Step 000128 =>> LR :: 0.000020 - Loss :: 0.0932:  64%|██████▍   | 129/200 [59:30<32:40, 27.61s/it]=>> [Epoch 000] Global Step 000129 =>> LR :: 0.000020 - Loss :: 0.1010:  64%|██████▍   | 129/200 [59:30<32:40, 27.61s/it]=>> [Epoch 000] Global Step 000129 =>> LR :: 0.000020 - Loss :: 0.1010:  65%|██████▌   | 130/200 [59:58<32:20, 27.72s/it]=>> [Epoch 000] Global Step 000130 =>> LR :: 0.000020 - Loss :: 0.1074:  65%|██████▌   | 130/200 [59:58<32:20, 27.72s/it]=>> [Epoch 000] Global Step 000130 =>> LR :: 0.000020 - Loss :: 0.1074:  66%|██████▌   | 131/200 [1:00:25<31:49, 27.67s/it]=>> [Epoch 000] Global Step 000131 =>> LR :: 0.000020 - Loss :: 0.0991:  66%|██████▌   | 131/200 [1:00:25<31:49, 27.67s/it]=>> [Epoch 000] Global Step 000131 =>> LR :: 0.000020 - Loss :: 0.0991:  66%|██████▌   | 132/200 [1:00:52<31:12, 27.53s/it]=>> [Epoch 000] Global Step 000132 =>> LR :: 0.000020 - Loss :: 0.0934:  66%|██████▌   | 132/200 [1:00:52<31:12, 27.53s/it]=>> [Epoch 000] Global Step 000132 =>> LR :: 0.000020 - Loss :: 0.0934:  66%|██████▋   | 133/200 [1:01:20<30:45, 27.54s/it]=>> [Epoch 000] Global Step 000133 =>> LR :: 0.000020 - Loss :: 0.0897:  66%|██████▋   | 133/200 [1:01:20<30:45, 27.54s/it]=>> [Epoch 000] Global Step 000133 =>> LR :: 0.000020 - Loss :: 0.0897:  67%|██████▋   | 134/200 [1:01:47<30:16, 27.52s/it]=>> [Epoch 000] Global Step 000134 =>> LR :: 0.000020 - Loss :: 0.0971:  67%|██████▋   | 134/200 [1:01:47<30:16, 27.52s/it]=>> [Epoch 000] Global Step 000134 =>> LR :: 0.000020 - Loss :: 0.0971:  68%|██████▊   | 135/200 [1:02:15<29:51, 27.56s/it]=>> [Epoch 000] Global Step 000135 =>> LR :: 0.000020 - Loss :: 0.0938:  68%|██████▊   | 135/200 [1:02:15<29:51, 27.56s/it]=>> [Epoch 000] Global Step 000135 =>> LR :: 0.000020 - Loss :: 0.0938:  68%|██████▊   | 136/200 [1:02:43<29:24, 27.57s/it]=>> [Epoch 000] Global Step 000136 =>> LR :: 0.000020 - Loss :: 0.1004:  68%|██████▊   | 136/200 [1:02:43<29:24, 27.57s/it]=>> [Epoch 000] Global Step 000136 =>> LR :: 0.000020 - Loss :: 0.1004:  68%|██████▊   | 137/200 [1:03:10<28:56, 27.57s/it]=>> [Epoch 000] Global Step 000137 =>> LR :: 0.000020 - Loss :: 0.0919:  68%|██████▊   | 137/200 [1:03:10<28:56, 27.57s/it]=>> [Epoch 000] Global Step 000137 =>> LR :: 0.000020 - Loss :: 0.0919:  69%|██████▉   | 138/200 [1:03:38<28:27, 27.54s/it]=>> [Epoch 000] Global Step 000138 =>> LR :: 0.000020 - Loss :: 0.0993:  69%|██████▉   | 138/200 [1:03:38<28:27, 27.54s/it]=>> [Epoch 000] Global Step 000138 =>> LR :: 0.000020 - Loss :: 0.0993:  70%|██████▉   | 139/200 [1:04:05<27:57, 27.50s/it]=>> [Epoch 000] Global Step 000139 =>> LR :: 0.000020 - Loss :: 0.0982:  70%|██████▉   | 139/200 [1:04:05<27:57, 27.50s/it]=>> [Epoch 000] Global Step 000139 =>> LR :: 0.000020 - Loss :: 0.0982:  70%|███████   | 140/200 [1:04:33<27:28, 27.48s/it]=>> [Epoch 000] Global Step 000140 =>> LR :: 0.000020 - Loss :: 0.0963:  70%|███████   | 140/200 [1:04:33<27:28, 27.48s/it]=>> [Epoch 000] Global Step 000140 =>> LR :: 0.000020 - Loss :: 0.0963:  70%|███████   | 141/200 [1:05:00<27:04, 27.54s/it]=>> [Epoch 000] Global Step 000141 =>> LR :: 0.000020 - Loss :: 0.0978:  70%|███████   | 141/200 [1:05:00<27:04, 27.54s/it]=>> [Epoch 000] Global Step 000141 =>> LR :: 0.000020 - Loss :: 0.0978:  71%|███████   | 142/200 [1:05:28<26:37, 27.54s/it]=>> [Epoch 000] Global Step 000142 =>> LR :: 0.000020 - Loss :: 0.0848:  71%|███████   | 142/200 [1:05:28<26:37, 27.54s/it]=>> [Epoch 000] Global Step 000142 =>> LR :: 0.000020 - Loss :: 0.0848:  72%|███████▏  | 143/200 [1:05:55<26:08, 27.51s/it]=>> [Epoch 000] Global Step 000143 =>> LR :: 0.000020 - Loss :: 0.0945:  72%|███████▏  | 143/200 [1:05:55<26:08, 27.51s/it]=>> [Epoch 000] Global Step 000143 =>> LR :: 0.000020 - Loss :: 0.0945:  72%|███████▏  | 144/200 [1:06:23<25:40, 27.50s/it]=>> [Epoch 000] Global Step 000144 =>> LR :: 0.000020 - Loss :: 0.0904:  72%|███████▏  | 144/200 [1:06:23<25:40, 27.50s/it]=>> [Epoch 000] Global Step 000144 =>> LR :: 0.000020 - Loss :: 0.0904:  72%|███████▎  | 145/200 [1:06:50<25:09, 27.45s/it]=>> [Epoch 000] Global Step 000145 =>> LR :: 0.000020 - Loss :: 0.0851:  72%|███████▎  | 145/200 [1:06:50<25:09, 27.45s/it]=>> [Epoch 000] Global Step 000145 =>> LR :: 0.000020 - Loss :: 0.0851:  73%|███████▎  | 146/200 [1:07:18<24:43, 27.47s/it]=>> [Epoch 000] Global Step 000146 =>> LR :: 0.000020 - Loss :: 0.0841:  73%|███████▎  | 146/200 [1:07:18<24:43, 27.47s/it]=>> [Epoch 000] Global Step 000146 =>> LR :: 0.000020 - Loss :: 0.0841:  74%|███████▎  | 147/200 [1:07:45<24:14, 27.44s/it]=>> [Epoch 000] Global Step 000147 =>> LR :: 0.000020 - Loss :: 0.0774:  74%|███████▎  | 147/200 [1:07:45<24:14, 27.44s/it]=>> [Epoch 000] Global Step 000147 =>> LR :: 0.000020 - Loss :: 0.0774:  74%|███████▍  | 148/200 [1:08:13<23:50, 27.52s/it]=>> [Epoch 000] Global Step 000148 =>> LR :: 0.000020 - Loss :: 0.0817:  74%|███████▍  | 148/200 [1:08:13<23:50, 27.52s/it]=>> [Epoch 000] Global Step 000148 =>> LR :: 0.000020 - Loss :: 0.0817:  74%|███████▍  | 149/200 [1:08:40<23:25, 27.56s/it]=>> [Epoch 000] Global Step 000149 =>> LR :: 0.000020 - Loss :: 0.0801:  74%|███████▍  | 149/200 [1:08:40<23:25, 27.56s/it]=>> [Epoch 000] Global Step 000149 =>> LR :: 0.000020 - Loss :: 0.0801:  75%|███████▌  | 150/200 [1:09:08<23:01, 27.64s/it]=>> [Epoch 000] Global Step 000150 =>> LR :: 0.000020 - Loss :: 0.0886:  75%|███████▌  | 150/200 [1:09:08<23:01, 27.64s/it]=>> [Epoch 000] Global Step 000150 =>> LR :: 0.000020 - Loss :: 0.0886:  76%|███████▌  | 151/200 [1:09:36<22:31, 27.59s/it]=>> [Epoch 000] Global Step 000151 =>> LR :: 0.000020 - Loss :: 0.0819:  76%|███████▌  | 151/200 [1:09:36<22:31, 27.59s/it]=>> [Epoch 000] Global Step 000151 =>> LR :: 0.000020 - Loss :: 0.0819:  76%|███████▌  | 152/200 [1:10:03<22:04, 27.59s/it]=>> [Epoch 000] Global Step 000152 =>> LR :: 0.000020 - Loss :: 0.0994:  76%|███████▌  | 152/200 [1:10:03<22:04, 27.59s/it]=>> [Epoch 000] Global Step 000152 =>> LR :: 0.000020 - Loss :: 0.0994:  76%|███████▋  | 153/200 [1:10:31<21:35, 27.56s/it]=>> [Epoch 000] Global Step 000153 =>> LR :: 0.000020 - Loss :: 0.0953:  76%|███████▋  | 153/200 [1:10:31<21:35, 27.56s/it]=>> [Epoch 000] Global Step 000153 =>> LR :: 0.000020 - Loss :: 0.0953:  77%|███████▋  | 154/200 [1:10:58<21:03, 27.47s/it]=>> [Epoch 000] Global Step 000154 =>> LR :: 0.000020 - Loss :: 0.1043:  77%|███████▋  | 154/200 [1:10:58<21:03, 27.47s/it]=>> [Epoch 000] Global Step 000154 =>> LR :: 0.000020 - Loss :: 0.1043:  78%|███████▊  | 155/200 [1:11:25<20:34, 27.44s/it]=>> [Epoch 000] Global Step 000155 =>> LR :: 0.000020 - Loss :: 0.0925:  78%|███████▊  | 155/200 [1:11:25<20:34, 27.44s/it]=>> [Epoch 000] Global Step 000155 =>> LR :: 0.000020 - Loss :: 0.0925:  78%|███████▊  | 156/200 [1:11:53<20:09, 27.49s/it]=>> [Epoch 000] Global Step 000156 =>> LR :: 0.000020 - Loss :: 0.0932:  78%|███████▊  | 156/200 [1:11:53<20:09, 27.49s/it]=>> [Epoch 000] Global Step 000156 =>> LR :: 0.000020 - Loss :: 0.0932:  78%|███████▊  | 157/200 [1:12:20<19:40, 27.46s/it]=>> [Epoch 000] Global Step 000157 =>> LR :: 0.000020 - Loss :: 0.0815:  78%|███████▊  | 157/200 [1:12:20<19:40, 27.46s/it]=>> [Epoch 000] Global Step 000157 =>> LR :: 0.000020 - Loss :: 0.0815:  79%|███████▉  | 158/200 [1:12:48<19:15, 27.51s/it]=>> [Epoch 000] Global Step 000158 =>> LR :: 0.000020 - Loss :: 0.0704:  79%|███████▉  | 158/200 [1:12:48<19:15, 27.51s/it]=>> [Epoch 000] Global Step 000158 =>> LR :: 0.000020 - Loss :: 0.0704:  80%|███████▉  | 159/200 [1:13:16<18:50, 27.56s/it]=>> [Epoch 000] Global Step 000159 =>> LR :: 0.000020 - Loss :: 0.0711:  80%|███████▉  | 159/200 [1:13:16<18:50, 27.56s/it]=>> [Epoch 000] Global Step 000159 =>> LR :: 0.000020 - Loss :: 0.0711:  80%|████████  | 160/200 [1:13:44<18:26, 27.66s/it]=>> [Epoch 000] Global Step 000160 =>> LR :: 0.000020 - Loss :: 0.0885:  80%|████████  | 160/200 [1:13:44<18:26, 27.66s/it]=>> [Epoch 000] Global Step 000160 =>> LR :: 0.000020 - Loss :: 0.0885:  80%|████████  | 161/200 [1:14:11<17:56, 27.60s/it]=>> [Epoch 000] Global Step 000161 =>> LR :: 0.000020 - Loss :: 0.0739:  80%|████████  | 161/200 [1:14:11<17:56, 27.60s/it]=>> [Epoch 000] Global Step 000161 =>> LR :: 0.000020 - Loss :: 0.0739:  81%|████████  | 162/200 [1:14:38<17:23, 27.46s/it]=>> [Epoch 000] Global Step 000162 =>> LR :: 0.000020 - Loss :: 0.0841:  81%|████████  | 162/200 [1:14:38<17:23, 27.46s/it]=>> [Epoch 000] Global Step 000162 =>> LR :: 0.000020 - Loss :: 0.0841:  82%|████████▏ | 163/200 [1:15:06<16:56, 27.47s/it]=>> [Epoch 000] Global Step 000163 =>> LR :: 0.000020 - Loss :: 0.0909:  82%|████████▏ | 163/200 [1:15:06<16:56, 27.47s/it]=>> [Epoch 000] Global Step 000163 =>> LR :: 0.000020 - Loss :: 0.0909:  82%|████████▏ | 164/200 [1:15:33<16:30, 27.51s/it]=>> [Epoch 000] Global Step 000164 =>> LR :: 0.000020 - Loss :: 0.1149:  82%|████████▏ | 164/200 [1:15:33<16:30, 27.51s/it]=>> [Epoch 000] Global Step 000164 =>> LR :: 0.000020 - Loss :: 0.1149:  82%|████████▎ | 165/200 [1:16:00<16:00, 27.43s/it]=>> [Epoch 000] Global Step 000165 =>> LR :: 0.000020 - Loss :: 0.0862:  82%|████████▎ | 165/200 [1:16:00<16:00, 27.43s/it]=>> [Epoch 000] Global Step 000165 =>> LR :: 0.000020 - Loss :: 0.0862:  83%|████████▎ | 166/200 [1:16:28<15:30, 27.38s/it]=>> [Epoch 000] Global Step 000166 =>> LR :: 0.000020 - Loss :: 0.0914:  83%|████████▎ | 166/200 [1:16:28<15:30, 27.38s/it]=>> [Epoch 000] Global Step 000166 =>> LR :: 0.000020 - Loss :: 0.0914:  84%|████████▎ | 167/200 [1:16:55<15:03, 27.37s/it]=>> [Epoch 000] Global Step 000167 =>> LR :: 0.000020 - Loss :: 0.1113:  84%|████████▎ | 167/200 [1:16:55<15:03, 27.37s/it]=>> [Epoch 000] Global Step 000167 =>> LR :: 0.000020 - Loss :: 0.1113:  84%|████████▍ | 168/200 [1:17:23<14:37, 27.41s/it]=>> [Epoch 000] Global Step 000168 =>> LR :: 0.000020 - Loss :: 0.1112:  84%|████████▍ | 168/200 [1:17:23<14:37, 27.41s/it]=>> [Epoch 000] Global Step 000168 =>> LR :: 0.000020 - Loss :: 0.1112:  84%|████████▍ | 169/200 [1:17:50<14:14, 27.56s/it]=>> [Epoch 000] Global Step 000169 =>> LR :: 0.000020 - Loss :: 0.1042:  84%|████████▍ | 169/200 [1:17:50<14:14, 27.56s/it]=>> [Epoch 000] Global Step 000169 =>> LR :: 0.000020 - Loss :: 0.1042:  85%|████████▌ | 170/200 [1:18:18<13:46, 27.56s/it]=>> [Epoch 000] Global Step 000170 =>> LR :: 0.000020 - Loss :: 0.0922:  85%|████████▌ | 170/200 [1:18:18<13:46, 27.56s/it]=>> [Epoch 000] Global Step 000170 =>> LR :: 0.000020 - Loss :: 0.0922:  86%|████████▌ | 171/200 [1:18:46<13:21, 27.64s/it]=>> [Epoch 000] Global Step 000171 =>> LR :: 0.000020 - Loss :: 0.0979:  86%|████████▌ | 171/200 [1:18:46<13:21, 27.64s/it]=>> [Epoch 000] Global Step 000171 =>> LR :: 0.000020 - Loss :: 0.0979:  86%|████████▌ | 172/200 [1:19:13<12:51, 27.57s/it]=>> [Epoch 000] Global Step 000172 =>> LR :: 0.000020 - Loss :: 0.0775:  86%|████████▌ | 172/200 [1:19:13<12:51, 27.57s/it]=>> [Epoch 000] Global Step 000172 =>> LR :: 0.000020 - Loss :: 0.0775:  86%|████████▋ | 173/200 [1:19:41<12:23, 27.55s/it]=>> [Epoch 000] Global Step 000173 =>> LR :: 0.000020 - Loss :: 0.0999:  86%|████████▋ | 173/200 [1:19:41<12:23, 27.55s/it]=>> [Epoch 000] Global Step 000173 =>> LR :: 0.000020 - Loss :: 0.0999:  87%|████████▋ | 174/200 [1:20:08<11:54, 27.49s/it]=>> [Epoch 000] Global Step 000174 =>> LR :: 0.000020 - Loss :: 0.0762:  87%|████████▋ | 174/200 [1:20:08<11:54, 27.49s/it]=>> [Epoch 000] Global Step 000174 =>> LR :: 0.000020 - Loss :: 0.0762:  88%|████████▊ | 175/200 [1:20:35<11:25, 27.42s/it]=>> [Epoch 000] Global Step 000175 =>> LR :: 0.000020 - Loss :: 0.0693:  88%|████████▊ | 175/200 [1:20:35<11:25, 27.42s/it]=>> [Epoch 000] Global Step 000175 =>> LR :: 0.000020 - Loss :: 0.0693:  88%|████████▊ | 176/200 [1:21:03<10:59, 27.50s/it]=>> [Epoch 000] Global Step 000176 =>> LR :: 0.000020 - Loss :: 0.0711:  88%|████████▊ | 176/200 [1:21:03<10:59, 27.50s/it]=>> [Epoch 000] Global Step 000176 =>> LR :: 0.000020 - Loss :: 0.0711:  88%|████████▊ | 177/200 [1:21:30<10:29, 27.37s/it]=>> [Epoch 000] Global Step 000177 =>> LR :: 0.000020 - Loss :: 0.0680:  88%|████████▊ | 177/200 [1:21:30<10:29, 27.37s/it]=>> [Epoch 000] Global Step 000177 =>> LR :: 0.000020 - Loss :: 0.0680:  89%|████████▉ | 178/200 [1:21:58<10:03, 27.45s/it]=>> [Epoch 000] Global Step 000178 =>> LR :: 0.000020 - Loss :: 0.0743:  89%|████████▉ | 178/200 [1:21:58<10:03, 27.45s/it]=>> [Epoch 000] Global Step 000178 =>> LR :: 0.000020 - Loss :: 0.0743:  90%|████████▉ | 179/200 [1:22:25<09:35, 27.40s/it]=>> [Epoch 000] Global Step 000179 =>> LR :: 0.000020 - Loss :: 0.0775:  90%|████████▉ | 179/200 [1:22:25<09:35, 27.40s/it]=>> [Epoch 000] Global Step 000179 =>> LR :: 0.000020 - Loss :: 0.0775:  90%|█████████ | 180/200 [1:22:53<09:09, 27.50s/it]=>> [Epoch 000] Global Step 000180 =>> LR :: 0.000020 - Loss :: 0.0755:  90%|█████████ | 180/200 [1:22:53<09:09, 27.50s/it]=>> [Epoch 000] Global Step 000180 =>> LR :: 0.000020 - Loss :: 0.0755:  90%|█████████ | 181/200 [1:23:20<08:41, 27.45s/it]=>> [Epoch 000] Global Step 000181 =>> LR :: 0.000020 - Loss :: 0.0926:  90%|█████████ | 181/200 [1:23:20<08:41, 27.45s/it]=>> [Epoch 000] Global Step 000181 =>> LR :: 0.000020 - Loss :: 0.0926:  91%|█████████ | 182/200 [1:23:48<08:13, 27.43s/it]=>> [Epoch 000] Global Step 000182 =>> LR :: 0.000020 - Loss :: 0.0780:  91%|█████████ | 182/200 [1:23:48<08:13, 27.43s/it]=>> [Epoch 000] Global Step 000182 =>> LR :: 0.000020 - Loss :: 0.0780:  92%|█████████▏| 183/200 [1:24:15<07:47, 27.52s/it]=>> [Epoch 000] Global Step 000183 =>> LR :: 0.000020 - Loss :: 0.1031:  92%|█████████▏| 183/200 [1:24:15<07:47, 27.52s/it]=>> [Epoch 000] Global Step 000183 =>> LR :: 0.000020 - Loss :: 0.1031:  92%|█████████▏| 184/200 [1:24:43<07:20, 27.51s/it]=>> [Epoch 000] Global Step 000184 =>> LR :: 0.000020 - Loss :: 0.1053:  92%|█████████▏| 184/200 [1:24:43<07:20, 27.51s/it]=>> [Epoch 000] Global Step 000184 =>> LR :: 0.000020 - Loss :: 0.1053:  92%|█████████▎| 185/200 [1:25:10<06:53, 27.53s/it]=>> [Epoch 000] Global Step 000185 =>> LR :: 0.000020 - Loss :: 0.0925:  92%|█████████▎| 185/200 [1:25:10<06:53, 27.53s/it]=>> [Epoch 000] Global Step 000185 =>> LR :: 0.000020 - Loss :: 0.0925:  93%|█████████▎| 186/200 [1:25:38<06:26, 27.58s/it]=>> [Epoch 000] Global Step 000186 =>> LR :: 0.000020 - Loss :: 0.0974:  93%|█████████▎| 186/200 [1:25:38<06:26, 27.58s/it]=>> [Epoch 000] Global Step 000186 =>> LR :: 0.000020 - Loss :: 0.0974:  94%|█████████▎| 187/200 [1:26:06<05:58, 27.57s/it]=>> [Epoch 000] Global Step 000187 =>> LR :: 0.000020 - Loss :: 0.1170:  94%|█████████▎| 187/200 [1:26:06<05:58, 27.57s/it]=>> [Epoch 000] Global Step 000187 =>> LR :: 0.000020 - Loss :: 0.1170:  94%|█████████▍| 188/200 [1:26:33<05:30, 27.52s/it]=>> [Epoch 000] Global Step 000188 =>> LR :: 0.000020 - Loss :: 0.1087:  94%|█████████▍| 188/200 [1:26:33<05:30, 27.52s/it]=>> [Epoch 000] Global Step 000188 =>> LR :: 0.000020 - Loss :: 0.1087:  94%|█████████▍| 189/200 [1:27:00<05:02, 27.50s/it]=>> [Epoch 000] Global Step 000189 =>> LR :: 0.000020 - Loss :: 0.1272:  94%|█████████▍| 189/200 [1:27:00<05:02, 27.50s/it]=>> [Epoch 000] Global Step 000189 =>> LR :: 0.000020 - Loss :: 0.1272:  95%|█████████▌| 190/200 [1:27:28<04:35, 27.59s/it]=>> [Epoch 000] Global Step 000190 =>> LR :: 0.000020 - Loss :: 0.0982:  95%|█████████▌| 190/200 [1:27:28<04:35, 27.59s/it]=>> [Epoch 000] Global Step 000190 =>> LR :: 0.000020 - Loss :: 0.0982:  96%|█████████▌| 191/200 [1:27:56<04:08, 27.62s/it]=>> [Epoch 000] Global Step 000191 =>> LR :: 0.000020 - Loss :: 0.0697:  96%|█████████▌| 191/200 [1:27:56<04:08, 27.62s/it]=>> [Epoch 000] Global Step 000191 =>> LR :: 0.000020 - Loss :: 0.0697:  96%|█████████▌| 192/200 [1:28:24<03:41, 27.72s/it]=>> [Epoch 000] Global Step 000192 =>> LR :: 0.000020 - Loss :: 0.0690:  96%|█████████▌| 192/200 [1:28:24<03:41, 27.72s/it]=>> [Epoch 000] Global Step 000192 =>> LR :: 0.000020 - Loss :: 0.0690:  96%|█████████▋| 193/200 [1:28:51<03:13, 27.65s/it]=>> [Epoch 000] Global Step 000193 =>> LR :: 0.000020 - Loss :: 0.0818:  96%|█████████▋| 193/200 [1:28:51<03:13, 27.65s/it]=>> [Epoch 000] Global Step 000193 =>> LR :: 0.000020 - Loss :: 0.0818:  97%|█████████▋| 194/200 [1:29:19<02:45, 27.65s/it]=>> [Epoch 000] Global Step 000194 =>> LR :: 0.000020 - Loss :: 0.0618:  97%|█████████▋| 194/200 [1:29:19<02:45, 27.65s/it]=>> [Epoch 000] Global Step 000194 =>> LR :: 0.000020 - Loss :: 0.0618:  98%|█████████▊| 195/200 [1:29:47<02:18, 27.68s/it]=>> [Epoch 000] Global Step 000195 =>> LR :: 0.000020 - Loss :: 0.0610:  98%|█████████▊| 195/200 [1:29:47<02:18, 27.68s/it]=>> [Epoch 000] Global Step 000195 =>> LR :: 0.000020 - Loss :: 0.0610:  98%|█████████▊| 196/200 [1:30:15<01:50, 27.75s/it]=>> [Epoch 000] Global Step 000196 =>> LR :: 0.000020 - Loss :: 0.0518:  98%|█████████▊| 196/200 [1:30:15<01:50, 27.75s/it]=>> [Epoch 000] Global Step 000196 =>> LR :: 0.000020 - Loss :: 0.0518:  98%|█████████▊| 197/200 [1:30:42<01:23, 27.74s/it]=>> [Epoch 000] Global Step 000197 =>> LR :: 0.000020 - Loss :: 0.0613:  98%|█████████▊| 197/200 [1:30:42<01:23, 27.74s/it]=>> [Epoch 000] Global Step 000197 =>> LR :: 0.000020 - Loss :: 0.0613:  99%|█████████▉| 198/200 [1:31:10<00:55, 27.63s/it]=>> [Epoch 000] Global Step 000198 =>> LR :: 0.000020 - Loss :: 0.0657:  99%|█████████▉| 198/200 [1:31:10<00:55, 27.63s/it]=>> [Epoch 000] Global Step 000198 =>> LR :: 0.000020 - Loss :: 0.0657: 100%|█████████▉| 199/200 [1:31:37<00:27, 27.64s/it]=>> [Epoch 000] Global Step 000199 =>> LR :: 0.000020 - Loss :: 0.0525: 100%|█████████▉| 199/200 [1:31:37<00:27, 27.64s/it]                                                                                                                           wandb:                                                                                
wandb: 
wandb: Run history:
wandb:          VLA Train/Action Token Accuracy ▁▁▁▃▃▄▄▄▄▄▄▄▄▄▄▅▄▄▅▅▅▆▆▆▆▇▇▆▆▆▆▆▇▇▇▇▆▆▇█
wandb:             VLA Train/CoT Token Accuracy ▁▂▂▂▃▄▄▅▅▆▆▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇█
wandb:                          VLA Train/Epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                        VLA Train/L1 Loss █▅▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                  VLA Train/Learning Rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                           VLA Train/Loss █▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                     VLA Train/Loss (Raw) █▇▇▅▅▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▃▂▃▂▂▂▂▂▂▁▂▂▁▂▂▁▁▁
wandb:                           VLA Train/Step ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇█
wandb:                      VLA Train/Step Time ▆▃▃▄▂▃▃█▃▁▂▂▃▃▃▅▃▁▃▂▂▂▂▄▂▃▂▃▃▂▄▂▂▁▃▁▃▄▄▄
wandb:            VLA Train/action_tag_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████
wandb:  VLA Train/gripper position_tag_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█████████████████████
wandb:    VLA Train/move reasoning_tag_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████
wandb:              VLA Train/move_tag_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁███████████████████████
wandb:              VLA Train/plan_tag_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█████████████████████████
wandb: VLA Train/subtask reasoning_tag_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████
wandb:           VLA Train/subtask_tag_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█████████████████
wandb:              VLA Train/task_tag_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██████████████████████
wandb:   VLA Train/visible objects_tag_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁███████████████████
wandb: 
wandb: Run summary:
wandb:          VLA Train/Action Token Accuracy 0.84821
wandb:             VLA Train/CoT Token Accuracy 0.97972
wandb:                          VLA Train/Epoch 0
wandb:                        VLA Train/L1 Loss 0.0278
wandb:                  VLA Train/Learning Rate 2e-05
wandb:                           VLA Train/Loss 0.05965
wandb:                     VLA Train/Loss (Raw) 0.05965
wandb:                           VLA Train/Step 200
wandb:                      VLA Train/Step Time 27.92128
wandb:            VLA Train/action_tag_accuracy 0.25155
wandb:  VLA Train/gripper position_tag_accuracy 0.86693
wandb:    VLA Train/move reasoning_tag_accuracy 0.9476
wandb:              VLA Train/move_tag_accuracy 0.53333
wandb:              VLA Train/plan_tag_accuracy 1
wandb: VLA Train/subtask reasoning_tag_accuracy 0.97089
wandb:           VLA Train/subtask_tag_accuracy 1
wandb:              VLA Train/task_tag_accuracy 1
wandb:   VLA Train/visible objects_tag_accuracy 0.94794
wandb: 
wandb: 🚀 View run prism-dinosiglip-224px+mx-libero-90+n1+b16+x7 at: https://wandb.ai/solace/ecot_reproduce_libero/runs/ez8jqye1
wandb: ⭐️ View project at: https://wandb.ai/solace/ecot_reproduce_libero
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /srv/rl2-lab/flash7/rbansal66/embodied-CoT/runs/prism-dinosiglip-224px+mx-libero-90+n1+b16+x7/wandb/run-20250607_222610-ez8jqye1/logs
