2025-06-07 22:22:59.306561: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.306560: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.306564: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.306617: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.306618: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.306620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.307449: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.307447: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.307451: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.308495: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.308550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.309373: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.310703: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.310725: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.311214: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.311661: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:22:59.311661: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:22:59.311683: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:22:59.313662: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:22:59.314646: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:22:59.657261: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.657327: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.658046: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.662141: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:22:59.695016: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.695046: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.695750: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.699674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:22:59.721885: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 22:22:59.721927: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 22:22:59.722910: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 22:22:59.726959: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 22:23:00.442460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-07 22:23:00.442460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-07 22:23:00.442474: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-07 22:23:00.442499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-07 22:23:00.442499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-07 22:23:00.451037: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-07 22:23:00.454659: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-07 22:23:00.507825: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:13<00:13, 13.73s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:14<00:14, 14.69s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:16<00:16, 16.13s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:14<00:14, 14.75s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:19<00:00,  8.78s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:19<00:00,  9.52s/it]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:15<00:15, 15.71s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:16<00:16, 16.56s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:20<00:00,  9.51s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:20<00:00, 10.29s/it]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:15<00:15, 15.39s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:20<00:00,  9.04s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:20<00:00, 10.10s/it]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:12<00:12, 12.96s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:19<00:00,  8.92s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:19<00:00,  9.80s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:19<00:00,  8.62s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:19<00:00,  9.68s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:19<00:00,  8.68s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:19<00:00,  9.86s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:18<00:00,  8.19s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:18<00:00,  9.27s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:15<00:00,  6.82s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:15<00:00,  7.74s/it]
2025-06-07 22:25:43.912706: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:44.354617: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:45.094321: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:45.526036: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:45.629774: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:46.094275: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:46.164846: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:46.631010: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:46.746054: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:47.222667: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:47.474249: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:47.571765: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:47.688662: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:47.965653: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:48.031083: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:48.156087: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:25:59.127487: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:26:00.445532: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:26:01.136965: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:26:02.101188: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:26:02.199092: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:26:03.137994: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:26:03.304606: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-07 22:26:03.667122: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
wandb: Currently logged in as: solace to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /srv/rl2-lab/flash7/rbansal66/embodied-CoT/runs/prism-dinosiglip-224px+mx-libero-90+n1+b16+x7/wandb/run-20250607_222610-ez8jqye1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prism-dinosiglip-224px+mx-libero-90+n1+b16+x7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/solace/ecot_reproduce_libero
wandb: üöÄ View run at https://wandb.ai/solace/ecot_reproduce_libero/runs/ez8jqye1
=>> [Epoch 000] Global Step 000000 =>> LR :: 0.000000:   0%|          | 0/200 [00:00<?, ?it/s]=>> [Epoch 000] Global Step 000000 =>> LR :: 0.000000:   0%|          | 1/200 [00:33<1:52:26, 33.90s/it]=>> [Epoch 000] Global Step 000001 =>> LR :: 0.000020 - Loss :: 2.0336:   0%|          | 1/200 [00:33<1:52:26, 33.90s/it]=>> [Epoch 000] Global Step 000001 =>> LR :: 0.000020 - Loss :: 2.0336:   1%|          | 2/200 [01:01<1:38:43, 29.92s/it]=>> [Epoch 000] Global Step 000002 =>> LR :: 0.000020 - Loss :: 1.2111:   1%|          | 2/200 [01:01<1:38:43, 29.92s/it]=>> [Epoch 000] Global Step 000002 =>> LR :: 0.000020 - Loss :: 1.2111:   2%|‚ñè         | 3/200 [01:29<1:35:41, 29.14s/it]=>> [Epoch 000] Global Step 000003 =>> LR :: 0.000020 - Loss :: 0.6832:   2%|‚ñè         | 3/200 [01:29<1:35:41, 29.14s/it]=>> [Epoch 000] Global Step 000003 =>> LR :: 0.000020 - Loss :: 0.6832:   2%|‚ñè         | 4/200 [01:56<1:33:08, 28.51s/it]=>> [Epoch 000] Global Step 000004 =>> LR :: 0.000020 - Loss :: 0.5582:   2%|‚ñè         | 4/200 [01:56<1:33:08, 28.51s/it]=>> [Epoch 000] Global Step 000004 =>> LR :: 0.000020 - Loss :: 0.5582:   2%|‚ñé         | 5/200 [02:24<1:32:07, 28.34s/it]=>> [Epoch 000] Global Step 000005 =>> LR :: 0.000020 - Loss :: 0.4810:   2%|‚ñé         | 5/200 [02:24<1:32:07, 28.34s/it]=>> [Epoch 000] Global Step 000005 =>> LR :: 0.000020 - Loss :: 0.4810:   3%|‚ñé         | 6/200 [02:52<1:31:01, 28.15s/it]=>> [Epoch 000] Global Step 000006 =>> LR :: 0.000020 - Loss :: 0.4492:   3%|‚ñé         | 6/200 [02:52<1:31:01, 28.15s/it]=>> [Epoch 000] Global Step 000006 =>> LR :: 0.000020 - Loss :: 0.4492:   4%|‚ñé         | 7/200 [03:20<1:29:55, 27.96s/it]=>> [Epoch 000] Global Step 000007 =>> LR :: 0.000020 - Loss :: 0.4772:   4%|‚ñé         | 7/200 [03:20<1:29:55, 27.96s/it]=>> [Epoch 000] Global Step 000007 =>> LR :: 0.000020 - Loss :: 0.4772:   4%|‚ñç         | 8/200 [03:47<1:29:11, 27.87s/it]=>> [Epoch 000] Global Step 000008 =>> LR :: 0.000020 - Loss :: 0.4691:   4%|‚ñç         | 8/200 [03:47<1:29:11, 27.87s/it]=>> [Epoch 000] Global Step 000008 =>> LR :: 0.000020 - Loss :: 0.4691:   4%|‚ñç         | 9/200 [04:15<1:28:51, 27.91s/it]=>> [Epoch 000] Global Step 000009 =>> LR :: 0.000020 - Loss :: 0.3981:   4%|‚ñç         | 9/200 [04:15<1:28:51, 27.91s/it]=>> [Epoch 000] Global Step 000009 =>> LR :: 0.000020 - Loss :: 0.3981:   5%|‚ñå         | 10/200 [04:43<1:28:00, 27.79s/it]=>> [Epoch 000] Global Step 000010 =>> LR :: 0.000020 - Loss :: 0.3722:   5%|‚ñå         | 10/200 [04:43<1:28:00, 27.79s/it]=>> [Epoch 000] Global Step 000010 =>> LR :: 0.000020 - Loss :: 0.3722:   6%|‚ñå         | 11/200 [05:10<1:27:04, 27.64s/it]=>> [Epoch 000] Global Step 000011 =>> LR :: 0.000020 - Loss :: 0.3420:   6%|‚ñå         | 11/200 [05:10<1:27:04, 27.64s/it]=>> [Epoch 000] Global Step 000011 =>> LR :: 0.000020 - Loss :: 0.3420:   6%|‚ñå         | 12/200 [05:38<1:26:38, 27.65s/it]=>> [Epoch 000] Global Step 000012 =>> LR :: 0.000020 - Loss :: 0.3449:   6%|‚ñå         | 12/200 [05:38<1:26:38, 27.65s/it]=>> [Epoch 000] Global Step 000012 =>> LR :: 0.000020 - Loss :: 0.3449:   6%|‚ñã         | 13/200 [06:06<1:26:15, 27.68s/it]=>> [Epoch 000] Global Step 000013 =>> LR :: 0.000020 - Loss :: 0.3300:   6%|‚ñã         | 13/200 [06:06<1:26:15, 27.68s/it]=>> [Epoch 000] Global Step 000013 =>> LR :: 0.000020 - Loss :: 0.3300:   7%|‚ñã         | 14/200 [06:34<1:26:00, 27.74s/it]=>> [Epoch 000] Global Step 000014 =>> LR :: 0.000020 - Loss :: 0.3248:   7%|‚ñã         | 14/200 [06:34<1:26:00, 27.74s/it]=>> [Epoch 000] Global Step 000014 =>> LR :: 0.000020 - Loss :: 0.3248:   8%|‚ñä         | 15/200 [07:01<1:25:19, 27.67s/it]=>> [Epoch 000] Global Step 000015 =>> LR :: 0.000020 - Loss :: 0.3065:   8%|‚ñä         | 15/200 [07:01<1:25:19, 27.67s/it]=>> [Epoch 000] Global Step 000015 =>> LR :: 0.000020 - Loss :: 0.3065:   8%|‚ñä         | 16/200 [07:29<1:24:53, 27.68s/it]=>> [Epoch 000] Global Step 000016 =>> LR :: 0.000020 - Loss :: 0.3097:   8%|‚ñä         | 16/200 [07:29<1:24:53, 27.68s/it]=>> [Epoch 000] Global Step 000016 =>> LR :: 0.000020 - Loss :: 0.3097:   8%|‚ñä         | 17/200 [07:56<1:24:17, 27.64s/it]=>> [Epoch 000] Global Step 000017 =>> LR :: 0.000020 - Loss :: 0.2970:   8%|‚ñä         | 17/200 [07:56<1:24:17, 27.64s/it]=>> [Epoch 000] Global Step 000017 =>> LR :: 0.000020 - Loss :: 0.2970:   9%|‚ñâ         | 18/200 [08:24<1:23:55, 27.67s/it]=>> [Epoch 000] Global Step 000018 =>> LR :: 0.000020 - Loss :: 0.3085:   9%|‚ñâ         | 18/200 [08:24<1:23:55, 27.67s/it]=>> [Epoch 000] Global Step 000018 =>> LR :: 0.000020 - Loss :: 0.3085:  10%|‚ñâ         | 19/200 [08:51<1:23:18, 27.61s/it]=>> [Epoch 000] Global Step 000019 =>> LR :: 0.000020 - Loss :: 0.2859:  10%|‚ñâ         | 19/200 [08:51<1:23:18, 27.61s/it]=>> [Epoch 000] Global Step 000019 =>> LR :: 0.000020 - Loss :: 0.2859:  10%|‚ñà         | 20/200 [09:19<1:22:47, 27.60s/it]=>> [Epoch 000] Global Step 000020 =>> LR :: 0.000020 - Loss :: 0.3502:  10%|‚ñà         | 20/200 [09:19<1:22:47, 27.60s/it]=>> [Epoch 000] Global Step 000020 =>> LR :: 0.000020 - Loss :: 0.3502:  10%|‚ñà         | 21/200 [09:46<1:22:12, 27.56s/it]=>> [Epoch 000] Global Step 000021 =>> LR :: 0.000020 - Loss :: 0.3033:  10%|‚ñà         | 21/200 [09:46<1:22:12, 27.56s/it]=>> [Epoch 000] Global Step 000021 =>> LR :: 0.000020 - Loss :: 0.3033:  11%|‚ñà         | 22/200 [10:14<1:21:44, 27.55s/it]=>> [Epoch 000] Global Step 000022 =>> LR :: 0.000020 - Loss :: 0.2864:  11%|‚ñà         | 22/200 [10:14<1:21:44, 27.55s/it]=>> [Epoch 000] Global Step 000022 =>> LR :: 0.000020 - Loss :: 0.2864:  12%|‚ñà‚ñè        | 23/200 [10:42<1:21:22, 27.58s/it]=>> [Epoch 000] Global Step 000023 =>> LR :: 0.000020 - Loss :: 0.2974:  12%|‚ñà‚ñè        | 23/200 [10:42<1:21:22, 27.58s/it]=>> [Epoch 000] Global Step 000023 =>> LR :: 0.000020 - Loss :: 0.2974:  12%|‚ñà‚ñè        | 24/200 [11:10<1:21:07, 27.66s/it]=>> [Epoch 000] Global Step 000024 =>> LR :: 0.000020 - Loss :: 0.2868:  12%|‚ñà‚ñè        | 24/200 [11:10<1:21:07, 27.66s/it]=>> [Epoch 000] Global Step 000024 =>> LR :: 0.000020 - Loss :: 0.2868:  12%|‚ñà‚ñé        | 25/200 [11:37<1:20:52, 27.73s/it]=>> [Epoch 000] Global Step 000025 =>> LR :: 0.000020 - Loss :: 0.2492:  12%|‚ñà‚ñé        | 25/200 [11:37<1:20:52, 27.73s/it]=>> [Epoch 000] Global Step 000025 =>> LR :: 0.000020 - Loss :: 0.2492:  13%|‚ñà‚ñé        | 26/200 [12:05<1:20:13, 27.67s/it]=>> [Epoch 000] Global Step 000026 =>> LR :: 0.000020 - Loss :: 0.2519:  13%|‚ñà‚ñé        | 26/200 [12:05<1:20:13, 27.67s/it]=>> [Epoch 000] Global Step 000026 =>> LR :: 0.000020 - Loss :: 0.2519:  14%|‚ñà‚ñé        | 27/200 [12:33<1:19:41, 27.64s/it]=>> [Epoch 000] Global Step 000027 =>> LR :: 0.000020 - Loss :: 0.2232:  14%|‚ñà‚ñé        | 27/200 [12:33<1:19:41, 27.64s/it]=>> [Epoch 000] Global Step 000027 =>> LR :: 0.000020 - Loss :: 0.2232:  14%|‚ñà‚ñç        | 28/200 [13:00<1:18:58, 27.55s/it]=>> [Epoch 000] Global Step 000028 =>> LR :: 0.000020 - Loss :: 0.2502:  14%|‚ñà‚ñç        | 28/200 [13:00<1:18:58, 27.55s/it]=>> [Epoch 000] Global Step 000028 =>> LR :: 0.000020 - Loss :: 0.2502:  14%|‚ñà‚ñç        | 29/200 [13:27<1:18:22, 27.50s/it]=>> [Epoch 000] Global Step 000029 =>> LR :: 0.000020 - Loss :: 0.2547:  14%|‚ñà‚ñç        | 29/200 [13:27<1:18:22, 27.50s/it]=>> [Epoch 000] Global Step 000029 =>> LR :: 0.000020 - Loss :: 0.2547:  15%|‚ñà‚ñå        | 30/200 [13:55<1:17:59, 27.52s/it]=>> [Epoch 000] Global Step 000030 =>> LR :: 0.000020 - Loss :: 0.2498:  15%|‚ñà‚ñå        | 30/200 [13:55<1:17:59, 27.52s/it]=>> [Epoch 000] Global Step 000030 =>> LR :: 0.000020 - Loss :: 0.2498:  16%|‚ñà‚ñå        | 31/200 [14:22<1:17:17, 27.44s/it]=>> [Epoch 000] Global Step 000031 =>> LR :: 0.000020 - Loss :: 0.2377:  16%|‚ñà‚ñå        | 31/200 [14:22<1:17:17, 27.44s/it]=>> [Epoch 000] Global Step 000031 =>> LR :: 0.000020 - Loss :: 0.2377:  16%|‚ñà‚ñå        | 32/200 [14:49<1:16:31, 27.33s/it]=>> [Epoch 000] Global Step 000032 =>> LR :: 0.000020 - Loss :: 0.2511:  16%|‚ñà‚ñå        | 32/200 [14:49<1:16:31, 27.33s/it]=>> [Epoch 000] Global Step 000032 =>> LR :: 0.000020 - Loss :: 0.2511:  16%|‚ñà‚ñã        | 33/200 [15:17<1:16:22, 27.44s/it]=>> [Epoch 000] Global Step 000033 =>> LR :: 0.000020 - Loss :: 0.2450:  16%|‚ñà‚ñã        | 33/200 [15:17<1:16:22, 27.44s/it]=>> [Epoch 000] Global Step 000033 =>> LR :: 0.000020 - Loss :: 0.2450:  17%|‚ñà‚ñã        | 34/200 [15:45<1:16:07, 27.52s/it]=>> [Epoch 000] Global Step 000034 =>> LR :: 0.000020 - Loss :: 0.2430:  17%|‚ñà‚ñã        | 34/200 [15:45<1:16:07, 27.52s/it]=>> [Epoch 000] Global Step 000034 =>> LR :: 0.000020 - Loss :: 0.2430:  18%|‚ñà‚ñä        | 35/200 [16:12<1:15:33, 27.48s/it]=>> [Epoch 000] Global Step 000035 =>> LR :: 0.000020 - Loss :: 0.2315:  18%|‚ñà‚ñä        | 35/200 [16:12<1:15:33, 27.48s/it]=>> [Epoch 000] Global Step 000035 =>> LR :: 0.000020 - Loss :: 0.2315:  18%|‚ñà‚ñä        | 36/200 [16:40<1:15:13, 27.52s/it]=>> [Epoch 000] Global Step 000036 =>> LR :: 0.000020 - Loss :: 0.2672:  18%|‚ñà‚ñä        | 36/200 [16:40<1:15:13, 27.52s/it]=>> [Epoch 000] Global Step 000036 =>> LR :: 0.000020 - Loss :: 0.2672:  18%|‚ñà‚ñä        | 37/200 [17:07<1:14:52, 27.56s/it]=>> [Epoch 000] Global Step 000037 =>> LR :: 0.000020 - Loss :: 0.2330:  18%|‚ñà‚ñä        | 37/200 [17:07<1:14:52, 27.56s/it]=>> [Epoch 000] Global Step 000037 =>> LR :: 0.000020 - Loss :: 0.2330:  19%|‚ñà‚ñâ        | 38/200 [17:35<1:14:17, 27.52s/it]=>> [Epoch 000] Global Step 000038 =>> LR :: 0.000020 - Loss :: 0.2406:  19%|‚ñà‚ñâ        | 38/200 [17:35<1:14:17, 27.52s/it]=>> [Epoch 000] Global Step 000038 =>> LR :: 0.000020 - Loss :: 0.2406:  20%|‚ñà‚ñâ        | 39/200 [18:02<1:13:36, 27.43s/it]=>> [Epoch 000] Global Step 000039 =>> LR :: 0.000020 - Loss :: 0.2737:  20%|‚ñà‚ñâ        | 39/200 [18:02<1:13:36, 27.43s/it]=>> [Epoch 000] Global Step 000039 =>> LR :: 0.000020 - Loss :: 0.2737:  20%|‚ñà‚ñà        | 40/200 [18:29<1:13:19, 27.50s/it]=>> [Epoch 000] Global Step 000040 =>> LR :: 0.000020 - Loss :: 0.2245:  20%|‚ñà‚ñà        | 40/200 [18:29<1:13:19, 27.50s/it]=>> [Epoch 000] Global Step 000040 =>> LR :: 0.000020 - Loss :: 0.2245:  20%|‚ñà‚ñà        | 41/200 [18:57<1:13:10, 27.61s/it]=>> [Epoch 000] Global Step 000041 =>> LR :: 0.000020 - Loss :: 0.2332:  20%|‚ñà‚ñà        | 41/200 [18:57<1:13:10, 27.61s/it]=>> [Epoch 000] Global Step 000041 =>> LR :: 0.000020 - Loss :: 0.2332:  21%|‚ñà‚ñà        | 42/200 [19:25<1:12:41, 27.60s/it]=>> [Epoch 000] Global Step 000042 =>> LR :: 0.000020 - Loss :: 0.2156:  21%|‚ñà‚ñà        | 42/200 [19:25<1:12:41, 27.60s/it]=>> [Epoch 000] Global Step 000042 =>> LR :: 0.000020 - Loss :: 0.2156:  22%|‚ñà‚ñà‚ñè       | 43/200 [19:53<1:12:18, 27.64s/it]=>> [Epoch 000] Global Step 000043 =>> LR :: 0.000020 - Loss :: 0.2419:  22%|‚ñà‚ñà‚ñè       | 43/200 [19:53<1:12:18, 27.64s/it]=>> [Epoch 000] Global Step 000043 =>> LR :: 0.000020 - Loss :: 0.2419:  22%|‚ñà‚ñà‚ñè       | 44/200 [20:20<1:11:49, 27.62s/it]=>> [Epoch 000] Global Step 000044 =>> LR :: 0.000020 - Loss :: 0.2347:  22%|‚ñà‚ñà‚ñè       | 44/200 [20:20<1:11:49, 27.62s/it]=>> [Epoch 000] Global Step 000044 =>> LR :: 0.000020 - Loss :: 0.2347:  22%|‚ñà‚ñà‚ñé       | 45/200 [20:48<1:11:22, 27.63s/it]=>> [Epoch 000] Global Step 000045 =>> LR :: 0.000020 - Loss :: 0.2347:  22%|‚ñà‚ñà‚ñé       | 45/200 [20:48<1:11:22, 27.63s/it]=>> [Epoch 000] Global Step 000045 =>> LR :: 0.000020 - Loss :: 0.2347:  23%|‚ñà‚ñà‚ñé       | 46/200 [21:17<1:11:42, 27.94s/it]=>> [Epoch 000] Global Step 000046 =>> LR :: 0.000020 - Loss :: 0.2176:  23%|‚ñà‚ñà‚ñé       | 46/200 [21:17<1:11:42, 27.94s/it]=>> [Epoch 000] Global Step 000046 =>> LR :: 0.000020 - Loss :: 0.2176:  24%|‚ñà‚ñà‚ñé       | 47/200 [21:44<1:10:58, 27.83s/it]=>> [Epoch 000] Global Step 000047 =>> LR :: 0.000020 - Loss :: 0.2014:  24%|‚ñà‚ñà‚ñé       | 47/200 [21:44<1:10:58, 27.83s/it]=>> [Epoch 000] Global Step 000047 =>> LR :: 0.000020 - Loss :: 0.2014:  24%|‚ñà‚ñà‚ñç       | 48/200 [22:12<1:10:21, 27.77s/it]=>> [Epoch 000] Global Step 000048 =>> LR :: 0.000020 - Loss :: 0.2257:  24%|‚ñà‚ñà‚ñç       | 48/200 [22:12<1:10:21, 27.77s/it]=>> [Epoch 000] Global Step 000048 =>> LR :: 0.000020 - Loss :: 0.2257:  24%|‚ñà‚ñà‚ñç       | 49/200 [22:39<1:09:27, 27.60s/it]=>> [Epoch 000] Global Step 000049 =>> LR :: 0.000020 - Loss :: 0.2367:  24%|‚ñà‚ñà‚ñç       | 49/200 [22:39<1:09:27, 27.60s/it]=>> [Epoch 000] Global Step 000049 =>> LR :: 0.000020 - Loss :: 0.2367:  25%|‚ñà‚ñà‚ñå       | 50/200 [23:06<1:08:46, 27.51s/it]=>> [Epoch 000] Global Step 000050 =>> LR :: 0.000020 - Loss :: 0.2169:  25%|‚ñà‚ñà‚ñå       | 50/200 [23:06<1:08:46, 27.51s/it]=>> [Epoch 000] Global Step 000050 =>> LR :: 0.000020 - Loss :: 0.2169:  26%|‚ñà‚ñà‚ñå       | 51/200 [23:34<1:08:25, 27.55s/it]=>> [Epoch 000] Global Step 000051 =>> LR :: 0.000020 - Loss :: 0.1997:  26%|‚ñà‚ñà‚ñå       | 51/200 [23:34<1:08:25, 27.55s/it]=>> [Epoch 000] Global Step 000051 =>> LR :: 0.000020 - Loss :: 0.1997:  26%|‚ñà‚ñà‚ñå       | 52/200 [24:01<1:07:47, 27.49s/it]=>> [Epoch 000] Global Step 000052 =>> LR :: 0.000020 - Loss :: 0.2044:  26%|‚ñà‚ñà‚ñå       | 52/200 [24:01<1:07:47, 27.49s/it]=>> [Epoch 000] Global Step 000052 =>> LR :: 0.000020 - Loss :: 0.2044:  26%|‚ñà‚ñà‚ñã       | 53/200 [24:29<1:07:26, 27.53s/it]=>> [Epoch 000] Global Step 000053 =>> LR :: 0.000020 - Loss :: 0.2109:  26%|‚ñà‚ñà‚ñã       | 53/200 [24:29<1:07:26, 27.53s/it]=>> [Epoch 000] Global Step 000053 =>> LR :: 0.000020 - Loss :: 0.2109:  27%|‚ñà‚ñà‚ñã       | 54/200 [24:57<1:07:05, 27.57s/it]=>> [Epoch 000] Global Step 000054 =>> LR :: 0.000020 - Loss :: 0.1993:  27%|‚ñà‚ñà‚ñã       | 54/200 [24:57<1:07:05, 27.57s/it]=>> [Epoch 000] Global Step 000054 =>> LR :: 0.000020 - Loss :: 0.1993:  28%|‚ñà‚ñà‚ñä       | 55/200 [25:24<1:06:31, 27.53s/it]=>> [Epoch 000] Global Step 000055 =>> LR :: 0.000020 - Loss :: 0.1994:  28%|‚ñà‚ñà‚ñä       | 55/200 [25:24<1:06:31, 27.53s/it]=>> [Epoch 000] Global Step 000055 =>> LR :: 0.000020 - Loss :: 0.1994:  28%|‚ñà‚ñà‚ñä       | 56/200 [25:51<1:06:03, 27.52s/it]=>> [Epoch 000] Global Step 000056 =>> LR :: 0.000020 - Loss :: 0.1874:  28%|‚ñà‚ñà‚ñä       | 56/200 [25:51<1:06:03, 27.52s/it]=>> [Epoch 000] Global Step 000056 =>> LR :: 0.000020 - Loss :: 0.1874:  28%|‚ñà‚ñà‚ñä       | 57/200 [26:19<1:05:39, 27.55s/it]=>> [Epoch 000] Global Step 000057 =>> LR :: 0.000020 - Loss :: 0.1785:  28%|‚ñà‚ñà‚ñä       | 57/200 [26:19<1:05:39, 27.55s/it]=>> [Epoch 000] Global Step 000057 =>> LR :: 0.000020 - Loss :: 0.1785:  29%|‚ñà‚ñà‚ñâ       | 58/200 [26:47<1:05:09, 27.53s/it]=>> [Epoch 000] Global Step 000058 =>> LR :: 0.000020 - Loss :: 0.1746:  29%|‚ñà‚ñà‚ñâ       | 58/200 [26:47<1:05:09, 27.53s/it]=>> [Epoch 000] Global Step 000058 =>> LR :: 0.000020 - Loss :: 0.1746:  30%|‚ñà‚ñà‚ñâ       | 59/200 [27:14<1:04:51, 27.60s/it]=>> [Epoch 000] Global Step 000059 =>> LR :: 0.000020 - Loss :: 0.1846:  30%|‚ñà‚ñà‚ñâ       | 59/200 [27:14<1:04:51, 27.60s/it]=>> [Epoch 000] Global Step 000059 =>> LR :: 0.000020 - Loss :: 0.1846:  30%|‚ñà‚ñà‚ñà       | 60/200 [27:42<1:04:08, 27.49s/it]=>> [Epoch 000] Global Step 000060 =>> LR :: 0.000020 - Loss :: 0.2153:  30%|‚ñà‚ñà‚ñà       | 60/200 [27:42<1:04:08, 27.49s/it]=>> [Epoch 000] Global Step 000060 =>> LR :: 0.000020 - Loss :: 0.2153:  30%|‚ñà‚ñà‚ñà       | 61/200 [28:09<1:03:34, 27.45s/it]=>> [Epoch 000] Global Step 000061 =>> LR :: 0.000020 - Loss :: 0.2024:  30%|‚ñà‚ñà‚ñà       | 61/200 [28:09<1:03:34, 27.45s/it]=>> [Epoch 000] Global Step 000061 =>> LR :: 0.000020 - Loss :: 0.2024:  31%|‚ñà‚ñà‚ñà       | 62/200 [28:37<1:03:17, 27.52s/it]=>> [Epoch 000] Global Step 000062 =>> LR :: 0.000020 - Loss :: 0.1985:  31%|‚ñà‚ñà‚ñà       | 62/200 [28:37<1:03:17, 27.52s/it]=>> [Epoch 000] Global Step 000062 =>> LR :: 0.000020 - Loss :: 0.1985:  32%|‚ñà‚ñà‚ñà‚ñè      | 63/200 [29:04<1:02:39, 27.44s/it]=>> [Epoch 000] Global Step 000063 =>> LR :: 0.000020 - Loss :: 0.1802:  32%|‚ñà‚ñà‚ñà‚ñè      | 63/200 [29:04<1:02:39, 27.44s/it]=>> [Epoch 000] Global Step 000063 =>> LR :: 0.000020 - Loss :: 0.1802:  32%|‚ñà‚ñà‚ñà‚ñè      | 64/200 [29:32<1:02:21, 27.51s/it]=>> [Epoch 000] Global Step 000064 =>> LR :: 0.000020 - Loss :: 0.1579:  32%|‚ñà‚ñà‚ñà‚ñè      | 64/200 [29:32<1:02:21, 27.51s/it]=>> [Epoch 000] Global Step 000064 =>> LR :: 0.000020 - Loss :: 0.1579:  32%|‚ñà‚ñà‚ñà‚ñé      | 65/200 [29:59<1:01:56, 27.53s/it]=>> [Epoch 000] Global Step 000065 =>> LR :: 0.000020 - Loss :: 0.1559:  32%|‚ñà‚ñà‚ñà‚ñé      | 65/200 [29:59<1:01:56, 27.53s/it]=>> [Epoch 000] Global Step 000065 =>> LR :: 0.000020 - Loss :: 0.1559:  33%|‚ñà‚ñà‚ñà‚ñé      | 66/200 [30:27<1:01:28, 27.52s/it]=>> [Epoch 000] Global Step 000066 =>> LR :: 0.000020 - Loss :: 0.1735:  33%|‚ñà‚ñà‚ñà‚ñé      | 66/200 [30:27<1:01:28, 27.52s/it]=>> [Epoch 000] Global Step 000066 =>> LR :: 0.000020 - Loss :: 0.1735:  34%|‚ñà‚ñà‚ñà‚ñé      | 67/200 [30:55<1:01:14, 27.63s/it]=>> [Epoch 000] Global Step 000067 =>> LR :: 0.000020 - Loss :: 0.1586:  34%|‚ñà‚ñà‚ñà‚ñé      | 67/200 [30:55<1:01:14, 27.63s/it]=>> [Epoch 000] Global Step 000067 =>> LR :: 0.000020 - Loss :: 0.1586:  34%|‚ñà‚ñà‚ñà‚ñç      | 68/200 [31:22<1:00:50, 27.65s/it]=>> [Epoch 000] Global Step 000068 =>> LR :: 0.000020 - Loss :: 0.1729:  34%|‚ñà‚ñà‚ñà‚ñç      | 68/200 [31:22<1:00:50, 27.65s/it]=>> [Epoch 000] Global Step 000068 =>> LR :: 0.000020 - Loss :: 0.1729:  34%|‚ñà‚ñà‚ñà‚ñç      | 69/200 [31:50<1:00:18, 27.62s/it]=>> [Epoch 000] Global Step 000069 =>> LR :: 0.000020 - Loss :: 0.1500:  34%|‚ñà‚ñà‚ñà‚ñç      | 69/200 [31:50<1:00:18, 27.62s/it]=>> [Epoch 000] Global Step 000069 =>> LR :: 0.000020 - Loss :: 0.1500:  35%|‚ñà‚ñà‚ñà‚ñå      | 70/200 [32:17<59:42, 27.56s/it]  =>> [Epoch 000] Global Step 000070 =>> LR :: 0.000020 - Loss :: 0.1687:  35%|‚ñà‚ñà‚ñà‚ñå      | 70/200 [32:17<59:42, 27.56s/it]=>> [Epoch 000] Global Step 000070 =>> LR :: 0.000020 - Loss :: 0.1687:  36%|‚ñà‚ñà‚ñà‚ñå      | 71/200 [32:45<59:11, 27.53s/it]=>> [Epoch 000] Global Step 000071 =>> LR :: 0.000020 - Loss :: 0.1635:  36%|‚ñà‚ñà‚ñà‚ñå      | 71/200 [32:45<59:11, 27.53s/it]=>> [Epoch 000] Global Step 000071 =>> LR :: 0.000020 - Loss :: 0.1635:  36%|‚ñà‚ñà‚ñà‚ñå      | 72/200 [33:12<58:40, 27.50s/it]=>> [Epoch 000] Global Step 000072 =>> LR :: 0.000020 - Loss :: 0.1589:  36%|‚ñà‚ñà‚ñà‚ñå      | 72/200 [33:12<58:40, 27.50s/it]=>> [Epoch 000] Global Step 000072 =>> LR :: 0.000020 - Loss :: 0.1589:  36%|‚ñà‚ñà‚ñà‚ñã      | 73/200 [33:39<58:07, 27.46s/it]=>> [Epoch 000] Global Step 000073 =>> LR :: 0.000020 - Loss :: 0.1796:  36%|‚ñà‚ñà‚ñà‚ñã      | 73/200 [33:39<58:07, 27.46s/it]=>> [Epoch 000] Global Step 000073 =>> LR :: 0.000020 - Loss :: 0.1796:  37%|‚ñà‚ñà‚ñà‚ñã      | 74/200 [34:07<57:41, 27.47s/it]=>> [Epoch 000] Global Step 000074 =>> LR :: 0.000020 - Loss :: 0.1732:  37%|‚ñà‚ñà‚ñà‚ñã      | 74/200 [34:07<57:41, 27.47s/it]=>> [Epoch 000] Global Step 000074 =>> LR :: 0.000020 - Loss :: 0.1732:  38%|‚ñà‚ñà‚ñà‚ñä      | 75/200 [34:35<57:20, 27.53s/it]=>> [Epoch 000] Global Step 000075 =>> LR :: 0.000020 - Loss :: 0.1641:  38%|‚ñà‚ñà‚ñà‚ñä      | 75/200 [34:35<57:20, 27.53s/it]=>> [Epoch 000] Global Step 000075 =>> LR :: 0.000020 - Loss :: 0.1641:  38%|‚ñà‚ñà‚ñà‚ñä      | 76/200 [35:02<56:56, 27.55s/it]=>> [Epoch 000] Global Step 000076 =>> LR :: 0.000020 - Loss :: 0.1709:  38%|‚ñà‚ñà‚ñà‚ñä      | 76/200 [35:02<56:56, 27.55s/it]=>> [Epoch 000] Global Step 000076 =>> LR :: 0.000020 - Loss :: 0.1709:  38%|‚ñà‚ñà‚ñà‚ñä      | 77/200 [35:30<56:20, 27.49s/it]=>> [Epoch 000] Global Step 000077 =>> LR :: 0.000020 - Loss :: 0.1689:  38%|‚ñà‚ñà‚ñà‚ñä      | 77/200 [35:30<56:20, 27.49s/it]=>> [Epoch 000] Global Step 000077 =>> LR :: 0.000020 - Loss :: 0.1689:  39%|‚ñà‚ñà‚ñà‚ñâ      | 78/200 [35:57<56:02, 27.56s/it]=>> [Epoch 000] Global Step 000078 =>> LR :: 0.000020 - Loss :: 0.1812:  39%|‚ñà‚ñà‚ñà‚ñâ      | 78/200 [35:57<56:02, 27.56s/it]=>> [Epoch 000] Global Step 000078 =>> LR :: 0.000020 - Loss :: 0.1812:  40%|‚ñà‚ñà‚ñà‚ñâ      | 79/200 [36:25<55:27, 27.50s/it]=>> [Epoch 000] Global Step 000079 =>> LR :: 0.000020 - Loss :: 0.1615:  40%|‚ñà‚ñà‚ñà‚ñâ      | 79/200 [36:25<55:27, 27.50s/it]=>> [Epoch 000] Global Step 000079 =>> LR :: 0.000020 - Loss :: 0.1615:  40%|‚ñà‚ñà‚ñà‚ñà      | 80/200 [36:52<54:57, 27.48s/it]=>> [Epoch 000] Global Step 000080 =>> LR :: 0.000020 - Loss :: 0.1686:  40%|‚ñà‚ñà‚ñà‚ñà      | 80/200 [36:52<54:57, 27.48s/it]=>> [Epoch 000] Global Step 000080 =>> LR :: 0.000020 - Loss :: 0.1686:  40%|‚ñà‚ñà‚ñà‚ñà      | 81/200 [37:20<54:31, 27.49s/it]=>> [Epoch 000] Global Step 000081 =>> LR :: 0.000020 - Loss :: 0.1390:  40%|‚ñà‚ñà‚ñà‚ñà      | 81/200 [37:20<54:31, 27.49s/it]=>> [Epoch 000] Global Step 000081 =>> LR :: 0.000020 - Loss :: 0.1390:  41%|‚ñà‚ñà‚ñà‚ñà      | 82/200 [37:47<53:58, 27.44s/it]=>> [Epoch 000] Global Step 000082 =>> LR :: 0.000020 - Loss :: 0.1586:  41%|‚ñà‚ñà‚ñà‚ñà      | 82/200 [37:47<53:58, 27.44s/it]=>> [Epoch 000] Global Step 000082 =>> LR :: 0.000020 - Loss :: 0.1586:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 83/200 [38:14<53:33, 27.46s/it]=>> [Epoch 000] Global Step 000083 =>> LR :: 0.000020 - Loss :: 0.1454:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 83/200 [38:14<53:33, 27.46s/it]=>> [Epoch 000] Global Step 000083 =>> LR :: 0.000020 - Loss :: 0.1454:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 84/200 [38:42<53:22, 27.61s/it]=>> [Epoch 000] Global Step 000084 =>> LR :: 0.000020 - Loss :: 0.1470:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 84/200 [38:42<53:22, 27.61s/it]=>> [Epoch 000] Global Step 000084 =>> LR :: 0.000020 - Loss :: 0.1470:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 85/200 [39:10<53:02, 27.68s/it]=>> [Epoch 000] Global Step 000085 =>> LR :: 0.000020 - Loss :: 0.1361:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 85/200 [39:10<53:02, 27.68s/it]=>> [Epoch 000] Global Step 000085 =>> LR :: 0.000020 - Loss :: 0.1361:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 86/200 [39:38<52:42, 27.74s/it]=>> [Epoch 000] Global Step 000086 =>> LR :: 0.000020 - Loss :: 0.1292:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 86/200 [39:38<52:42, 27.74s/it]=>> [Epoch 000] Global Step 000086 =>> LR :: 0.000020 - Loss :: 0.1292:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 87/200 [40:06<52:10, 27.70s/it]=>> [Epoch 000] Global Step 000087 =>> LR :: 0.000020 - Loss :: 0.1444:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 87/200 [40:06<52:10, 27.70s/it]=>> [Epoch 000] Global Step 000087 =>> LR :: 0.000020 - Loss :: 0.1444:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 88/200 [40:33<51:43, 27.71s/it]=>> [Epoch 000] Global Step 000088 =>> LR :: 0.000020 - Loss :: 0.1393:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 88/200 [40:33<51:43, 27.71s/it]=>> [Epoch 000] Global Step 000088 =>> LR :: 0.000020 - Loss :: 0.1393:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 89/200 [41:01<51:21, 27.76s/it]=>> [Epoch 000] Global Step 000089 =>> LR :: 0.000020 - Loss :: 0.1277:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 89/200 [41:01<51:21, 27.76s/it]=>> [Epoch 000] Global Step 000089 =>> LR :: 0.000020 - Loss :: 0.1277:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 90/200 [41:29<50:50, 27.73s/it]=>> [Epoch 000] Global Step 000090 =>> LR :: 0.000020 - Loss :: 0.1315:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 90/200 [41:29<50:50, 27.73s/it]=>> [Epoch 000] Global Step 000090 =>> LR :: 0.000020 - Loss :: 0.1315:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 91/200 [41:57<50:24, 27.75s/it]=>> [Epoch 000] Global Step 000091 =>> LR :: 0.000020 - Loss :: 0.1466:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 91/200 [41:57<50:24, 27.75s/it]=>> [Epoch 000] Global Step 000091 =>> LR :: 0.000020 - Loss :: 0.1466:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 92/200 [42:24<49:49, 27.68s/it]=>> [Epoch 000] Global Step 000092 =>> LR :: 0.000020 - Loss :: 0.1505:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 92/200 [42:24<49:49, 27.68s/it]=>> [Epoch 000] Global Step 000092 =>> LR :: 0.000020 - Loss :: 0.1505:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 93/200 [42:52<49:22, 27.69s/it]=>> [Epoch 000] Global Step 000093 =>> LR :: 0.000020 - Loss :: 0.1345:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 93/200 [42:52<49:22, 27.69s/it]=>> [Epoch 000] Global Step 000093 =>> LR :: 0.000020 - Loss :: 0.1345:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 94/200 [43:20<48:57, 27.71s/it]=>> [Epoch 000] Global Step 000094 =>> LR :: 0.000020 - Loss :: 0.1286:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 94/200 [43:20<48:57, 27.71s/it]=>> [Epoch 000] Global Step 000094 =>> LR :: 0.000020 - Loss :: 0.1286:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 95/200 [43:47<48:27, 27.69s/it]=>> [Epoch 000] Global Step 000095 =>> LR :: 0.000020 - Loss :: 0.1491:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 95/200 [43:47<48:27, 27.69s/it]=>> [Epoch 000] Global Step 000095 =>> LR :: 0.000020 - Loss :: 0.1491:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 96/200 [44:15<48:00, 27.70s/it]=>> [Epoch 000] Global Step 000096 =>> LR :: 0.000020 - Loss :: 0.1402:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 96/200 [44:15<48:00, 27.70s/it]=>> [Epoch 000] Global Step 000096 =>> LR :: 0.000020 - Loss :: 0.1402:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 97/200 [44:42<47:18, 27.56s/it]=>> [Epoch 000] Global Step 000097 =>> LR :: 0.000020 - Loss :: 0.1380:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 97/200 [44:42<47:18, 27.56s/it]=>> [Epoch 000] Global Step 000097 =>> LR :: 0.000020 - Loss :: 0.1380:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 98/200 [45:10<46:50, 27.56s/it]=>> [Epoch 000] Global Step 000098 =>> LR :: 0.000020 - Loss :: 0.1364:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 98/200 [45:10<46:50, 27.56s/it]=>> [Epoch 000] Global Step 000098 =>> LR :: 0.000020 - Loss :: 0.1364:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 99/200 [45:38<46:38, 27.70s/it]=>> [Epoch 000] Global Step 000099 =>> LR :: 0.000020 - Loss :: 0.1452:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 99/200 [45:38<46:38, 27.70s/it]=>> [Epoch 000] Global Step 000099 =>> LR :: 0.000020 - Loss :: 0.1452:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 100/200 [46:06<46:10, 27.70s/it]=>> [Epoch 000] Global Step 000100 =>> LR :: 0.000020 - Loss :: 0.1433:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 100/200 [46:06<46:10, 27.70s/it]=>> [Epoch 000] Global Step 000100 =>> LR :: 0.000020 - Loss :: 0.1433:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 101/200 [46:38<47:50, 28.99s/it]=>> [Epoch 000] Global Step 000101 =>> LR :: 0.000020 - Loss :: 0.1382:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 101/200 [46:38<47:50, 28.99s/it]=>> [Epoch 000] Global Step 000101 =>> LR :: 0.000020 - Loss :: 0.1382:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 102/200 [47:05<46:22, 28.39s/it]=>> [Epoch 000] Global Step 000102 =>> LR :: 0.000020 - Loss :: 0.1287:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 102/200 [47:05<46:22, 28.39s/it]=>> [Epoch 000] Global Step 000102 =>> LR :: 0.000020 - Loss :: 0.1287:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 103/200 [47:32<45:31, 28.16s/it]=>> [Epoch 000] Global Step 000103 =>> LR :: 0.000020 - Loss :: 0.1279:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 103/200 [47:32<45:31, 28.16s/it]=>> [Epoch 000] Global Step 000103 =>> LR :: 0.000020 - Loss :: 0.1279:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 104/200 [48:00<44:43, 27.96s/it]=>> [Epoch 000] Global Step 000104 =>> LR :: 0.000020 - Loss :: 0.1334:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 104/200 [48:00<44:43, 27.96s/it]=>> [Epoch 000] Global Step 000104 =>> LR :: 0.000020 - Loss :: 0.1334:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 105/200 [48:27<44:03, 27.83s/it]=>> [Epoch 000] Global Step 000105 =>> LR :: 0.000020 - Loss :: 0.1375:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 105/200 [48:27<44:03, 27.83s/it]=>> [Epoch 000] Global Step 000105 =>> LR :: 0.000020 - Loss :: 0.1375:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 106/200 [48:55<43:43, 27.91s/it]=>> [Epoch 000] Global Step 000106 =>> LR :: 0.000020 - Loss :: 0.1287:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 106/200 [48:55<43:43, 27.91s/it]=>> [Epoch 000] Global Step 000106 =>> LR :: 0.000020 - Loss :: 0.1287:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 107/200 [49:23<43:11, 27.86s/it]=>> [Epoch 000] Global Step 000107 =>> LR :: 0.000020 - Loss :: 0.1268:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 107/200 [49:23<43:11, 27.86s/it]=>> [Epoch 000] Global Step 000107 =>> LR :: 0.000020 - Loss :: 0.1268:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 108/200 [49:51<42:37, 27.80s/it]=>> [Epoch 000] Global Step 000108 =>> LR :: 0.000020 - Loss :: 0.1198:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 108/200 [49:51<42:37, 27.80s/it]=>> [Epoch 000] Global Step 000108 =>> LR :: 0.000020 - Loss :: 0.1198:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 109/200 [50:18<42:05, 27.76s/it]=>> [Epoch 000] Global Step 000109 =>> LR :: 0.000020 - Loss :: 0.1184:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 109/200 [50:18<42:05, 27.76s/it]=>> [Epoch 000] Global Step 000109 =>> LR :: 0.000020 - Loss :: 0.1184:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 110/200 [50:46<41:39, 27.77s/it]=>> [Epoch 000] Global Step 000110 =>> LR :: 0.000020 - Loss :: 0.1320:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 110/200 [50:46<41:39, 27.77s/it]=>> [Epoch 000] Global Step 000110 =>> LR :: 0.000020 - Loss :: 0.1320:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 111/200 [51:14<40:58, 27.63s/it]=>> [Epoch 000] Global Step 000111 =>> LR :: 0.000020 - Loss :: 0.1270:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 111/200 [51:14<40:58, 27.63s/it]=>> [Epoch 000] Global Step 000111 =>> LR :: 0.000020 - Loss :: 0.1270:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 112/200 [51:41<40:30, 27.62s/it]=>> [Epoch 000] Global Step 000112 =>> LR :: 0.000020 - Loss :: 0.1304:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 112/200 [51:41<40:30, 27.62s/it]=>> [Epoch 000] Global Step 000112 =>> LR :: 0.000020 - Loss :: 0.1304:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 113/200 [52:09<40:06, 27.66s/it]=>> [Epoch 000] Global Step 000113 =>> LR :: 0.000020 - Loss :: 0.1146:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 113/200 [52:09<40:06, 27.66s/it]=>> [Epoch 000] Global Step 000113 =>> LR :: 0.000020 - Loss :: 0.1146:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 114/200 [52:36<39:31, 27.58s/it]=>> [Epoch 000] Global Step 000114 =>> LR :: 0.000020 - Loss :: 0.1274:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 114/200 [52:36<39:31, 27.58s/it]=>> [Epoch 000] Global Step 000114 =>> LR :: 0.000020 - Loss :: 0.1274:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 115/200 [53:04<39:01, 27.55s/it]=>> [Epoch 000] Global Step 000115 =>> LR :: 0.000020 - Loss :: 0.1194:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 115/200 [53:04<39:01, 27.55s/it]=>> [Epoch 000] Global Step 000115 =>> LR :: 0.000020 - Loss :: 0.1194:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 116/200 [53:32<38:38, 27.61s/it]=>> [Epoch 000] Global Step 000116 =>> LR :: 0.000020 - Loss :: 0.1224:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 116/200 [53:32<38:38, 27.61s/it]=>> [Epoch 000] Global Step 000116 =>> LR :: 0.000020 - Loss :: 0.1224:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 117/200 [53:59<38:05, 27.54s/it]=>> [Epoch 000] Global Step 000117 =>> LR :: 0.000020 - Loss :: 0.1160:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 117/200 [53:59<38:05, 27.54s/it]=>> [Epoch 000] Global Step 000117 =>> LR :: 0.000020 - Loss :: 0.1160:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 118/200 [54:27<37:40, 27.57s/it]=>> [Epoch 000] Global Step 000118 =>> LR :: 0.000020 - Loss :: 0.1245:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 118/200 [54:27<37:40, 27.57s/it]=>> [Epoch 000] Global Step 000118 =>> LR :: 0.000020 - Loss :: 0.1245:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 119/200 [54:54<37:14, 27.58s/it]=>> [Epoch 000] Global Step 000119 =>> LR :: 0.000020 - Loss :: 0.1113:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 119/200 [54:54<37:14, 27.58s/it]=>> [Epoch 000] Global Step 000119 =>> LR :: 0.000020 - Loss :: 0.1113:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 120/200 [55:21<36:39, 27.49s/it]=>> [Epoch 000] Global Step 000120 =>> LR :: 0.000020 - Loss :: 0.1215:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 120/200 [55:21<36:39, 27.49s/it]=>> [Epoch 000] Global Step 000120 =>> LR :: 0.000020 - Loss :: 0.1215:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 121/200 [55:49<36:14, 27.52s/it]=>> [Epoch 000] Global Step 000121 =>> LR :: 0.000020 - Loss :: 0.1164:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 121/200 [55:49<36:14, 27.52s/it]=>> [Epoch 000] Global Step 000121 =>> LR :: 0.000020 - Loss :: 0.1164:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 122/200 [56:16<35:43, 27.48s/it]=>> [Epoch 000] Global Step 000122 =>> LR :: 0.000020 - Loss :: 0.1214:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 122/200 [56:16<35:43, 27.48s/it]=>> [Epoch 000] Global Step 000122 =>> LR :: 0.000020 - Loss :: 0.1214:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 123/200 [56:44<35:16, 27.48s/it]=>> [Epoch 000] Global Step 000123 =>> LR :: 0.000020 - Loss :: 0.1135:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 123/200 [56:44<35:16, 27.48s/it]=>> [Epoch 000] Global Step 000123 =>> LR :: 0.000020 - Loss :: 0.1135:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 124/200 [57:12<34:55, 27.58s/it]=>> [Epoch 000] Global Step 000124 =>> LR :: 0.000020 - Loss :: 0.1139:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 124/200 [57:12<34:55, 27.58s/it]=>> [Epoch 000] Global Step 000124 =>> LR :: 0.000020 - Loss :: 0.1139:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 125/200 [57:39<34:24, 27.52s/it]=>> [Epoch 000] Global Step 000125 =>> LR :: 0.000020 - Loss :: 0.1119:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 125/200 [57:39<34:24, 27.52s/it]=>> [Epoch 000] Global Step 000125 =>> LR :: 0.000020 - Loss :: 0.1119:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 126/200 [58:07<33:55, 27.51s/it]=>> [Epoch 000] Global Step 000126 =>> LR :: 0.000020 - Loss :: 0.1083:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 126/200 [58:07<33:55, 27.51s/it]=>> [Epoch 000] Global Step 000126 =>> LR :: 0.000020 - Loss :: 0.1083:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 127/200 [58:34<33:36, 27.62s/it]=>> [Epoch 000] Global Step 000127 =>> LR :: 0.000020 - Loss :: 0.1023:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 127/200 [58:34<33:36, 27.62s/it]=>> [Epoch 000] Global Step 000127 =>> LR :: 0.000020 - Loss :: 0.1023:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 128/200 [59:02<33:13, 27.69s/it]=>> [Epoch 000] Global Step 000128 =>> LR :: 0.000020 - Loss :: 0.0932:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 128/200 [59:02<33:13, 27.69s/it]=>> [Epoch 000] Global Step 000128 =>> LR :: 0.000020 - Loss :: 0.0932:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 129/200 [59:30<32:40, 27.61s/it]=>> [Epoch 000] Global Step 000129 =>> LR :: 0.000020 - Loss :: 0.1010:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 129/200 [59:30<32:40, 27.61s/it]=>> [Epoch 000] Global Step 000129 =>> LR :: 0.000020 - Loss :: 0.1010:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 130/200 [59:58<32:20, 27.72s/it]=>> [Epoch 000] Global Step 000130 =>> LR :: 0.000020 - Loss :: 0.1074:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 130/200 [59:58<32:20, 27.72s/it]=>> [Epoch 000] Global Step 000130 =>> LR :: 0.000020 - Loss :: 0.1074:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 131/200 [1:00:25<31:49, 27.67s/it]=>> [Epoch 000] Global Step 000131 =>> LR :: 0.000020 - Loss :: 0.0991:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 131/200 [1:00:25<31:49, 27.67s/it]=>> [Epoch 000] Global Step 000131 =>> LR :: 0.000020 - Loss :: 0.0991:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 132/200 [1:00:52<31:12, 27.53s/it]=>> [Epoch 000] Global Step 000132 =>> LR :: 0.000020 - Loss :: 0.0934:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 132/200 [1:00:52<31:12, 27.53s/it]=>> [Epoch 000] Global Step 000132 =>> LR :: 0.000020 - Loss :: 0.0934:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 133/200 [1:01:20<30:45, 27.54s/it]=>> [Epoch 000] Global Step 000133 =>> LR :: 0.000020 - Loss :: 0.0897:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 133/200 [1:01:20<30:45, 27.54s/it]=>> [Epoch 000] Global Step 000133 =>> LR :: 0.000020 - Loss :: 0.0897:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 134/200 [1:01:47<30:16, 27.52s/it]=>> [Epoch 000] Global Step 000134 =>> LR :: 0.000020 - Loss :: 0.0971:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 134/200 [1:01:47<30:16, 27.52s/it]=>> [Epoch 000] Global Step 000134 =>> LR :: 0.000020 - Loss :: 0.0971:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 135/200 [1:02:15<29:51, 27.56s/it]=>> [Epoch 000] Global Step 000135 =>> LR :: 0.000020 - Loss :: 0.0938:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 135/200 [1:02:15<29:51, 27.56s/it]=>> [Epoch 000] Global Step 000135 =>> LR :: 0.000020 - Loss :: 0.0938:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 136/200 [1:02:43<29:24, 27.57s/it]=>> [Epoch 000] Global Step 000136 =>> LR :: 0.000020 - Loss :: 0.1004:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 136/200 [1:02:43<29:24, 27.57s/it]=>> [Epoch 000] Global Step 000136 =>> LR :: 0.000020 - Loss :: 0.1004:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 137/200 [1:03:10<28:56, 27.57s/it]=>> [Epoch 000] Global Step 000137 =>> LR :: 0.000020 - Loss :: 0.0919:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 137/200 [1:03:10<28:56, 27.57s/it]=>> [Epoch 000] Global Step 000137 =>> LR :: 0.000020 - Loss :: 0.0919:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 138/200 [1:03:38<28:27, 27.54s/it]=>> [Epoch 000] Global Step 000138 =>> LR :: 0.000020 - Loss :: 0.0993:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 138/200 [1:03:38<28:27, 27.54s/it]=>> [Epoch 000] Global Step 000138 =>> LR :: 0.000020 - Loss :: 0.0993:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 139/200 [1:04:05<27:57, 27.50s/it]=>> [Epoch 000] Global Step 000139 =>> LR :: 0.000020 - Loss :: 0.0982:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 139/200 [1:04:05<27:57, 27.50s/it]=>> [Epoch 000] Global Step 000139 =>> LR :: 0.000020 - Loss :: 0.0982:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 140/200 [1:04:33<27:28, 27.48s/it]=>> [Epoch 000] Global Step 000140 =>> LR :: 0.000020 - Loss :: 0.0963:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 140/200 [1:04:33<27:28, 27.48s/it]=>> [Epoch 000] Global Step 000140 =>> LR :: 0.000020 - Loss :: 0.0963:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 141/200 [1:05:00<27:04, 27.54s/it]=>> [Epoch 000] Global Step 000141 =>> LR :: 0.000020 - Loss :: 0.0978:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 141/200 [1:05:00<27:04, 27.54s/it]=>> [Epoch 000] Global Step 000141 =>> LR :: 0.000020 - Loss :: 0.0978:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 142/200 [1:05:28<26:37, 27.54s/it]=>> [Epoch 000] Global Step 000142 =>> LR :: 0.000020 - Loss :: 0.0848:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 142/200 [1:05:28<26:37, 27.54s/it]=>> [Epoch 000] Global Step 000142 =>> LR :: 0.000020 - Loss :: 0.0848:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 143/200 [1:05:55<26:08, 27.51s/it]=>> [Epoch 000] Global Step 000143 =>> LR :: 0.000020 - Loss :: 0.0945:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 143/200 [1:05:55<26:08, 27.51s/it]=>> [Epoch 000] Global Step 000143 =>> LR :: 0.000020 - Loss :: 0.0945:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 144/200 [1:06:23<25:40, 27.50s/it]=>> [Epoch 000] Global Step 000144 =>> LR :: 0.000020 - Loss :: 0.0904:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 144/200 [1:06:23<25:40, 27.50s/it]=>> [Epoch 000] Global Step 000144 =>> LR :: 0.000020 - Loss :: 0.0904:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 145/200 [1:06:50<25:09, 27.45s/it]=>> [Epoch 000] Global Step 000145 =>> LR :: 0.000020 - Loss :: 0.0851:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 145/200 [1:06:50<25:09, 27.45s/it]=>> [Epoch 000] Global Step 000145 =>> LR :: 0.000020 - Loss :: 0.0851:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 146/200 [1:07:18<24:43, 27.47s/it]=>> [Epoch 000] Global Step 000146 =>> LR :: 0.000020 - Loss :: 0.0841:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 146/200 [1:07:18<24:43, 27.47s/it]=>> [Epoch 000] Global Step 000146 =>> LR :: 0.000020 - Loss :: 0.0841:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 147/200 [1:07:45<24:14, 27.44s/it]=>> [Epoch 000] Global Step 000147 =>> LR :: 0.000020 - Loss :: 0.0774:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 147/200 [1:07:45<24:14, 27.44s/it]=>> [Epoch 000] Global Step 000147 =>> LR :: 0.000020 - Loss :: 0.0774:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 148/200 [1:08:13<23:50, 27.52s/it]=>> [Epoch 000] Global Step 000148 =>> LR :: 0.000020 - Loss :: 0.0817:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 148/200 [1:08:13<23:50, 27.52s/it]=>> [Epoch 000] Global Step 000148 =>> LR :: 0.000020 - Loss :: 0.0817:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 149/200 [1:08:40<23:25, 27.56s/it]=>> [Epoch 000] Global Step 000149 =>> LR :: 0.000020 - Loss :: 0.0801:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 149/200 [1:08:40<23:25, 27.56s/it]=>> [Epoch 000] Global Step 000149 =>> LR :: 0.000020 - Loss :: 0.0801:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 150/200 [1:09:08<23:01, 27.64s/it]=>> [Epoch 000] Global Step 000150 =>> LR :: 0.000020 - Loss :: 0.0886:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 150/200 [1:09:08<23:01, 27.64s/it]=>> [Epoch 000] Global Step 000150 =>> LR :: 0.000020 - Loss :: 0.0886:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 151/200 [1:09:36<22:31, 27.59s/it]=>> [Epoch 000] Global Step 000151 =>> LR :: 0.000020 - Loss :: 0.0819:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 151/200 [1:09:36<22:31, 27.59s/it]=>> [Epoch 000] Global Step 000151 =>> LR :: 0.000020 - Loss :: 0.0819:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 152/200 [1:10:03<22:04, 27.59s/it]=>> [Epoch 000] Global Step 000152 =>> LR :: 0.000020 - Loss :: 0.0994:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 152/200 [1:10:03<22:04, 27.59s/it]=>> [Epoch 000] Global Step 000152 =>> LR :: 0.000020 - Loss :: 0.0994:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 153/200 [1:10:31<21:35, 27.56s/it]=>> [Epoch 000] Global Step 000153 =>> LR :: 0.000020 - Loss :: 0.0953:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 153/200 [1:10:31<21:35, 27.56s/it]=>> [Epoch 000] Global Step 000153 =>> LR :: 0.000020 - Loss :: 0.0953:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 154/200 [1:10:58<21:03, 27.47s/it]=>> [Epoch 000] Global Step 000154 =>> LR :: 0.000020 - Loss :: 0.1043:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 154/200 [1:10:58<21:03, 27.47s/it]=>> [Epoch 000] Global Step 000154 =>> LR :: 0.000020 - Loss :: 0.1043:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 155/200 [1:11:25<20:34, 27.44s/it]=>> [Epoch 000] Global Step 000155 =>> LR :: 0.000020 - Loss :: 0.0925:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 155/200 [1:11:25<20:34, 27.44s/it]=>> [Epoch 000] Global Step 000155 =>> LR :: 0.000020 - Loss :: 0.0925:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 156/200 [1:11:53<20:09, 27.49s/it]=>> [Epoch 000] Global Step 000156 =>> LR :: 0.000020 - Loss :: 0.0932:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 156/200 [1:11:53<20:09, 27.49s/it]=>> [Epoch 000] Global Step 000156 =>> LR :: 0.000020 - Loss :: 0.0932:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 157/200 [1:12:20<19:40, 27.46s/it]=>> [Epoch 000] Global Step 000157 =>> LR :: 0.000020 - Loss :: 0.0815:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 157/200 [1:12:20<19:40, 27.46s/it]=>> [Epoch 000] Global Step 000157 =>> LR :: 0.000020 - Loss :: 0.0815:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 158/200 [1:12:48<19:15, 27.51s/it]=>> [Epoch 000] Global Step 000158 =>> LR :: 0.000020 - Loss :: 0.0704:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 158/200 [1:12:48<19:15, 27.51s/it]=>> [Epoch 000] Global Step 000158 =>> LR :: 0.000020 - Loss :: 0.0704:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 159/200 [1:13:16<18:50, 27.56s/it]=>> [Epoch 000] Global Step 000159 =>> LR :: 0.000020 - Loss :: 0.0711:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 159/200 [1:13:16<18:50, 27.56s/it]=>> [Epoch 000] Global Step 000159 =>> LR :: 0.000020 - Loss :: 0.0711:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 160/200 [1:13:44<18:26, 27.66s/it]=>> [Epoch 000] Global Step 000160 =>> LR :: 0.000020 - Loss :: 0.0885:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 160/200 [1:13:44<18:26, 27.66s/it]=>> [Epoch 000] Global Step 000160 =>> LR :: 0.000020 - Loss :: 0.0885:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 161/200 [1:14:11<17:56, 27.60s/it]=>> [Epoch 000] Global Step 000161 =>> LR :: 0.000020 - Loss :: 0.0739:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 161/200 [1:14:11<17:56, 27.60s/it]=>> [Epoch 000] Global Step 000161 =>> LR :: 0.000020 - Loss :: 0.0739:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 162/200 [1:14:38<17:23, 27.46s/it]=>> [Epoch 000] Global Step 000162 =>> LR :: 0.000020 - Loss :: 0.0841:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 162/200 [1:14:38<17:23, 27.46s/it]=>> [Epoch 000] Global Step 000162 =>> LR :: 0.000020 - Loss :: 0.0841:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 163/200 [1:15:06<16:56, 27.47s/it]=>> [Epoch 000] Global Step 000163 =>> LR :: 0.000020 - Loss :: 0.0909:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 163/200 [1:15:06<16:56, 27.47s/it]=>> [Epoch 000] Global Step 000163 =>> LR :: 0.000020 - Loss :: 0.0909:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 164/200 [1:15:33<16:30, 27.51s/it]=>> [Epoch 000] Global Step 000164 =>> LR :: 0.000020 - Loss :: 0.1149:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 164/200 [1:15:33<16:30, 27.51s/it]=>> [Epoch 000] Global Step 000164 =>> LR :: 0.000020 - Loss :: 0.1149:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 165/200 [1:16:00<16:00, 27.43s/it]=>> [Epoch 000] Global Step 000165 =>> LR :: 0.000020 - Loss :: 0.0862:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 165/200 [1:16:00<16:00, 27.43s/it]=>> [Epoch 000] Global Step 000165 =>> LR :: 0.000020 - Loss :: 0.0862:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 166/200 [1:16:28<15:30, 27.38s/it]=>> [Epoch 000] Global Step 000166 =>> LR :: 0.000020 - Loss :: 0.0914:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 166/200 [1:16:28<15:30, 27.38s/it]=>> [Epoch 000] Global Step 000166 =>> LR :: 0.000020 - Loss :: 0.0914:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 167/200 [1:16:55<15:03, 27.37s/it]=>> [Epoch 000] Global Step 000167 =>> LR :: 0.000020 - Loss :: 0.1113:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 167/200 [1:16:55<15:03, 27.37s/it]=>> [Epoch 000] Global Step 000167 =>> LR :: 0.000020 - Loss :: 0.1113:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 168/200 [1:17:23<14:37, 27.41s/it]=>> [Epoch 000] Global Step 000168 =>> LR :: 0.000020 - Loss :: 0.1112:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 168/200 [1:17:23<14:37, 27.41s/it]=>> [Epoch 000] Global Step 000168 =>> LR :: 0.000020 - Loss :: 0.1112:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 169/200 [1:17:50<14:14, 27.56s/it]=>> [Epoch 000] Global Step 000169 =>> LR :: 0.000020 - Loss :: 0.1042:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 169/200 [1:17:50<14:14, 27.56s/it]=>> [Epoch 000] Global Step 000169 =>> LR :: 0.000020 - Loss :: 0.1042:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 170/200 [1:18:18<13:46, 27.56s/it]=>> [Epoch 000] Global Step 000170 =>> LR :: 0.000020 - Loss :: 0.0922:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 170/200 [1:18:18<13:46, 27.56s/it]=>> [Epoch 000] Global Step 000170 =>> LR :: 0.000020 - Loss :: 0.0922:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 171/200 [1:18:46<13:21, 27.64s/it]=>> [Epoch 000] Global Step 000171 =>> LR :: 0.000020 - Loss :: 0.0979:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 171/200 [1:18:46<13:21, 27.64s/it]=>> [Epoch 000] Global Step 000171 =>> LR :: 0.000020 - Loss :: 0.0979:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 172/200 [1:19:13<12:51, 27.57s/it]=>> [Epoch 000] Global Step 000172 =>> LR :: 0.000020 - Loss :: 0.0775:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 172/200 [1:19:13<12:51, 27.57s/it]=>> [Epoch 000] Global Step 000172 =>> LR :: 0.000020 - Loss :: 0.0775:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 173/200 [1:19:41<12:23, 27.55s/it]=>> [Epoch 000] Global Step 000173 =>> LR :: 0.000020 - Loss :: 0.0999:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 173/200 [1:19:41<12:23, 27.55s/it]=>> [Epoch 000] Global Step 000173 =>> LR :: 0.000020 - Loss :: 0.0999:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 174/200 [1:20:08<11:54, 27.49s/it]=>> [Epoch 000] Global Step 000174 =>> LR :: 0.000020 - Loss :: 0.0762:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 174/200 [1:20:08<11:54, 27.49s/it]=>> [Epoch 000] Global Step 000174 =>> LR :: 0.000020 - Loss :: 0.0762:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 175/200 [1:20:35<11:25, 27.42s/it]=>> [Epoch 000] Global Step 000175 =>> LR :: 0.000020 - Loss :: 0.0693:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 175/200 [1:20:35<11:25, 27.42s/it]=>> [Epoch 000] Global Step 000175 =>> LR :: 0.000020 - Loss :: 0.0693:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 176/200 [1:21:03<10:59, 27.50s/it]=>> [Epoch 000] Global Step 000176 =>> LR :: 0.000020 - Loss :: 0.0711:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 176/200 [1:21:03<10:59, 27.50s/it]=>> [Epoch 000] Global Step 000176 =>> LR :: 0.000020 - Loss :: 0.0711:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 177/200 [1:21:30<10:29, 27.37s/it]=>> [Epoch 000] Global Step 000177 =>> LR :: 0.000020 - Loss :: 0.0680:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 177/200 [1:21:30<10:29, 27.37s/it]=>> [Epoch 000] Global Step 000177 =>> LR :: 0.000020 - Loss :: 0.0680:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 178/200 [1:21:58<10:03, 27.45s/it]=>> [Epoch 000] Global Step 000178 =>> LR :: 0.000020 - Loss :: 0.0743:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 178/200 [1:21:58<10:03, 27.45s/it]=>> [Epoch 000] Global Step 000178 =>> LR :: 0.000020 - Loss :: 0.0743:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 179/200 [1:22:25<09:35, 27.40s/it]=>> [Epoch 000] Global Step 000179 =>> LR :: 0.000020 - Loss :: 0.0775:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 179/200 [1:22:25<09:35, 27.40s/it]=>> [Epoch 000] Global Step 000179 =>> LR :: 0.000020 - Loss :: 0.0775:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 180/200 [1:22:53<09:09, 27.50s/it]=>> [Epoch 000] Global Step 000180 =>> LR :: 0.000020 - Loss :: 0.0755:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 180/200 [1:22:53<09:09, 27.50s/it]=>> [Epoch 000] Global Step 000180 =>> LR :: 0.000020 - Loss :: 0.0755:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 181/200 [1:23:20<08:41, 27.45s/it]=>> [Epoch 000] Global Step 000181 =>> LR :: 0.000020 - Loss :: 0.0926:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 181/200 [1:23:20<08:41, 27.45s/it]=>> [Epoch 000] Global Step 000181 =>> LR :: 0.000020 - Loss :: 0.0926:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 182/200 [1:23:48<08:13, 27.43s/it]=>> [Epoch 000] Global Step 000182 =>> LR :: 0.000020 - Loss :: 0.0780:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 182/200 [1:23:48<08:13, 27.43s/it]=>> [Epoch 000] Global Step 000182 =>> LR :: 0.000020 - Loss :: 0.0780:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 183/200 [1:24:15<07:47, 27.52s/it]=>> [Epoch 000] Global Step 000183 =>> LR :: 0.000020 - Loss :: 0.1031:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 183/200 [1:24:15<07:47, 27.52s/it]=>> [Epoch 000] Global Step 000183 =>> LR :: 0.000020 - Loss :: 0.1031:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 184/200 [1:24:43<07:20, 27.51s/it]=>> [Epoch 000] Global Step 000184 =>> LR :: 0.000020 - Loss :: 0.1053:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 184/200 [1:24:43<07:20, 27.51s/it]=>> [Epoch 000] Global Step 000184 =>> LR :: 0.000020 - Loss :: 0.1053:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 185/200 [1:25:10<06:53, 27.53s/it]=>> [Epoch 000] Global Step 000185 =>> LR :: 0.000020 - Loss :: 0.0925:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 185/200 [1:25:10<06:53, 27.53s/it]=>> [Epoch 000] Global Step 000185 =>> LR :: 0.000020 - Loss :: 0.0925:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 186/200 [1:25:38<06:26, 27.58s/it]=>> [Epoch 000] Global Step 000186 =>> LR :: 0.000020 - Loss :: 0.0974:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 186/200 [1:25:38<06:26, 27.58s/it]=>> [Epoch 000] Global Step 000186 =>> LR :: 0.000020 - Loss :: 0.0974:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 187/200 [1:26:06<05:58, 27.57s/it]=>> [Epoch 000] Global Step 000187 =>> LR :: 0.000020 - Loss :: 0.1170:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 187/200 [1:26:06<05:58, 27.57s/it]=>> [Epoch 000] Global Step 000187 =>> LR :: 0.000020 - Loss :: 0.1170:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 188/200 [1:26:33<05:30, 27.52s/it]=>> [Epoch 000] Global Step 000188 =>> LR :: 0.000020 - Loss :: 0.1087:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 188/200 [1:26:33<05:30, 27.52s/it]=>> [Epoch 000] Global Step 000188 =>> LR :: 0.000020 - Loss :: 0.1087:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 189/200 [1:27:00<05:02, 27.50s/it]=>> [Epoch 000] Global Step 000189 =>> LR :: 0.000020 - Loss :: 0.1272:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 189/200 [1:27:00<05:02, 27.50s/it]=>> [Epoch 000] Global Step 000189 =>> LR :: 0.000020 - Loss :: 0.1272:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 190/200 [1:27:28<04:35, 27.59s/it]=>> [Epoch 000] Global Step 000190 =>> LR :: 0.000020 - Loss :: 0.0982:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 190/200 [1:27:28<04:35, 27.59s/it]=>> [Epoch 000] Global Step 000190 =>> LR :: 0.000020 - Loss :: 0.0982:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 191/200 [1:27:56<04:08, 27.62s/it]=>> [Epoch 000] Global Step 000191 =>> LR :: 0.000020 - Loss :: 0.0697:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 191/200 [1:27:56<04:08, 27.62s/it]=>> [Epoch 000] Global Step 000191 =>> LR :: 0.000020 - Loss :: 0.0697:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 192/200 [1:28:24<03:41, 27.72s/it]=>> [Epoch 000] Global Step 000192 =>> LR :: 0.000020 - Loss :: 0.0690:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 192/200 [1:28:24<03:41, 27.72s/it]=>> [Epoch 000] Global Step 000192 =>> LR :: 0.000020 - Loss :: 0.0690:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 193/200 [1:28:51<03:13, 27.65s/it]=>> [Epoch 000] Global Step 000193 =>> LR :: 0.000020 - Loss :: 0.0818:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 193/200 [1:28:51<03:13, 27.65s/it]=>> [Epoch 000] Global Step 000193 =>> LR :: 0.000020 - Loss :: 0.0818:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 194/200 [1:29:19<02:45, 27.65s/it]=>> [Epoch 000] Global Step 000194 =>> LR :: 0.000020 - Loss :: 0.0618:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 194/200 [1:29:19<02:45, 27.65s/it]=>> [Epoch 000] Global Step 000194 =>> LR :: 0.000020 - Loss :: 0.0618:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 195/200 [1:29:47<02:18, 27.68s/it]=>> [Epoch 000] Global Step 000195 =>> LR :: 0.000020 - Loss :: 0.0610:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 195/200 [1:29:47<02:18, 27.68s/it]=>> [Epoch 000] Global Step 000195 =>> LR :: 0.000020 - Loss :: 0.0610:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 196/200 [1:30:15<01:50, 27.75s/it]=>> [Epoch 000] Global Step 000196 =>> LR :: 0.000020 - Loss :: 0.0518:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 196/200 [1:30:15<01:50, 27.75s/it]=>> [Epoch 000] Global Step 000196 =>> LR :: 0.000020 - Loss :: 0.0518:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 197/200 [1:30:42<01:23, 27.74s/it]=>> [Epoch 000] Global Step 000197 =>> LR :: 0.000020 - Loss :: 0.0613:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 197/200 [1:30:42<01:23, 27.74s/it]=>> [Epoch 000] Global Step 000197 =>> LR :: 0.000020 - Loss :: 0.0613:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 198/200 [1:31:10<00:55, 27.63s/it]=>> [Epoch 000] Global Step 000198 =>> LR :: 0.000020 - Loss :: 0.0657:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 198/200 [1:31:10<00:55, 27.63s/it]=>> [Epoch 000] Global Step 000198 =>> LR :: 0.000020 - Loss :: 0.0657: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 199/200 [1:31:37<00:27, 27.64s/it]=>> [Epoch 000] Global Step 000199 =>> LR :: 0.000020 - Loss :: 0.0525: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 199/200 [1:31:37<00:27, 27.64s/it]                                                                                                                           wandb:                                                                                
wandb: 
wandb: Run history:
wandb:          VLA Train/Action Token Accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñà
wandb:             VLA Train/CoT Token Accuracy ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:                          VLA Train/Epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                        VLA Train/L1 Loss ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  VLA Train/Learning Rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                           VLA Train/Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                     VLA Train/Loss (Raw) ‚ñà‚ñá‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                           VLA Train/Step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:                      VLA Train/Step Time ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ
wandb:            VLA Train/action_tag_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:  VLA Train/gripper position_tag_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:    VLA Train/move reasoning_tag_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:              VLA Train/move_tag_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:              VLA Train/plan_tag_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: VLA Train/subtask reasoning_tag_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:           VLA Train/subtask_tag_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:              VLA Train/task_tag_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:   VLA Train/visible objects_tag_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:          VLA Train/Action Token Accuracy 0.84821
wandb:             VLA Train/CoT Token Accuracy 0.97972
wandb:                          VLA Train/Epoch 0
wandb:                        VLA Train/L1 Loss 0.0278
wandb:                  VLA Train/Learning Rate 2e-05
wandb:                           VLA Train/Loss 0.05965
wandb:                     VLA Train/Loss (Raw) 0.05965
wandb:                           VLA Train/Step 200
wandb:                      VLA Train/Step Time 27.92128
wandb:            VLA Train/action_tag_accuracy 0.25155
wandb:  VLA Train/gripper position_tag_accuracy 0.86693
wandb:    VLA Train/move reasoning_tag_accuracy 0.9476
wandb:              VLA Train/move_tag_accuracy 0.53333
wandb:              VLA Train/plan_tag_accuracy 1
wandb: VLA Train/subtask reasoning_tag_accuracy 0.97089
wandb:           VLA Train/subtask_tag_accuracy 1
wandb:              VLA Train/task_tag_accuracy 1
wandb:   VLA Train/visible objects_tag_accuracy 0.94794
wandb: 
wandb: üöÄ View run prism-dinosiglip-224px+mx-libero-90+n1+b16+x7 at: https://wandb.ai/solace/ecot_reproduce_libero/runs/ez8jqye1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/solace/ecot_reproduce_libero
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /srv/rl2-lab/flash7/rbansal66/embodied-CoT/runs/prism-dinosiglip-224px+mx-libero-90+n1+b16+x7/wandb/run-20250607_222610-ez8jqye1/logs
