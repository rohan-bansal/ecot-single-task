2025-06-03 14:53:14.629694: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629694: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629697: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629700: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629698: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629698: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629701: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629701: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.629753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.629754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.629755: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.629754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.629754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.629754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.629793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.730107: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.730115: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.730117: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.730119: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.730119: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.730119: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.730119: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.730121: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.912482: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:14.912473: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:14.912481: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:14.912482: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:14.912473: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:14.912473: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:14.912478: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:14.912477: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:16.972960: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-03 14:53:16.972960: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-03 14:53:16.972960: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-03 14:53:16.973173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-03 14:53:16.973173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-03 14:53:16.973177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-03 14:53:16.973194: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-03 14:53:16.973223: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:20<00:20, 20.73s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:22<00:22, 22.34s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:22<00:22, 22.09s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:22<00:22, 22.26s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.41s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:21<00:21, 21.20s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:20<00:20, 20.35s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:21<00:21, 21.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 11.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.69s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 13.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.57s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 13.43s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.75s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 12.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 12.64s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.22s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.80s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 13.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.40s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 13.36s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.67s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 13.46s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.80s/it]
2025-06-03 14:59:03.298670: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:03.339412: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:03.632052: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:04.257743: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:04.277843: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:04.550828: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:08.414132: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:08.685376: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:08.741591: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:08.808449: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:09.331513: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:09.569891: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:09.635338: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:09.700233: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:10.461145: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:11.500595: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:38.574779: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:43.270392: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:44.994913: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:49.028877: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:49.556076: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:50.445052: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:51.185591: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:52.845942: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
wandb: Currently logged in as: solace to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /srv/rl2-lab/flash7/rbansal66/embodied-CoT/runs/prism-dinosiglip-224px+mx-bridge+n1+b16+x7/wandb/run-20250603_150112-2w82amgw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prism-dinosiglip-224px+mx-bridge+n1+b16+x7
wandb: ⭐️ View project at https://wandb.ai/solace/ecot_reproduce
wandb: 🚀 View run at https://wandb.ai/solace/ecot_reproduce/runs/2w82amgw

=>> [Epoch 000] Global Step 000000 =>> LR :: 0.000000:   0%|          | 0/200 [00:00<?, ?it/s]2025-06-03 15:03:39.786218: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:04:03.519247: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:04:13.452286: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:04:47.149747: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:05:51.719361: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:11:51.275956: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:12:20.816029: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 2 of 100
2025-06-03 15:13:45.523733: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:15:05.256491: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:15:40.782201: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 2 of 100
2025-06-03 15:15:46.938828: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 5 of 100
2025-06-03 15:16:25.148824: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 2 of 100
2025-06-03 15:17:10.791294: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 2 of 100
2025-06-03 15:17:54.319232: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 41 of 100
2025-06-03 15:18:00.012315: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 41 of 100
2025-06-03 15:18:45.314705: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 42 of 100
2025-06-03 15:20:03.935869: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 41 of 100
2025-06-03 15:20:43.494944: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 41 of 100
2025-06-03 15:20:59.658002: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 41 of 100
2025-06-03 15:21:16.855911: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 41 of 100
2025-06-03 15:21:20.991030: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 41 of 100
2025-06-03 15:22:03.274062: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 77 of 100
2025-06-03 15:22:03.274619: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.
2025-06-03 15:22:24.242723: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 78 of 100
2025-06-03 15:22:24.243274: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.
2025-06-03 15:22:27.591170: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 77 of 100
2025-06-03 15:22:27.592762: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.
2025-06-03 15:24:57.406079: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 78 of 100
2025-06-03 15:24:57.407206: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.
2025-06-03 15:25:12.582787: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 78 of 100
2025-06-03 15:25:12.583454: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.
2025-06-03 15:25:28.007964: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 78 of 100
2025-06-03 15:25:28.008731: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.
2025-06-03 15:25:52.623061: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 78 of 100
2025-06-03 15:25:52.623878: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.
2025-06-03 15:26:04.877142: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 78 of 100
2025-06-03 15:26:04.877632: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.

=>> [Epoch 000] Global Step 000000 =>> LR :: 0.000000:   0%|          | 1/200 [33:23<110:45:55, 2003.80s/it]
=>> [Epoch 000] Global Step 000001 =>> LR :: 0.000020 - Loss :: 2.2888:   0%|          | 1/200 [33:23<110:45:55, 2003.80s/it]
=>> [Epoch 000] Global Step 000001 =>> LR :: 0.000020 - Loss :: 2.2888:   1%|          | 2/200 [34:17<47:07:48, 856.91s/it]  
=>> [Epoch 000] Global Step 000002 =>> LR :: 0.000020 - Loss :: 1.4491:   1%|          | 2/200 [34:17<47:07:48, 856.91s/it]
=>> [Epoch 000] Global Step 000002 =>> LR :: 0.000020 - Loss :: 1.4491:   2%|▏         | 3/200 [35:13<26:51:30, 490.81s/it]
=>> [Epoch 000] Global Step 000003 =>> LR :: 0.000020 - Loss :: 0.9524:   2%|▏         | 3/200 [35:13<26:51:30, 490.81s/it]
=>> [Epoch 000] Global Step 000003 =>> LR :: 0.000020 - Loss :: 0.9524:   2%|▏         | 4/200 [36:09<17:22:41, 319.19s/it]
=>> [Epoch 000] Global Step 000004 =>> LR :: 0.000020 - Loss :: 0.8238:   2%|▏         | 4/200 [36:09<17:22:41, 319.19s/it]
=>> [Epoch 000] Global Step 000004 =>> LR :: 0.000020 - Loss :: 0.8238:   2%|▎         | 5/200 [37:06<12:10:31, 224.77s/it]
=>> [Epoch 000] Global Step 000005 =>> LR :: 0.000020 - Loss :: 0.7091:   2%|▎         | 5/200 [37:06<12:10:31, 224.77s/it]
=>> [Epoch 000] Global Step 000005 =>> LR :: 0.000020 - Loss :: 0.7091:   3%|▎         | 6/200 [41:52<13:14:07, 245.60s/it]
=>> [Epoch 000] Global Step 000006 =>> LR :: 0.000020 - Loss :: 0.5890:   3%|▎         | 6/200 [41:52<13:14:07, 245.60s/it]
=>> [Epoch 000] Global Step 000006 =>> LR :: 0.000020 - Loss :: 0.5890:   4%|▎         | 7/200 [42:46<9:48:02, 182.81s/it] 
=>> [Epoch 000] Global Step 000007 =>> LR :: 0.000020 - Loss :: 0.4666:   4%|▎         | 7/200 [42:46<9:48:02, 182.81s/it]
=>> [Epoch 000] Global Step 000007 =>> LR :: 0.000020 - Loss :: 0.4666:   4%|▍         | 8/200 [46:46<10:43:49, 201.19s/it]
=>> [Epoch 000] Global Step 000008 =>> LR :: 0.000020 - Loss :: 0.4459:   4%|▍         | 8/200 [46:46<10:43:49, 201.19s/it]
=>> [Epoch 000] Global Step 000008 =>> LR :: 0.000020 - Loss :: 0.4459:   4%|▍         | 9/200 [47:43<8:16:46, 156.05s/it] 
=>> [Epoch 000] Global Step 000009 =>> LR :: 0.000020 - Loss :: 0.3646:   4%|▍         | 9/200 [47:43<8:16:46, 156.05s/it]
=>> [Epoch 000] Global Step 000009 =>> LR :: 0.000020 - Loss :: 0.3646:   5%|▌         | 10/200 [48:37<6:34:34, 124.60s/it]
=>> [Epoch 000] Global Step 000010 =>> LR :: 0.000020 - Loss :: 0.3455:   5%|▌         | 10/200 [48:37<6:34:34, 124.60s/it]
=>> [Epoch 000] Global Step 000010 =>> LR :: 0.000020 - Loss :: 0.3455:   6%|▌         | 11/200 [51:13<7:02:39, 134.18s/it]
=>> [Epoch 000] Global Step 000011 =>> LR :: 0.000020 - Loss :: 0.3256:   6%|▌         | 11/200 [51:13<7:02:39, 134.18s/it]
=>> [Epoch 000] Global Step 000011 =>> LR :: 0.000020 - Loss :: 0.3256:   6%|▌         | 12/200 [52:08<5:44:55, 110.08s/it]
=>> [Epoch 000] Global Step 000012 =>> LR :: 0.000020 - Loss :: 0.2725:   6%|▌         | 12/200 [52:08<5:44:55, 110.08s/it]
=>> [Epoch 000] Global Step 000012 =>> LR :: 0.000020 - Loss :: 0.2725:   6%|▋         | 13/200 [55:17<6:57:28, 133.95s/it]
=>> [Epoch 000] Global Step 000013 =>> LR :: 0.000020 - Loss :: 0.2576:   6%|▋         | 13/200 [55:17<6:57:28, 133.95s/it]
=>> [Epoch 000] Global Step 000013 =>> LR :: 0.000020 - Loss :: 0.2576:   7%|▋         | 14/200 [56:11<5:40:18, 109.78s/it]
=>> [Epoch 000] Global Step 000014 =>> LR :: 0.000020 - Loss :: 0.2194:   7%|▋         | 14/200 [56:11<5:40:18, 109.78s/it]
=>> [Epoch 000] Global Step 000014 =>> LR :: 0.000020 - Loss :: 0.2194:   8%|▊         | 15/200 [1:00:42<8:08:56, 158.57s/it]
=>> [Epoch 000] Global Step 000015 =>> LR :: 0.000020 - Loss :: 0.1842:   8%|▊         | 15/200 [1:00:42<8:08:56, 158.57s/it]
=>> [Epoch 000] Global Step 000015 =>> LR :: 0.000020 - Loss :: 0.1842:   8%|▊         | 16/200 [1:01:35<6:28:04, 126.55s/it]
=>> [Epoch 000] Global Step 000016 =>> LR :: 0.000020 - Loss :: 0.1401:   8%|▊         | 16/200 [1:01:35<6:28:04, 126.55s/it]
=>> [Epoch 000] Global Step 000016 =>> LR :: 0.000020 - Loss :: 0.1401:   8%|▊         | 17/200 [1:02:30<5:20:27, 105.07s/it]
=>> [Epoch 000] Global Step 000017 =>> LR :: 0.000020 - Loss :: 0.1496:   8%|▊         | 17/200 [1:02:30<5:20:27, 105.07s/it]
=>> [Epoch 000] Global Step 000017 =>> LR :: 0.000020 - Loss :: 0.1496:   9%|▉         | 18/200 [1:06:49<7:39:23, 151.45s/it]
=>> [Epoch 000] Global Step 000018 =>> LR :: 0.000020 - Loss :: 0.1252:   9%|▉         | 18/200 [1:06:49<7:39:23, 151.45s/it]
=>> [Epoch 000] Global Step 000018 =>> LR :: 0.000020 - Loss :: 0.1252:  10%|▉         | 19/200 [1:07:44<6:09:27, 122.47s/it]
=>> [Epoch 000] Global Step 000019 =>> LR :: 0.000020 - Loss :: 0.1086:  10%|▉         | 19/200 [1:07:44<6:09:27, 122.47s/it]
=>> [Epoch 000] Global Step 000019 =>> LR :: 0.000020 - Loss :: 0.1086:  10%|█         | 20/200 [1:12:48<8:51:00, 177.00s/it]
=>> [Epoch 000] Global Step 000020 =>> LR :: 0.000020 - Loss :: 0.1099:  10%|█         | 20/200 [1:12:48<8:51:00, 177.00s/it]
=>> [Epoch 000] Global Step 000020 =>> LR :: 0.000020 - Loss :: 0.1099:  10%|█         | 21/200 [1:13:47<7:02:24, 141.59s/it]
=>> [Epoch 000] Global Step 000021 =>> LR :: 0.000020 - Loss :: 0.0722:  10%|█         | 21/200 [1:13:47<7:02:24, 141.59s/it]
=>> [Epoch 000] Global Step 000021 =>> LR :: 0.000020 - Loss :: 0.0722:  11%|█         | 22/200 [1:17:44<8:25:07, 170.27s/it]
=>> [Epoch 000] Global Step 000022 =>> LR :: 0.000020 - Loss :: 0.0673:  11%|█         | 22/200 [1:17:44<8:25:07, 170.27s/it]
=>> [Epoch 000] Global Step 000022 =>> LR :: 0.000020 - Loss :: 0.0673:  12%|█▏        | 23/200 [1:18:42<6:42:15, 136.36s/it]
=>> [Epoch 000] Global Step 000023 =>> LR :: 0.000020 - Loss :: 0.0625:  12%|█▏        | 23/200 [1:18:42<6:42:15, 136.36s/it]
=>> [Epoch 000] Global Step 000023 =>> LR :: 0.000020 - Loss :: 0.0625:  12%|█▏        | 24/200 [1:22:05<7:39:19, 156.59s/it]
=>> [Epoch 000] Global Step 000024 =>> LR :: 0.000020 - Loss :: 0.0529:  12%|█▏        | 24/200 [1:22:05<7:39:19, 156.59s/it]
=>> [Epoch 000] Global Step 000024 =>> LR :: 0.000020 - Loss :: 0.0529:  12%|█▎        | 25/200 [1:23:02<6:09:37, 126.73s/it]
=>> [Epoch 000] Global Step 000025 =>> LR :: 0.000020 - Loss :: 0.0412:  12%|█▎        | 25/200 [1:23:02<6:09:37, 126.73s/it]
=>> [Epoch 000] Global Step 000025 =>> LR :: 0.000020 - Loss :: 0.0412:  13%|█▎        | 26/200 [1:23:59<5:06:42, 105.76s/it]
=>> [Epoch 000] Global Step 000026 =>> LR :: 0.000020 - Loss :: 0.0721:  13%|█▎        | 26/200 [1:23:59<5:06:42, 105.76s/it]
=>> [Epoch 000] Global Step 000026 =>> LR :: 0.000020 - Loss :: 0.0721:  14%|█▎        | 27/200 [1:29:48<8:34:43, 178.52s/it]
=>> [Epoch 000] Global Step 000027 =>> LR :: 0.000020 - Loss :: 0.0476:  14%|█▎        | 27/200 [1:29:48<8:34:43, 178.52s/it]
=>> [Epoch 000] Global Step 000027 =>> LR :: 0.000020 - Loss :: 0.0476:  14%|█▍        | 28/200 [1:30:43<6:46:03, 141.65s/it]
=>> [Epoch 000] Global Step 000028 =>> LR :: 0.000020 - Loss :: 0.0455:  14%|█▍        | 28/200 [1:30:43<6:46:03, 141.65s/it]
=>> [Epoch 000] Global Step 000028 =>> LR :: 0.000020 - Loss :: 0.0455:  14%|█▍        | 29/200 [1:33:53<7:24:41, 156.03s/it]
=>> [Epoch 000] Global Step 000029 =>> LR :: 0.000020 - Loss :: 0.0323:  14%|█▍        | 29/200 [1:33:53<7:24:41, 156.03s/it]
=>> [Epoch 000] Global Step 000029 =>> LR :: 0.000020 - Loss :: 0.0323:  15%|█▌        | 30/200 [1:34:49<5:57:04, 126.03s/it]
=>> [Epoch 000] Global Step 000030 =>> LR :: 0.000020 - Loss :: 0.0312:  15%|█▌        | 30/200 [1:34:49<5:57:04, 126.03s/it]
=>> [Epoch 000] Global Step 000030 =>> LR :: 0.000020 - Loss :: 0.0312:  16%|█▌        | 31/200 [1:38:41<7:24:57, 157.97s/it]
=>> [Epoch 000] Global Step 000031 =>> LR :: 0.000020 - Loss :: 0.0228:  16%|█▌        | 31/200 [1:38:41<7:24:57, 157.97s/it]
=>> [Epoch 000] Global Step 000031 =>> LR :: 0.000020 - Loss :: 0.0228:  16%|█▌        | 32/200 [1:39:33<5:52:39, 125.95s/it]
=>> [Epoch 000] Global Step 000032 =>> LR :: 0.000020 - Loss :: 0.0176:  16%|█▌        | 32/200 [1:39:33<5:52:39, 125.95s/it]
=>> [Epoch 000] Global Step 000032 =>> LR :: 0.000020 - Loss :: 0.0176:  16%|█▋        | 33/200 [1:40:24<4:48:11, 103.54s/it]
=>> [Epoch 000] Global Step 000033 =>> LR :: 0.000020 - Loss :: 0.0113:  16%|█▋        | 33/200 [1:40:24<4:48:11, 103.54s/it]
=>> [Epoch 000] Global Step 000033 =>> LR :: 0.000020 - Loss :: 0.0113:  17%|█▋        | 34/200 [1:44:00<6:19:42, 137.24s/it]
=>> [Epoch 000] Global Step 000034 =>> LR :: 0.000020 - Loss :: 0.0092:  17%|█▋        | 34/200 [1:44:00<6:19:42, 137.24s/it]
=>> [Epoch 000] Global Step 000034 =>> LR :: 0.000020 - Loss :: 0.0092:  18%|█▊        | 35/200 [1:44:56<5:10:53, 113.05s/it]
=>> [Epoch 000] Global Step 000035 =>> LR :: 0.000020 - Loss :: 0.0131:  18%|█▊        | 35/200 [1:44:56<5:10:53, 113.05s/it]
=>> [Epoch 000] Global Step 000035 =>> LR :: 0.000020 - Loss :: 0.0131:  18%|█▊        | 36/200 [1:49:53<7:39:27, 168.10s/it]
=>> [Epoch 000] Global Step 000036 =>> LR :: 0.000020 - Loss :: 0.0116:  18%|█▊        | 36/200 [1:49:53<7:39:27, 168.10s/it]
=>> [Epoch 000] Global Step 000036 =>> LR :: 0.000020 - Loss :: 0.0116:  18%|█▊        | 37/200 [1:50:45<6:02:09, 133.31s/it]
=>> [Epoch 000] Global Step 000037 =>> LR :: 0.000020 - Loss :: 0.0173:  18%|█▊        | 37/200 [1:50:45<6:02:09, 133.31s/it]
=>> [Epoch 000] Global Step 000037 =>> LR :: 0.000020 - Loss :: 0.0173:  19%|█▉        | 38/200 [1:54:08<6:56:09, 154.14s/it]
=>> [Epoch 000] Global Step 000038 =>> LR :: 0.000020 - Loss :: 0.0092:  19%|█▉        | 38/200 [1:54:08<6:56:09, 154.14s/it]
=>> [Epoch 000] Global Step 000038 =>> LR :: 0.000020 - Loss :: 0.0092:  20%|█▉        | 39/200 [1:55:04<5:35:00, 124.85s/it]
=>> [Epoch 000] Global Step 000039 =>> LR :: 0.000020 - Loss :: 0.0084:  20%|█▉        | 39/200 [1:55:04<5:35:00, 124.85s/it]
=>> [Epoch 000] Global Step 000039 =>> LR :: 0.000020 - Loss :: 0.0084:  20%|██        | 40/200 [1:56:00<4:38:01, 104.26s/it]
=>> [Epoch 000] Global Step 000040 =>> LR :: 0.000020 - Loss :: 0.0132:  20%|██        | 40/200 [1:56:00<4:38:01, 104.26s/it]
=>> [Epoch 000] Global Step 000040 =>> LR :: 0.000020 - Loss :: 0.0132:  20%|██        | 41/200 [2:00:01<6:24:44, 145.18s/it]
=>> [Epoch 000] Global Step 000041 =>> LR :: 0.000020 - Loss :: 0.0059:  20%|██        | 41/200 [2:00:01<6:24:44, 145.18s/it]
=>> [Epoch 000] Global Step 000041 =>> LR :: 0.000020 - Loss :: 0.0059:  21%|██        | 42/200 [2:00:52<5:08:11, 117.03s/it]
=>> [Epoch 000] Global Step 000042 =>> LR :: 0.000020 - Loss :: 0.0086:  21%|██        | 42/200 [2:00:52<5:08:11, 117.03s/it]
=>> [Epoch 000] Global Step 000042 =>> LR :: 0.000020 - Loss :: 0.0086:  22%|██▏       | 43/200 [2:04:14<6:12:12, 142.25s/it]
=>> [Epoch 000] Global Step 000043 =>> LR :: 0.000020 - Loss :: 0.0078:  22%|██▏       | 43/200 [2:04:14<6:12:12, 142.25s/it]
=>> [Epoch 000] Global Step 000043 =>> LR :: 0.000020 - Loss :: 0.0078:  22%|██▏       | 44/200 [2:05:07<5:00:52, 115.72s/it]
=>> [Epoch 000] Global Step 000044 =>> LR :: 0.000020 - Loss :: 0.0039:  22%|██▏       | 44/200 [2:05:07<5:00:52, 115.72s/it]
=>> [Epoch 000] Global Step 000044 =>> LR :: 0.000020 - Loss :: 0.0039:  22%|██▎       | 45/200 [2:11:15<8:14:23, 191.38s/it]
=>> [Epoch 000] Global Step 000045 =>> LR :: 0.000020 - Loss :: 0.0059:  22%|██▎       | 45/200 [2:11:15<8:14:23, 191.38s/it]
=>> [Epoch 000] Global Step 000045 =>> LR :: 0.000020 - Loss :: 0.0059:  23%|██▎       | 46/200 [2:12:07<6:23:39, 149.48s/it]
=>> [Epoch 000] Global Step 000046 =>> LR :: 0.000020 - Loss :: 0.0082:  23%|██▎       | 46/200 [2:12:07<6:23:39, 149.48s/it]
=>> [Epoch 000] Global Step 000046 =>> LR :: 0.000020 - Loss :: 0.0082:  24%|██▎       | 47/200 [2:13:01<5:07:55, 120.75s/it]
=>> [Epoch 000] Global Step 000047 =>> LR :: 0.000020 - Loss :: 0.0145:  24%|██▎       | 47/200 [2:13:01<5:07:55, 120.75s/it]
=>> [Epoch 000] Global Step 000047 =>> LR :: 0.000020 - Loss :: 0.0145:  24%|██▍       | 48/200 [2:16:02<5:51:38, 138.80s/it]
=>> [Epoch 000] Global Step 000048 =>> LR :: 0.000020 - Loss :: 0.0233:  24%|██▍       | 48/200 [2:16:02<5:51:38, 138.80s/it]
=>> [Epoch 000] Global Step 000048 =>> LR :: 0.000020 - Loss :: 0.0233:  24%|██▍       | 49/200 [2:16:55<4:44:58, 113.23s/it]
=>> [Epoch 000] Global Step 000049 =>> LR :: 0.000020 - Loss :: 0.0167:  24%|██▍       | 49/200 [2:16:55<4:44:58, 113.23s/it]
=>> [Epoch 000] Global Step 000049 =>> LR :: 0.000020 - Loss :: 0.0167:  25%|██▌       | 50/200 [2:20:16<5:48:39, 139.46s/it]
=>> [Epoch 000] Global Step 000050 =>> LR :: 0.000020 - Loss :: 0.0084:  25%|██▌       | 50/200 [2:20:16<5:48:39, 139.46s/it]
=>> [Epoch 000] Global Step 000050 =>> LR :: 0.000020 - Loss :: 0.0084:  26%|██▌       | 51/200 [2:21:08<4:41:04, 113.18s/it]
=>> [Epoch 000] Global Step 000051 =>> LR :: 0.000020 - Loss :: 0.0137:  26%|██▌       | 51/200 [2:21:08<4:41:04, 113.18s/it]
=>> [Epoch 000] Global Step 000051 =>> LR :: 0.000020 - Loss :: 0.0137:  26%|██▌       | 52/200 [2:27:20<7:50:37, 190.79s/it]
=>> [Epoch 000] Global Step 000052 =>> LR :: 0.000020 - Loss :: 0.0120:  26%|██▌       | 52/200 [2:27:20<7:50:37, 190.79s/it]
=>> [Epoch 000] Global Step 000052 =>> LR :: 0.000020 - Loss :: 0.0120:  26%|██▋       | 53/200 [2:28:13<6:06:41, 149.67s/it]
=>> [Epoch 000] Global Step 000053 =>> LR :: 0.000020 - Loss :: 0.0103:  26%|██▋       | 53/200 [2:28:13<6:06:41, 149.67s/it]
=>> [Epoch 000] Global Step 000053 =>> LR :: 0.000020 - Loss :: 0.0103:  27%|██▋       | 54/200 [2:29:06<4:53:27, 120.60s/it]
=>> [Epoch 000] Global Step 000054 =>> LR :: 0.000020 - Loss :: 0.0075:  27%|██▋       | 54/200 [2:29:06<4:53:27, 120.60s/it]
=>> [Epoch 000] Global Step 000054 =>> LR :: 0.000020 - Loss :: 0.0075:  28%|██▊       | 55/200 [2:32:08<5:35:42, 138.91s/it]
=>> [Epoch 000] Global Step 000055 =>> LR :: 0.000020 - Loss :: 0.0047:  28%|██▊       | 55/200 [2:32:08<5:35:42, 138.91s/it]
=>> [Epoch 000] Global Step 000055 =>> LR :: 0.000020 - Loss :: 0.0047:  28%|██▊       | 56/200 [2:33:02<4:32:42, 113.63s/it]
=>> [Epoch 000] Global Step 000056 =>> LR :: 0.000020 - Loss :: 0.0063:  28%|██▊       | 56/200 [2:33:02<4:32:42, 113.63s/it]
=>> [Epoch 000] Global Step 000056 =>> LR :: 0.000020 - Loss :: 0.0063:  28%|██▊       | 57/200 [2:34:01<3:51:50, 97.27s/it] 
=>> [Epoch 000] Global Step 000057 =>> LR :: 0.000020 - Loss :: 0.0122:  28%|██▊       | 57/200 [2:34:01<3:51:50, 97.27s/it]
=>> [Epoch 000] Global Step 000057 =>> LR :: 0.000020 - Loss :: 0.0122:  29%|██▉       | 58/200 [2:34:59<3:21:57, 85.34s/it]
=>> [Epoch 000] Global Step 000058 =>> LR :: 0.000020 - Loss :: 0.0062:  29%|██▉       | 58/200 [2:34:59<3:21:57, 85.34s/it]
=>> [Epoch 000] Global Step 000058 =>> LR :: 0.000020 - Loss :: 0.0062:  30%|██▉       | 59/200 [2:41:52<7:11:50, 183.77s/it]
=>> [Epoch 000] Global Step 000059 =>> LR :: 0.000020 - Loss :: 0.0044:  30%|██▉       | 59/200 [2:41:52<7:11:50, 183.77s/it]
=>> [Epoch 000] Global Step 000059 =>> LR :: 0.000020 - Loss :: 0.0044:  30%|███       | 60/200 [2:42:49<5:39:50, 145.64s/it]
=>> [Epoch 000] Global Step 000060 =>> LR :: 0.000020 - Loss :: 0.0038:  30%|███       | 60/200 [2:42:49<5:39:50, 145.64s/it]
=>> [Epoch 000] Global Step 000060 =>> LR :: 0.000020 - Loss :: 0.0038:  30%|███       | 61/200 [2:48:26<7:50:37, 203.15s/it]
=>> [Epoch 000] Global Step 000061 =>> LR :: 0.000020 - Loss :: 0.0042:  30%|███       | 61/200 [2:48:26<7:50:37, 203.15s/it]
=>> [Epoch 000] Global Step 000061 =>> LR :: 0.000020 - Loss :: 0.0042:  31%|███       | 62/200 [2:49:18<6:02:58, 157.81s/it]
=>> [Epoch 000] Global Step 000062 =>> LR :: 0.000020 - Loss :: 0.0047:  31%|███       | 62/200 [2:49:18<6:02:58, 157.81s/it]
=>> [Epoch 000] Global Step 000062 =>> LR :: 0.000020 - Loss :: 0.0047:  32%|███▏      | 63/200 [2:50:18<4:52:49, 128.25s/it]
=>> [Epoch 000] Global Step 000063 =>> LR :: 0.000020 - Loss :: 0.0058:  32%|███▏      | 63/200 [2:50:18<4:52:49, 128.25s/it]
=>> [Epoch 000] Global Step 000063 =>> LR :: 0.000020 - Loss :: 0.0058:  32%|███▏      | 64/200 [2:51:35<4:15:54, 112.90s/it]
=>> [Epoch 000] Global Step 000064 =>> LR :: 0.000020 - Loss :: 0.0030:  32%|███▏      | 64/200 [2:51:35<4:15:54, 112.90s/it]
=>> [Epoch 000] Global Step 000064 =>> LR :: 0.000020 - Loss :: 0.0030:  32%|███▎      | 65/200 [2:52:33<3:36:56, 96.42s/it] 
=>> [Epoch 000] Global Step 000065 =>> LR :: 0.000020 - Loss :: 0.0032:  32%|███▎      | 65/200 [2:52:33<3:36:56, 96.42s/it]
=>> [Epoch 000] Global Step 000065 =>> LR :: 0.000020 - Loss :: 0.0032:  33%|███▎      | 66/200 [2:59:15<7:00:16, 188.18s/it]
=>> [Epoch 000] Global Step 000066 =>> LR :: 0.000020 - Loss :: 0.0021:  33%|███▎      | 66/200 [2:59:15<7:00:16, 188.18s/it]
=>> [Epoch 000] Global Step 000066 =>> LR :: 0.000020 - Loss :: 0.0021:  34%|███▎      | 67/200 [3:00:11<5:29:13, 148.52s/it]
=>> [Epoch 000] Global Step 000067 =>> LR :: 0.000020 - Loss :: 0.0017:  34%|███▎      | 67/200 [3:00:11<5:29:13, 148.52s/it]
=>> [Epoch 000] Global Step 000067 =>> LR :: 0.000020 - Loss :: 0.0017:  34%|███▍      | 68/200 [3:03:18<5:52:24, 160.18s/it]
=>> [Epoch 000] Global Step 000068 =>> LR :: 0.000020 - Loss :: 0.0012:  34%|███▍      | 68/200 [3:03:18<5:52:24, 160.18s/it]
=>> [Epoch 000] Global Step 000068 =>> LR :: 0.000020 - Loss :: 0.0012:  34%|███▍      | 69/200 [3:04:15<4:41:44, 129.04s/it]
=>> [Epoch 000] Global Step 000069 =>> LR :: 0.000020 - Loss :: 0.0008:  34%|███▍      | 69/200 [3:04:15<4:41:44, 129.04s/it]
=>> [Epoch 000] Global Step 000069 =>> LR :: 0.000020 - Loss :: 0.0008:  35%|███▌      | 70/200 [3:05:06<3:49:08, 105.76s/it]
=>> [Epoch 000] Global Step 000070 =>> LR :: 0.000020 - Loss :: 0.0005:  35%|███▌      | 70/200 [3:05:06<3:49:08, 105.76s/it]
=>> [Epoch 000] Global Step 000070 =>> LR :: 0.000020 - Loss :: 0.0005:  36%|███▌      | 71/200 [3:06:36<3:37:13, 101.04s/it]
=>> [Epoch 000] Global Step 000071 =>> LR :: 0.000020 - Loss :: 0.0002:  36%|███▌      | 71/200 [3:06:36<3:37:13, 101.04s/it]
=>> [Epoch 000] Global Step 000071 =>> LR :: 0.000020 - Loss :: 0.0002:  36%|███▌      | 72/200 [3:07:31<3:06:11, 87.28s/it] 
=>> [Epoch 000] Global Step 000072 =>> LR :: 0.000020 - Loss :: 0.0002:  36%|███▌      | 72/200 [3:07:31<3:06:11, 87.28s/it]
=>> [Epoch 000] Global Step 000072 =>> LR :: 0.000020 - Loss :: 0.0002:  36%|███▋      | 73/200 [3:15:34<7:15:55, 205.95s/it]
=>> [Epoch 000] Global Step 000073 =>> LR :: 0.000020 - Loss :: 0.0001:  36%|███▋      | 73/200 [3:15:34<7:15:55, 205.95s/it]
=>> [Epoch 000] Global Step 000073 =>> LR :: 0.000020 - Loss :: 0.0001:  37%|███▋      | 74/200 [3:16:29<5:37:07, 160.54s/it]
=>> [Epoch 000] Global Step 000074 =>> LR :: 0.000020 - Loss :: 0.0002:  37%|███▋      | 74/200 [3:16:29<5:37:07, 160.54s/it]
=>> [Epoch 000] Global Step 000074 =>> LR :: 0.000020 - Loss :: 0.0002:  38%|███▊      | 75/200 [3:18:54<5:24:38, 155.83s/it]
=>> [Epoch 000] Global Step 000075 =>> LR :: 0.000020 - Loss :: 0.0001:  38%|███▊      | 75/200 [3:18:54<5:24:38, 155.83s/it]
=>> [Epoch 000] Global Step 000075 =>> LR :: 0.000020 - Loss :: 0.0001:  38%|███▊      | 76/200 [3:19:51<4:21:02, 126.31s/it]
=>> [Epoch 000] Global Step 000076 =>> LR :: 0.000020 - Loss :: 0.00:47<3:35:33, 105.15s/it]=>> [Epoch 000] Global Step 000077 =>> LR :: 0.000020 - Loss :: 0.0001:  39%|███▉      | 78/200 [3:24:20<4:39:45, 137.59s/it]=>> [Epoch 000] Global Step 000078 =>> LR :: 0.000020 - Loss :: 0.0001:  39%|███▉      | 78/200 [3:24:20<4:39:45, 137.59s/it]=>> [Epoch 000] Global Step 000078 =>> LR :: 0.000020 - Loss :: 0.0001:  40%|███▉      | 79/200 [3:25:16<3:48:10, 113.15s/it]=>> [Epoch 000] Global Step 000079 =>> LR :: 0.000020 - Loss :: 0.0001:  40%|███▉      | 79/200 [3:25:16<3:48:10, 113.15s/it]                                                                                                                                                                                                                                                                                                                           =>> [Epoch 000] Global Step 000079 =>> LR :: 0.000020 - Loss :: 0.0001:  40%|████      | 80/200 [3:31:16<6:14:10, 187.08s/it]=>> [Epoch 000] Global Step 000080 =>> LR :: 0.000020 - Loss :: 0.0001:  40%|████      | 80/200 [3:31:16<6:14:10, 187.08s/it]=>> [Epoch 000] Global Step 000080 =>> LR :: 0.000020 - Loss :: 0.0001:  40%|████      | 81/200 [3:32:07<4:50:13, 146.33s/it]=>> [Epoch 000] Global Step 000081 =>> LR :: 0.000020 - Loss :: 0.0000:  40%|████      | 81/200 [3:32:07<4:50:13, 146.33s/it]=>> [Epoch 000] Global Step 000081 =>> LR :: 0.000020 - Loss :: 0.0000:  41%|████      | 82/200 [3:33:46<4:19:43, 132.06s/it]=>> [Epoch 000] Global Step 000082 =>> LR :: 0.000020 - Loss :: 0.0000:  41%|████      | 82/200 [3:33:46<4:19:43, 132.06s/it]=>> [Epoch 000] Global Step 000082 =>> LR :: 0.000020 - Loss :: 0.0000:  42%|████▏     | 83/200 [3:34:40<3:31:42, 108.57s/it]=>> [Epoch 000] Global Step 000083 =>> LR :: 0.000020 - Loss :: 0.0000:  42%|████▏     | 83/200 [3:34:40<3:31:42, 108.57s/it]=>> [Epoch 000] Global Step 000083 =>> LR :: 0.000020 - Loss :: 0.0000:  42%|████▏     | 84/200 [3:35:34<2:58:16, 92.21s/it] =>> [Epoch 000] Global Step 000084 =>> LR :: 0.000020 - Loss :: 0.0000:  42%|████▏     | 84/200 [3:35:34<2:58:16, 92.21s/it]=>> [Epoch 000] Global Step 000084 =>> LR :: 0.000020 - Loss :: 0.0000:  42%|████▎     | 85/200 [3:40:16<4:45:50, 149.13s/it]=>> [Epoch 000] Global Step 000085 =>> LR :: 0.000020 - Loss :: 0.0000:  42%|████▎     | 85/200 [3:40:16<4:45:50, 149.13s/it]=>> [Epoch 000] Global Step 000085 =>> LR :: 0.000020 - Loss :: 0.0000:  43%|████▎     | 86/200 [3:41:08<3:48:00, 120.01s/it]=>> [Epoch 000] Global Step 000086 =>> LR :: 0.000020 - Loss :: 0.0000:  43%|████▎     | 86/200 [3:41:08<3:48:00, 120.01s/it]=>> [Epoch 000] Global Step 000086 =>> LR :: 0.000020 - Loss :: 0.0000:  44%|████▎     | 87/200 [3:45:13<4:56:34, 157.47s/it]=>> [Epoch 000] Global Step 000087 =>> LR :: 0.000020 - Loss :: 0.0000:  44%|████▎     | 87/200 [3:45:13<4:56:34, 157.47s/it]=>> [Epoch 000] Global Step 000087 =>> LR :: 0.000020 - Loss :: 0.0000:  44%|████▍     | 88/200 [3:46:10<3:57:41, 127.34s/it]=>> [Epoch 000] Global Step 000088 =>> LR :: 0.000020 - Loss :: 0.0000:  44%|████▍     | 88/200 [3:46:10<3:57:41, 127.34s/it]=>> [Epoch 000] Global Step 000088 =>> LR :: 0.000020 - Loss :: 0.0000:  44%|████▍     | 89/200 [3:48:20<3:57:26, 128.35s/it]=>> [Epoch 000] Global Step 000089 =>> LR :: 0.000020 - Loss :: 0.0000:  44%|████▍     | 89/200 [3:48:20<3:57:26, 128.35s/it]=>> [Epoch 000] Global Step 000089 =>> LR :: 0.000020 - Loss :: 0.0000:  45%|████▌     | 90/200 [3:49:13<3:13:42, 105.66s/it]=>> [Epoch 000] Global Step 000090 =>> LR :: 0.000020 - Loss :: 0.0000:  45%|████▌     | 90/200 [3:49:13<3:13:42, 105.66s/it]=>> [Epoch 000] Global Step 000090 =>> LR :: 0.000020 - Loss :: 0.0000:  46%|████▌     | 91/200 [3:50:09<2:44:55, 90.78s/it] =>> [Epoch 000] Global Step 000091 =>> LR :: 0.000020 - Loss :: 0.0000:  46%|████▌     | 91/200 [3:50:09<2:44:55, 90.78s/it]=>> [Epoch 000] Global Step 000091 =>> LR :: 0.000020 - Loss :: 0.0000:  46%|████▌     | 92/200 [3:56:11<5:09:52, 172.15s/it]=>> [Epoch 000] Global Step 000092 =>> LR :: 0.000020 - Loss :: 0.0000:  46%|████▌     | 92/200 [3:56:11<5:09:52, 172.15s/it]=>> [Epoch 000] Global Step 000092 =>> LR :: 0.000020 - Loss :: 0.0000:  46%|████▋     | 93/200 [3:57:04<4:03:14, 136.40s/it]=>> [Epoch 000] Global Step 000093 =>> LR :: 0.000020 - Loss :: 0.0000:  46%|████▋     | 93/200 [3:57:04<4:03:14, 136.40s/it]=>> [Epoch 000] Global Step 000093 =>> LR :: 0.000020 - Loss :: 0.0000:  47%|████▋     | 94/200 [4:00:14<4:29:03, 152.30s/it]=>> [Epoch 000] Global Step 000094 =>> LR :: 0.000020 - Loss :: 0.0000:  47%|████▋     | 94/200 [4:00:14<4:29:03, 152.30s/it]=>> [Epoch 000] Global Step 000094 =>> LR :: 0.000020 - Loss :: 0.0000:  48%|████▊     | 95/200 [4:01:07<3:34:47, 122.74s/it]=>> [Epoch 000] Global Step 000095 =>> LR :: 0.000020 - Loss :: 0.0000:  48%|████▊     | 95/200 [4:01:07<3:34:47, 122.74s/it]=>> [Epoch 000] Global Step 000095 =>> LR :: 0.000020 - Loss :: 0.0000:  48%|████▊     | 96/200 [4:06:39<5:21:13, 185.33s/it]=>> [Epoch 000] Global Step 000096 =>> LR :: 0.000020 - Loss :: 0.0000:  48%|████▊     | 96/200 [4:06:39<5:21:13, 185.33s/it]=>> [Epoch 000] Global Step 000096 =>> LR :: 0.000020 - Loss :: 0.0000:  48%|████▊     | 97/200 [4:07:29<4:08:52, 144.98s/it]=>> [Epoch 000] Global Step 000097 =>> LR :: 0.000020 - Loss :: 0.0000:  48%|████▊     | 97/200 [4:07:29<4:08:52, 144.98s/it]=>> [Epoch 000] Global Step 000097 =>> LR :: 0.000020 - Loss :: 0.0000:  49%|████▉     | 98/200 [4:10:41<4:29:57, 158.80s/it]=>> [Epoch 000] Global Step 000098 =>> LR :: 0.000020 - Loss :: 0.0000:  49%|████▉     | 98/200 [4:10:41<4:29:57, 158.80s/it]=>> [Epoch 000] Global Step 000098 =>> LR :: 0.000020 - Loss :: 0.0000:  50%|████▉     | 99/200 [4:11:37<3:35:33, 128.05s/it]=>> [Epoch 000] Global Step 000099 =>> LR :: 0.000020 - Loss :: 0.0000:  50%|████▉     | 99/200 [4:11:37<3:35:33, 128.05s/it]=>> [Epoch 000] Global Step 000099 =>> LR :: 0.000020 - Loss :: 0.0000:  50%|█████     | 100/200 [4:12:37<2:59:34, 107.75s/it]=>> [Epoch 000] Global Step 000100 =>> LR :: 0.000020 - Loss :: 0.0000:  50%|█████     | 100/200 [4:12:37<2:59:34, 107.75s/it]=>> [Epoch 000] Global Step 000100 =>> LR :: 0.000020 - Loss :: 0.0000:  50%|█████     | 101/200 [4:17:33<4:30:55, 164.19s/it]=>> [Epoch 000] Global Step 000101 =>> LR :: 0.000020 - Loss :: 0.0000:  50%|█████     | 101/200 [4:17:33<4:30:55, 164.19s/it]=>> [Epoch 000] Global Step 000101 =>> LR :: 0.000020 - Loss :: 0.0000:  51%|█████     | 102/200 [4:18:23<3:32:07, 129.87s/it]=>> [Epoch 000] Global Step 000102 =>> LR :: 0.000020 - Loss :: 0.0000:  51%|█████     | 102/200 [4:18:23<3:32:07, 129.87s/it]=>> [Epoch 000] Global Step 000102 =>> LR :: 0.000020 - Loss :: 0.0000:  52%|█████▏    | 103/200 [4:22:08<4:16:04, 158.40s/it]=>> [Epoch 000] Global Step 000103 =>> LR :: 0.000020 - Loss :: 0.0000:  52%|█████▏    | 103/200 [4:22:08<4:16:04, 158.40s/it]=>> [Epoch 000] Global Step 000103 =>> LR :: 0.000020 - Loss :: 0.0000:  52%|█████▏    | 104/200 [4:23:06<3:25:24, 128.38s/it]=>> [Epoch 000] Global Step 000104 =>> LR :: 0.000020 - Loss :: 0.0000:  52%|█████▏    | 104/200 [4:23:06<3:25:24, 128.38s/it]=>> [Epoch 000] Global Step 000104 =>> LR :: 0.000020 - Loss :: 0.0000:  52%|█████▎    | 105/200 [4:26:20<3:54:19, 148.00s/it]=>> [Epoch 000] Global Step 000105 =>> LR :: 0.000020 - Loss :: 0.0000:  52%|█████▎    | 105/200 [4:26:20<3:54:19, 148.00s/it]=>> [Epoch 000] Global Step 000105 =>> LR :: 0.000020 - Loss :: 0.0000:  53%|█████▎    | 106/200 [4:27:25<3:12:50, 123.09s/it]=>> [Epoch 000] Global Step 000106 =>> LR :: 0.000020 - Loss :: 0.0000:  53%|█████▎    | 106/200 [4:27:25<3:12:50, 123.09s/it]=>> [Epoch 000] Global Step 000106 =>> LR :: 0.000020 - Loss :: 0.0000:  54%|█████▎    | 107/200 [4:28:27<2:42:30, 104.84s/it]=>> [Epoch 000] Global Step 000107 =>> LR :: 0.000020 - Loss :: 0.0000:  54%|█████▎    | 107/200 [4:28:27<2:42:30, 104.84s/it]=>> [Epoch 000] Global Step 000107 =>> LR :: 0.000020 - Loss :: 0.0000:  54%|█████▍    | 108/200 [4:31:45<3:23:30, 132.73s/it]=>> [Epoch 000] Global Step 000108 =>> LR :: 0.000020 - Loss :: 0.0000:  54%|█████▍    | 108/200 [4:31:45<3:23:30, 132.73s/it]=>> [Epoch 000] Global Step 000108 =>> LR :: 0.000020 - Loss :: 0.0000:  55%|█████▍    | 109/200 [4:32:45<2:48:00, 110.78s/it]=>> [Epoch 000] Global Step 000109 =>> LR :: 0.000020 - Loss :: 0.0000:  55%|█████▍    | 109/200 [4:32:45<2:48:00, 110.78s/it]=>> [Epoch 000] Global Step 000109 =>> LR :: 0.000020 - Loss :: 0.0000:  55%|█████▌    | 110/200 [4:36:56<3:49:36, 153.08s/it]=>> [Epoch 000] Global Step 000110 =>> LR :: 0.000020 - Loss :: 0.0000:  55%|█████▌    | 110/200 [4:36:56<3:49:36, 153.08s/it]=>> [Epoch 000] Global Step 000110 =>> LR :: 0.000020 - Loss :: 0.0000:  56%|█████▌    | 111/200 [4:37:48<3:01:56, 122.65s/it]=>> [Epoch 000] Global Step 000111 =>> LR :: 0.000020 - Loss :: 0.0000:  56%|█████▌    | 111/200 [4:37:48<3:01:56, 122.65s/it]=>> [Epoch 000] Global Step 000111 =>> LR :: 0.000020 - Loss :: 0.0000:  56%|█████▌    | 112/200 [4:41:39<3:47:30, 155.12s/it]=>> [Epoch 000] Global Step 000112 =>> LR :: 0.000020 - Loss :: 0.0000:  56%|█████▌    | 112/200 [4:41:39<3:47:30, 155.12s/it]=>> [Epoch 000] Global Step 000112 =>> LR :: 0.000020 - Loss :: 0.0000:  56%|█████▋    | 113/200 [4:42:32<3:00:26, 124.44s/it]=>> [Epoch 000] Global Step 000113 =>> LR :: 0.000020 - Loss :: 0.0000:  56%|█████▋    | 113/200 [4:42:32<3:00:26, 124.44s/it]=>> [Epoch 000] Global Step 000113 =>> LR :: 0.000020 - Loss :: 0.0000:  57%|█████▋    | 114/200 [4:43:24<2:27:08, 102.65s/it]=>> [Epoch 000] Global Step 000114 =>> LR :: 0.000020 - Loss :: 0.0000:  57%|█████▋    | 114/200 [4:43:24<2:27:08, 102.65s/it]=>> [Epoch 000] Global Step 000114 =>> LR :: 0.000020 - Loss :: 0.0000:  57%|█████▊    | 115/200 [4:46:31<3:01:36, 128.20s/it]=>> [Epoch 000] Global Step 000115 =>> LR :: 0.000020 - Loss :: 0.0000:  57%|█████▊    | 115/200 [4:46:31<3:01:36, 128.20s/it]=>> [Epoch 000] Global Step 000115 =>> LR :: 0.000020 - Loss :: 0.0000:  58%|█████▊    | 116/200 [4:47:25<2:28:15, 105.90s/it]=>> [Epoch 000] Global Step 000116 =>> LR :: 0.000020 - Loss :: 0.0000:  58%|█████▊    | 116/200 [4:47:25<2:28:15, 105.90s/it]=>> [Epoch 000] Global Step 000116 =>> LR :: 0.000020 - Loss :: 0.0000:  58%|█████▊    | 117/200 [4:52:23<3:46:06, 163.45s/it]=>> [Epoch 000] Global Step 000117 =>> LR :: 0.000020 - Loss :: 0.0000:  58%|█████▊    | 117/200 [4:52:23<3:46:06, 163.45s/it]=>> [Epoch 000] Global Step 000117 =>> LR :: 0.000020 - Loss :: 0.0000:  59%|█████▉    | 118/200 [4:53:21<3:00:17, 131.92s/it]=>> [Epoch 000] Global Step 000118 =>> LR :: 0.000020 - Loss :: 0.0000:  59%|█████▉    | 118/200 [4:53:21<3:00:17, 131.92s/it]=>> [Epoch 000] Global Step 000118 =>> LR :: 0.000020 - Loss :: 0.0000:  60%|█████▉    | 119/200 [4:55:55<3:06:47, 138.36s/it]=>> [Epoch 000] Global Step 000119 =>> LR :: 0.000020 - Loss :: 0.0000:  60%|█████▉    | 119/200 [4:55:55<3:06:47, 138.36s/it]=>> [Epoch 000] Global Step 000119 =>> LR :: 0.000020 - Loss :: 0.0000:  60%|██████    | 120/200 [4:56:53<2:32:24, 114.30s/it]=>> [Epoch 000] Global Step 000120 =>> LR :: 0.000020 - Loss :: 0.0000:  60%|██████    | 120/200 [4:56:53<2:32:24, 114.30s/it]=>> [Epoch 000] Global Step 000120 =>> LR :: 0.000020 - Loss :: 0.0000:  60%|██████    | 121/200 [4:57:46<2:06:11, 95.84s/it] =>> [Epoch 000] Global Step 000121 =>> LR :: 0.000020 - Loss :: 0.0000:  60%|██████    | 121/200 [4:57:46<2:06:11, 95.84s/it]=>> [Epoch 000] Global Step 000121 =>> LR :: 0.000020 - Loss :: 0.0000:  61%|██████    | 122/200 [5:02:11<3:10:31, 146.56s/it]=>> [Epoch 000] Global Step 000122 =>> LR :: 0.000020 - Loss :: 0.0000:  61%|██████    | 122/200 [5:02:11<3:10:31, 146.56s/it]=>> [Epoch 000] Global Step 000122 =>> LR :: 0.000020 - Loss :: 0.0000:  62%|██████▏   | 123/200 [5:03:04<2:32:10, 118.58s/it]=>> [Epoch 000] Global Step 000123 =>> LR :: 0.000020 - Loss :: 0.0000:  62%|██████▏   | 123/200 [5:03:04<2:32:10, 118.58s/it]=>> [Epoch 000] Global Step 000123 =>> LR :: 0.000020 - Loss :: 0.0000:  62%|██████▏   | 124/200 [5:07:56<3:36:19, 170.78s/it]=>> [Epoch 000] Global Step 000124 =>> LR :: 0.000020 - Loss :: 0.0000:  62%|██████▏   | 124/200 [5:07:56<3:36:19, 170.78s/it]=>> [Epoch 000] Global Step 000124 =>> LR :: 0.000020 - Loss :: 0.0000:  62%|██████▎   | 125/200 [5:08:54<2:51:06, 136.88s/it]=>> [Epoch 000] Global Step 000125 =>> LR :: 0.000020 - Loss :: 0.0000:  62%|██████▎   | 125/200 [5:08:54<2:51:06, 136.88s/it]=>> [Epoch 000] Global Step 000125 =>> LR :: 0.000020 - Loss :: 0.0000:  63%|██████▎   | 126/200 [5:13:54<3:49:11, 185.83s/it]=>> [Epoch 000] Global Step 000126 =>> LR :: 0.000020 - Loss :: 0.0000:  63%|██████▎   | 126/200 [5:13:54<3:49:11, 185.83s/it]=>> [Epoch 000] Global Step 000126 =>> LR :: 0.000020 - Loss :: 0.0000:  64%|██████▎   | 127/200 [5:14:51<2:58:48, 146.96s/it]=>> [Epoch 000] Global Step 000127 =>> LR :: 0.000020 - Loss :: 0.0000:  64%|██████▎   | 127/200 [5:14:51<2:58:48, 146.96s/it]=>> [Epoch 000] Global Step 000127 =>> LR :: 0.000020 - Loss :: 0.0000:  64%|██████▍   | 128/200 [5:15:39<2:21:01, 117.53s/it]=>> [Epoch 000] Global Step 000128 =>> LR :: 0.000020 - Loss :: 0.0000:  64%|██████▍   | 128/200 [5:15:39<2:21:01, 117.53s/it]=>> [Epoch 000] Global Step 000128 =>> LR :: 0.000020 - Loss :: 0.0000:  64%|██████▍   | 129/200 [5:19:13<2:53:05, 146.27s/it]=>> [Epoch 000] Global Step 000129 =>> LR :: 0.000020 - Loss :: 0.0000:  64%|██████▍   | 129/200 [5:19:13<2:53:05, 146.27s/it]=>> [Epoch 000] Global Step 000129 =>> LR :: 0.000020 - Loss :: 0.0000:  65%|██████▌   | 130/200 [5:20:11<2:20:01, 120.02s/it]=>> [Epoch 000] Global Step 000130 =>> LR :: 0.000020 - Loss :: 0.0000:  65%|██████▌   | 130/200 [5:20:12<2:20:01, 120.02s/it]=>> [Epoch 000] Global Step 000130 =>> LR :: 0.000020 - Loss :: 0.0000:  66%|██████▌   | 131/200 [5:22:06<2:16:17, 118.52s/it]=>> [Epoch 000] Global Step 000131 =>> LR :: 0.000020 - Loss :: 0.0000:  66%|██████▌   | 131/200 [5:22:06<2:16:17, 118.52s/it]=>> [Epoch 000] Global Step 000131 =>> LR :: 0.000020 - Loss :: 0.0000:  66%|██████▌   | 132/200 [5:23:02<1:53:02, 99.74s/it] =>> [Epoch 000] Global Step 000132 =>> LR :: 0.000020 - Loss :: 0.0000:  66%|██████▌   | 132/200 [5:23:02<1:53:02, 99.74s/it]=>> [Epoch 000] Global Step 000132 =>> LR :: 0.000020 - Loss :: 0.0000:  66%|██████▋   | 133/200 [5:28:55<3:16:07, 175.64s/it]=>> [Epoch 000] Global Step 000133 =>> LR :: 0.000020 - Loss :: 0.0000:  66%|██████▋   | 133/200 [5:28:55<3:16:07, 175.64s/it]=>> [Epoch 000] Global Step 000133 =>> LR :: 0.000020 - Loss :: 0.0000:  67%|██████▋   | 134/200 [5:29:50<2:33:18, 139.37s/it]=>> [Epoch 000] Global Step 000134 =>> LR :: 0.000020 - Loss :: 0.0000:  67%|██████▋   | 134/200 [5:29:50<2:33:18, 139.37s/it]=>> [Epoch 000] Global Step 000134 =>> LR :: 0.000020 - Loss :: 0.0000:  68%|██████▊   | 135/200 [5:33:25<2:55:34, 162.07s/it]=>> [Epoch 000] Global Step 000135 =>> LR :: 0.000020 - Loss :: 0.0000:  68%|██████▊   | 135/200 [5:33:25<2:55:34, 162.07s/it]=>> [Epoch 000] Global Step 000135 =>> LR :: 0.000020 - Loss :: 0.0000:  68%|██████▊   | 136/200 [5:34:16<2:17:18, 128.72s/it]=>> [Epoch 000] Global Step 000136 =>> LR :: 0.000020 - Loss :: 0.0000:  68%|██████▊   | 136/200 [5:34:16<2:17:18, 128.72s/it]=>> [Epoch 000] Global Step 000136 =>> LR :: 0.000020 - Loss :: 0.0000:  68%|██████▊   | 137/200 [5:35:10<1:51:34, 106.26s/it]=>> [Epoch 000] Global Step 000137 =>> LR :: 0.000020 - Loss :: 0.0000:  68%|██████▊   | 137/200 [5:35:10<1:51:34, 106.26s/it]=>> [Epoch 000] Global Step 000137 =>> LR :: 0.000020 - Loss :: 0.0000:  69%|██████▉   | 138/200 [5:37:00<1:50:57, 107.38s/it]=>> [Epoch 000] Global Step 000138 =>> LR :: 0.000020 - Loss :: 0.0000:  69%|██████▉   | 138/200 [5:37:00<1:50:57, 107.38s/it]=>> [Epoch 000] Global Step 000138 =>> LR :: 0.000020 - Loss :: 0.0000:  70%|██████▉   | 139/200 [5:37:51<1:32:06, 90.60s/it] =>> [Epoch 000] Global Step 000139 =>> LR :: 0.000020 - Loss :: 0.0000:  70%|██████▉   | 139/200 [5:37:51<1:32:06, 90.60s/it]=>> [Epoch 000] Global Step 000139 =>> LR :: 0.000020 - Loss :: 0.0000:  70%|███████   | 140/200 [5:41:41<2:12:18, 132.30s/it]=>> [Epoch 000] Global Step 000140 =>> LR :: 0.000020 - Loss :: 0.0000:  70%|███████   | 140/200 [5:41:41<2:12:18, 132.30s/it]=>> [Epoch 000] Global Step 000140 =>> LR :: 0.000020 - Loss :: 0.0000:  70%|███████   | 141/200 [5:42:33<1:46:25, 108.23s/it]=>> [Epoch 000] Global Step 000141 =>> LR :: 0.000020 - Loss :: 0.0000:  70%|███████   | 141/200 [5:42:33<1:46:25, 108.23s/it]=>> [Epoch 000] Global Step 000141 =>> LR :: 0.000020 - Loss :: 0.0000:  71%|███████   | 142/200 [5:46:29<2:21:35, 146.47s/it]=>> [Epoch 000] Global Step 000142 =>> LR :: 0.000020 - Loss :: 0.0000:  71%|███████   | 142/200 [5:46:29<2:21:35, 146.47s/it]=>> [Epoch 000] Global Step 000142 =>> LR :: 0.000020 - Loss :: 0.0000:  72%|███████▏  | 143/200 [5:47:20<1:52:13, 118.13s/it]=>> [Epoch 000] Global Step 000143 =>> LR :: 0.000020 - Loss :: 0.0000:  72%|███████▏  | 143/200 [5:47:20<1:52:13, 118.13s/it]=>> [Epoch 000] Global Step 000143 =>> LR :: 0.000020 - Loss :: 0.0000:  72%|███████▏  | 144/200 [5:48:13<1:31:56, 98.51s/it] =>> [Epoch 000] Global Step 000144 =>> LR :: 0.000020 - Loss :: 0.0000:  72%|███████▏  | 144/200 [5:48:13<1:31:56, 98.51s/it]=>> [Epoch 000] Global Step 000144 =>> LR :: 0.000020 - Loss :: 0.0000:  72%|███████▎  | 145/200 [5:54:49<2:52:02, 187.68s/it]=>> [Epoch 000] Global Step 000145 =>> LR :: 0.000020 - Loss :: 0.0000:  72%|███████▎  | 145/200 [5:54:49<2:52:02, 187.68s/it]=>> [Epoch 000] Global Step 000145 =>> LR :: 0.000020 - Loss :: 0.0000:  73%|███████▎  | 146/200 [5:55:43<2:12:50, 147.60s/it]=>> [Epoch 000] Global Step 000146 =>> LR :: 0.000020 - Loss :: 0.0000:  73%|███████▎  | 146/200 [5:55:43<2:12:50, 147.60s/it]=>> [Epoch 000] Global Step 000146 =>> LR :: 0.000020 - Loss :: 0.0000:  74%|███████▎  | 147/200 [5:58:57<2:22:37, 161.46s/it]=>> [Epoch 000] Global Step 000147 =>> LR :: 0.000020 - Loss :: 0.0000:  74%|███████▎  | 147/200 [5:58:57<2:22:37, 161.46s/it]=>> [Epoch 000] Global Step 000147 =>> LR :: 0.000020 - Loss :: 0.0000:  74%|███████▍  | 148/200 [5:59:53<1:52:28, 129.78s/it]=>> [Epoch 000] Global Step 000148 =>> LR :: 0.000020 - Loss :: 0.0000:  74%|███████▍  | 148/200 [5:59:53<1:52:28, 129.78s/it]=>> [Epoch 000] Global Step 000148 =>> LR :: 0.000020 - Loss :: 0.0000:  74%|███████▍  | 149/200 [6:03:14<2:08:25, 151.10s/it]=>> [Epoch 000] Global Step 000149 =>> LR :: 0.000020 - Loss :: 0.0000:  74%|███████▍  | 149/200 [6:03:14<2:08:25, 151.10s/it]=>> [Epoch 000] Global Step 000149 =>> LR :: 0.000020 - Loss :: 0.0000:  75%|███████▌  | 150/200 [6:04:12<1:42:40, 123.20s/it]=>> [Epoch 000] Global Step 000150 =>> LR :: 0.000020 - Loss :: 0.0000:  75%|███████▌  | 150/200 [6:04:12<1:42:40, 123.20s/it]=>> [Epoch 000] Global Step 000150 =>> LR :: 0.000020 - Loss :: 0.0000:  76%|███████▌  | 151/200 [6:05:12<1:25:17, 104.44s/it]=>> [Epoch 000] Global Step 000151 =>> LR :: 0.000020 - Loss :: 0.0000:  76%|███████▌  | 151/200 [6:05:12<1:25:17, 104.44s/it]=>> [Epoch 000] Global Step 000151 =>> LR :: 0.000020 - Loss :: 0.0000:  76%|███████▌  | 152/200 [6:09:33<2:00:56, 151.18s/it]=>> [Epoch 000] Global Step 000152 =>> LR :: 0.000020 - Loss :: 0.0000:  76%|███████▌  | 152/200 [6:09:33<2:00:56, 151.18s/it]=>> [Epoch 000] Global Step 000152 =>> LR :: 0.000020 - Loss :: 0.0000:  76%|███████▋  | 153/200 [6:10:30<1:36:29, 123.19s/it]=>> [Epoch 000] Global Step 000153 =>> LR :: 0.000020 - Loss :: 0.0000:  76%|███████▋  | 153/200 [6:10:30<1:36:29, 123.19s/it]=>> [Epoch 000] Global Step 000153 =>> LR :: 0.000020 - Loss :: 0.0000:  77%|███████▋  | 154/200 [6:14:17<1:58:07, 154.07s/it]=>> [Epoch 000] Global Step 000154 =>> LR :: 0.000020 - Loss :: 0.0000:  77%|███████▋  | 154/200 [6:14:17<1:58:07, 154.07s/it]=>> [Epoch 000] Global Step 000154 =>> LR :: 0.000020 - Loss :: 0.0000:  78%|███████▊  | 155/200 [6:15:14<1:33:52, 125.16s/it]=>> [Epoch 000] Global Step 000155 =>> LR :: 0.000020 - Loss :: 0.0000:  78%|███████▊  | 155/200 [6:15:14<1:33:52, 125.16s/it]=>> [Epoch 000] Global Step 000155 =>> LR :: 0.000020 - Loss :: 0.0000:  78%|███████▊  | 156/200 [6:19:00<1:53:59, 155.43s/it]=>> [Epoch 000] Global Step 000156 =>> LR :: 0.000020 - Loss :: 0.0000:  78%|███████▊  | 156/200 [6:19:00<1:53:59, 155.43s/it]=>> [Epoch 000] Global Step 000156 =>> LR :: 0.000020 - Loss :: 0.0000:  78%|███████▊  | 157/200 [6:19:56<1:29:54, 125.46s/it]=>> [Epoch 000] Global Step 000157 =>> LR :: 0.000020 - Loss :: 0.0000:  78%|███████▊  | 157/200 [6:19:56<1:29:54, 125.46s/it]=>> [Epoch 000] Global Step 000157 =>> LR :: 0.000020 - Loss :: 0.0000:  79%|███████▉  | 158/200 [6:20:53<1:13:22, 104.83s/it]=>> [Epoch 000] Global Step 000158 =>> LR :: 0.000020 - Loss :: 0.0000:  79%|███████▉  | 158/200 [6:20:53<1:13:22, 104.83s/it]=>> [Epoch 000] Global Step 000158 =>> LR :: 0.000020 - Loss :: 0.0000:  80%|███████▉  | 159/200 [6:25:39<1:48:47, 159.21s/it]=>> [Epoch 000] Global Step 000159 =>> LR :: 0.000020 - Loss :: 0.0000:  80%|███████▉  | 159/200 [6:25:39<1:48:47, 159.21s/it]=>> [Epoch 000] Global Step 000159 =>> LR :: 0.000020 - Loss :: 0.0000:  80%|████████  | 160/200 [6:26:32<1:24:53, 127.34s/it]=>> [Epoch 000] Global Step 000160 =>> LR :: 0.000020 - Loss :: 0.0000:  80%|████████  | 160/200 [6:26:32<1:24:53, 127.34s/it]=>> [Epoch 000] Global Step 000160 =>> LR :: 0.000020 - Loss :: 0.0000:  80%|████████  | 161/200 [6:30:35<1:45:26, 162.22s/it]=>> [Epoch 000] Global Step 000161 =>> LR :: 0.000020 - Loss :: 0.0000:  80%|████████  | 161/200 [6:30:35<1:45:26, 162.22s/it]=>> [Epoch 000] Global Step 000161 =>> LR :: 0.000020 - Loss :: 0.0000:  81%|████████  | 162/200 [6:31:27<1:21:48, 129.18s/it]=>> [Epoch 000] Global Step 000162 =>> LR :: 0.000020 - Loss :: 0.0000:  81%|████████  | 162/200 [6:31:27<1:21:48, 129.18s/it]=>> [Epoch 000] Global Step 000162 =>> LR :: 0.000020 - Loss :: 0.0000:  82%|████████▏ | 163/200 [6:34:59<1:34:50, 153.78s/it]=>> [Epoch 000] Global Step 000163 =>> LR :: 0.000020 - Loss :: 0.0000:  82%|████████▏ | 163/200 [6:34:59<1:34:50, 153.78s/it]=>> [Epoch 000] Global Step 000163 =>> LR :: 0.000020 - Loss :: 0.0000:  82%|████████▏ | 164/200 [6:35:54<1:14:32, 124.23s/it]=>> [Epoch 000] Global Step 000164 =>> LR :: 0.000020 - Loss :: 0.0000:  82%|████████▏ | 164/200 [6:35:54<1:14:32, 124.23s/it]=>> [Epoch 000] Global Step 000164 =>> LR :: 0.000020 - Loss :: 0.0000:  82%|████████▎ | 165/200 [6:36:49<1:00:24, 103.55s/it]=>> [Epoch 000] Global Step 000165 =>> LR :: 0.000020 - Loss :: 0.0000:  82%|████████▎ | 165/200 [6:36:49<1:00:24, 103.55s/it]=>> [Epoch 000] Global Step 000165 =>> LR :: 0.000020 - Loss :: 0.0000:  83%|████████▎ | 166/200 [6:42:04<1:34:34, 166.89s/it]=>> [Epoch 000] Global Step 000166 =>> LR :: 0.000020 - Loss :: 0.0000:  83%|████████▎ | 166/200 [6:42:04<1:34:34, 166.89s/it]=>> [Epoch 000] Global Step 000166 =>> LR :: 0.000020 - Loss :: 0.0000:  84%|████████▎ | 167/200 [6:43:03<1:13:59, 134.54s/it]=>> [Epoch 000] Global Step 000167 =>> LR :: 0.000020 - Loss :: 0.0000:  84%|████████▎ | 167/200 [6:43:03<1:13:59, 134.54s/it]=>> [Epoch 000] Global Step 000167 =>> LR :: 0.000020 - Loss :: 0.0000:  84%|████████▍ | 168/200 [6:45:36<1:14:45, 140.18s/it]=>> [Epoch 000] Global Step 000168 =>> LR :: 0.000020 - Loss :: 0.0000:  84%|████████▍ | 168/200 [6:45:36<1:14:45, 140.18s/it]=>> [Epoch 000] Global Step 000168 =>> LR :: 0.000020 - Loss :: 0.0000:  84%|████████▍ | 169/200 [6:46:37<1:00:09, 116.43s/it]=>> [Epoch 000] Global Step 000169 =>> LR :: 0.000020 - Loss :: 0.0000:  84%|████████▍ | 169/200 [6:46:37<1:00:09, 116.43s/it]=>> [Epoch 000] Global Step 000169 =>> LR :: 0.000020 - Loss :: 0.0000:  85%|████████▌ | 170/200 [6:52:16<1:31:31, 183.05s/it]=>> [Epoch 000] Global Step 000170 =>> LR :: 0.000020 - Loss :: 0.0000:  85%|████████▌ | 170/200 [6:52:16<1:31:31, 183.05s/it]=>> [Epoch 000] Global Step 000170 =>> LR :: 0.000020 - Loss :: 0.0000:  86%|████████▌ | 171/200 [6:53:08<1:09:34, 143.95s/it]=>> [Epoch 000] Global Step 000171 =>> LR :: 0.000020 - Loss :: 0.0000:  86%|████████▌ | 171/200 [6:53:08<1:09:34, 143.95s/it]=>> [Epoch 000] Global Step 000171 =>> LR :: 0.000020 - Loss :: 0.0000:  86%|████████▌ | 172/200 [6:55:44<1:08:50, 147.52s/it]=>> [Epoch 000] Global Step 000172 =>> LR :: 0.000020 - Loss :: 0.0000:  86%|████████▌ | 172/200 [6:55:44<1:08:50, 147.52s/it]=>> [Epoch 000] Global Step 000172 =>> LR :: 0.000020 - Loss :: 0.0000:  86%|████████▋ | 173/200 [6:56:40<53:58, 119.94s/it]  =>> [Epoch 000] Global Step 000173 =>> LR :: 0.000020 - Loss :: 0.0000:  86%|████████▋ | 173/200 [6:56:40<53:58, 119.94s/it]=>> [Epoch 000] Global Step 000173 =>> LR :: 0.000020 - Loss :: 0.0000:  87%|████████▋ | 174/200 [6:57:39<44:02, 101.64s/it]=>> [Epoch 000] Global Step 000174 =>> LR :: 0.000020 - Loss :: 0.0000:  87%|████████▋ | 174/200 [6:57:39<44:02, 101.64s/it]=>> [Epoch 000] Global Step 000174 =>> LR :: 0.000020 - Loss :: 0.0000:  88%|████████▊ | 175/200 [7:01:05<55:21, 132.87s/it]=>> [Epoch 000] Global Step 000175 =>> LR :: 0.000020 - Loss :: 0.0000:  88%|████████▊ | 175/200 [7:01:05<55:21, 132.87s/it]=>> [Epoch 000] Global Step 000175 =>> LR :: 0.000020 - Loss :: 0.0000:  88%|████████▊ | 176/200 [7:01:59<43:41, 109.24s/it]=>> [Epoch 000] Global Step 000176 =>> LR :: 0.000020 - Loss :: 0.0000:  88%|████████▊ | 176/200 [7:01:59<43:41, 109.24s/it]=>> [Epoch 000] Global Step 000176 =>> LR :: 0.000020 - Loss :: 0.0000:  88%|████████▊ | 177/200 [7:04:40<47:48, 124.74s/it]=>> [Epoch 000] Global Step 000177 =>> LR :: 0.000020 - Loss :: 0.0000:  88%|████████▊ | 177/200 [7:04:40<47:48, 124.74s/it]=>> [Epoch 000] Global Step 000177 =>> LR :: 0.000020 - Loss :: 0.0000:  89%|████████▉ | 178/200 [7:05:37<38:17, 104.45s/it]=>> [Epoch 000] Global Step 000178 =>> LR :: 0.000020 - Loss :: 0.0000:  89%|████████▉ | 178/200 [7:05:37<38:17, 104.45s/it]=>> [Epoch 000] Global Step 000178 =>> LR :: 0.000020 - Loss :: 0.0000:  90%|████████▉ | 179/200 [7:11:39<1:03:39, 181.89s/it]=>> [Epoch 000] Global Step 000179 =>> LR :: 0.000020 - Loss :: 0.0000:  90%|████████▉ | 179/200 [7:11:39<1:03:39, 181.89s/it]=>> [Epoch 000] Global Step 000179 =>> LR :: 0.000020 - Loss :: 0.0000:  90%|█████████ | 180/200 [7:12:32<47:43, 143.16s/it]  =>> [Epoch 000] Global Step 000180 =>> LR :: 0.000020 - Loss :: 0.0000:  90%|█████████ | 180/200 [7:12:32<47:43, 143.16s/it]=>> [Epoch 000] Global Step 000180 =>> LR :: 0.000020 - Loss :: 0.0000:  90%|█████████ | 181/200 [7:13:25<36:47, 116.19s/it]=>> [Epoch 000] Global Step 000181 =>> LR :: 0.000020 - Loss :: 0.0000:  90%|█████████ | 181/200 [7:13:25<36:47, 116.19s/it]=>> [Epoch 000] Global Step 000181 =>> LR :: 0.000020 - Loss :: 0.0000:  91%|█████████ | 182/200 [7:15:43<36:46, 122.58s/it]=>> [Epoch 000] Global Step 000182 =>> LR :: 0.000020 - Loss :: 0.0000:  91%|█████████ | 182/200 [7:15:43<36:46, 122.58s/it]=>> [Epoch 000] Global Step 000182 =>> LR :: 0.000020 - Loss :: 0.0000:  92%|█████████▏| 183/200 [7:16:35<28:45, 101.50s/it]=>> [Epoch 000] Global Step 000183 =>> LR :: 0.000020 - Loss :: 0.0000:  92%|█████████▏| 183/200 [7:16:35<28:45, 101.50s/it]=>> [Epoch 000] Global Step 000183 =>> LR :: 0.000020 - Loss :: 0.0000:  92%|█████████▏| 184/200 [7:20:39<38:27, 144.20s/it]=>> [Epoch 000] Global Step 000184 =>> LR :: 0.000020 - Loss :: 0.0000:  92%|█████████▏| 184/200 [7:20:39<38:27, 144.20s/it]=>> [Epoch 000] Global Step 000184 =>> LR :: 0.000020 - Loss :: 0.0000:  92%|█████████▎| 185/200 [7:21:31<29:07, 116.51s/it]=>> [Epoch 000] Global Step 000185 =>> LR :: 0.000020 - Loss :: 0.0000:  92%|█████████▎| 185/200 [7:21:31<29:07, 116.51s/it]=>> [Epoch 000] Global Step 000185 =>> LR :: 0.000020 - Loss :: 0.0000:  93%|█████████▎| 186/200 [7:26:19<39:12, 168.03s/it]=>> [Epoch 000] Global Step 000186 =>> LR :: 0.000020 - Loss :: 0.0000:  93%|█████████▎| 186/200 [7:26:19<39:12, 168.03s/it]=>> [Epoch 000] Global Step 000186 =>> LR :: 0.000020 - Loss :: 0.0000:  94%|█████████▎| 187/200 [7:27:14<29:01, 133.98s/it]=>> [Epoch 000] Global Step 000187 =>> LR :: 0.000020 - Loss :: 0.0000:  94%|█████████▎| 187/200 [7:27:14<29:01, 133.98s/it]=>> [Epoch 000] Global Step 000187 =>> LR :: 0.000020 - Loss :: 0.0000:  94%|█████████▍| 188/200 [7:28:10<22:07, 110.64s/it]=>> [Epoch 000] Global Step 000188 =>> LR :: 0.000020 - Loss :: 0.0000:  94%|█████████▍| 188/200 [7:28:10<22:07, 110.64s/it]=>> [Epoch 000] Global Step 000188 =>> LR :: 0.000020 - Loss :: 0.0000:  94%|█████████▍| 189/200 [7:31:13<24:16, 132.45s/it]=>> [Epoch 000] Global Step 000189 =>> LR :: 0.000020 - Loss :: 0.0000:  94%|█████████▍| 189/200 [7:31:13<24:16, 132.45s/it]=>> [Epoch 000] Global Step 000189 =>> LR :: 0.000020 - Loss :: 0.0000:  95%|█████████▌| 190/200 [7:32:07<18:08, 108.86s/it]=>> [Epoch 000] Global Step 000190 =>> LR :: 0.000020 - Loss :: 0.0000:  95%|█████████▌| 190/200 [7:32:07<18:08, 108.86s/it]=>> [Epoch 000] Global Step 000190 =>> LR :: 0.000020 - Loss :: 0.0000:  96%|█████████▌| 191/200 [7:35:18<20:02, 133.58s/it]=>> [Epoch 000] Global Step 000191 =>> LR :: 0.000020 - Loss :: 0.0000:  96%|█████████▌| 191/200 [7:35:18<20:02, 133.58s/it]=>> [Epoch 000] Global Step 000191 =>> LR :: 0.000020 - Loss :: 0.0000:  96%|█████████▌| 192/200 [7:36:15<14:45, 110.65s/it]=>> [Epoch 000] Global Step 000192 =>> LR :: 0.000020 - Loss :: 0.0000:  96%|█████████▌| 192/200 [7:36:15<14:45, 110.65s/it]=>> [Epoch 000] Global Step 000192 =>> LR :: 0.000020 - Loss :: 0.0000:  96%|█████████▋| 193/200 [7:39:55<16:43, 143.40s/it]=>> [Epoch 000] Global Step 000193 =>> LR :: 0.000020 - Loss :: 0.0000:  96%|█████████▋| 193/200 [7:39:55<16:43, 143.40s/it]=>> [Epoch 000] Global Step 000193 =>> LR :: 0.000020 - Loss :: 0.0000:  97%|█████████▋| 194/200 [7:40:51<11:41, 117.00s/it]=>> [Epoch 000] Global Step 000194 =>> LR :: 0.000020 - Loss :: 0.0000:  97%|█████████▋| 194/200 [7:40:51<11:41, 117.00s/it]=>> [Epoch 000] Global Step 000194 =>> LR :: 0.000020 - Loss :: 0.0000:  98%|█████████▊| 195/200 [7:41:57<08:29, 101.88s/it]=>> [Epoch 000] Global Step 000195 =>> LR :: 0.000020 - Loss :: 0.0000:  98%|█████████▊| 195/200 [7:41:57<08:29, 101.88s/it]=>> [Epoch 000] Global Step 000195 =>> LR :: 0.000020 - Loss :: 0.0000:  98%|█████████▊| 196/200 [7:46:55<10:43, 160.77s/it]=>> [Epoch 000] Global Step 000196 =>> LR :: 0.000020 - Loss :: 0.0000:  98%|█████████▊| 196/200 [7:46:55<10:43, 160.77s/it]=>> [Epoch 000] Global Step 000196 =>> LR :: 0.000020 - Loss :: 0.0000:  98%|█████████▊| 197/200 [7:47:48<06:25, 128.44s/it]=>> [Epoch 000] Global Step 000197 =>> LR :: 0.000020 - Loss :: 0.0000:  98%|█████████▊| 197/200 [7:47:48<06:25, 128.44s/it]=>> [Epoch 000] Global Step 000197 =>> LR :: 0.000020 - Loss :: 0.0000:  99%|█████████▉| 198/200 [7:51:23<05:08, 154.22s/it]=>> [Epoch 000] Global Step 000198 =>> LR :: 0.000020 - Loss :: 0.0000:  99%|█████████▉| 198/200 [7:51:23<05:08, 154.22s/it]=>> [Epoch 000] Global Step 000198 =>> LR :: 0.000020 - Loss :: 0.0000: 100%|█████████▉| 199/200 [7:52:14<02:03, 123.47s/it]=>> [Epoch 000] Global Step 000199 =>> LR :: 0.000020 - Loss :: 0.0000: 100%|█████████▉| 199/200 [7:52:14<02:03, 123.47s/it]