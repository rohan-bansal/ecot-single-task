2025-06-03 14:53:14.629694: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629694: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629697: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629700: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629698: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629698: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629701: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629701: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-03 14:53:14.629752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.629753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.629754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.629755: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.629754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.629754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.629754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.629793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-03 14:53:14.730107: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.730115: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.730117: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.730119: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.730119: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.730119: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.730119: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.730121: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-03 14:53:14.912482: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:14.912473: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:14.912481: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:14.912482: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:14.912473: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:14.912473: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:14.912478: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:14.912477: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-03 14:53:16.972960: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-03 14:53:16.972960: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-03 14:53:16.972960: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-03 14:53:16.973173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-03 14:53:16.973173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-03 14:53:16.973177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-03 14:53:16.973194: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-03 14:53:16.973223: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:20<00:20, 20.73s/it]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:22<00:22, 22.34s/it]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:22<00:22, 22.09s/it]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:22<00:22, 22.26s/it]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:18<00:18, 18.41s/it]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:21<00:21, 21.20s/it]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:20<00:20, 20.35s/it]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:21<00:21, 21.90s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:25<00:00, 11.68s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:25<00:00, 12.69s/it]

Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:29<00:00, 13.27s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:29<00:00, 14.57s/it]

Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:29<00:00, 13.43s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:29<00:00, 14.75s/it]

Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:28<00:00, 12.99s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:27<00:00, 12.64s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:28<00:00, 14.22s/it]

Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:27<00:00, 13.80s/it]

Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:28<00:00, 13.28s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:28<00:00, 14.40s/it]

Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:29<00:00, 13.36s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:29<00:00, 14.67s/it]

Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:29<00:00, 13.46s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:29<00:00, 14.80s/it]
2025-06-03 14:59:03.298670: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:03.339412: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:03.632052: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:04.257743: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:04.277843: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:04.550828: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:08.414132: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:08.685376: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:08.741591: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:08.808449: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:09.331513: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:09.569891: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:09.635338: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:09.700233: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:10.461145: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 14:59:11.500595: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:38.574779: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:43.270392: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:44.994913: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:49.028877: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:49.556076: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:50.445052: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:51.185591: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-06-03 15:00:52.845942: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
wandb: Currently logged in as: solace to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /srv/rl2-lab/flash7/rbansal66/embodied-CoT/runs/prism-dinosiglip-224px+mx-bridge+n1+b16+x7/wandb/run-20250603_150112-2w82amgw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prism-dinosiglip-224px+mx-bridge+n1+b16+x7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/solace/ecot_reproduce
wandb: üöÄ View run at https://wandb.ai/solace/ecot_reproduce/runs/2w82amgw

=>> [Epoch 000] Global Step 000000 =>> LR :: 0.000000:   0%|          | 0/200 [00:00<?, ?it/s]2025-06-03 15:03:39.786218: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:04:03.519247: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:04:13.452286: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:04:47.149747: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:05:51.719361: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:11:51.275956: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:12:20.816029: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 2 of 100
2025-06-03 15:13:45.523733: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:15:05.256491: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 1 of 100
2025-06-03 15:15:40.782201: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 2 of 100
2025-06-03 15:15:46.938828: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 5 of 100
2025-06-03 15:16:25.148824: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 2 of 100
2025-06-03 15:17:10.791294: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 2 of 100
2025-06-03 15:17:54.319232: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 41 of 100
2025-06-03 15:18:00.012315: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 41 of 100
2025-06-03 15:18:45.314705: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 42 of 100
2025-06-03 15:20:03.935869: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 41 of 100
2025-06-03 15:20:43.494944: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 41 of 100
2025-06-03 15:20:59.658002: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 41 of 100
2025-06-03 15:21:16.855911: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 41 of 100
2025-06-03 15:21:20.991030: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 41 of 100
2025-06-03 15:22:03.274062: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 77 of 100
2025-06-03 15:22:03.274619: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.
2025-06-03 15:22:24.242723: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 78 of 100
2025-06-03 15:22:24.243274: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.
2025-06-03 15:22:27.591170: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 77 of 100
2025-06-03 15:22:27.592762: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.
2025-06-03 15:24:57.406079: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 78 of 100
2025-06-03 15:24:57.407206: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.
2025-06-03 15:25:12.582787: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 78 of 100
2025-06-03 15:25:12.583454: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.
2025-06-03 15:25:28.007964: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 78 of 100
2025-06-03 15:25:28.008731: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.
2025-06-03 15:25:52.623061: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 78 of 100
2025-06-03 15:25:52.623878: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.
2025-06-03 15:26:04.877142: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:45: Filling up shuffle buffer (this may take a while): 78 of 100
2025-06-03 15:26:04.877632: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.

=>> [Epoch 000] Global Step 000000 =>> LR :: 0.000000:   0%|          | 1/200 [33:23<110:45:55, 2003.80s/it]
=>> [Epoch 000] Global Step 000001 =>> LR :: 0.000020 - Loss :: 2.2888:   0%|          | 1/200 [33:23<110:45:55, 2003.80s/it]
=>> [Epoch 000] Global Step 000001 =>> LR :: 0.000020 - Loss :: 2.2888:   1%|          | 2/200 [34:17<47:07:48, 856.91s/it]  
=>> [Epoch 000] Global Step 000002 =>> LR :: 0.000020 - Loss :: 1.4491:   1%|          | 2/200 [34:17<47:07:48, 856.91s/it]
=>> [Epoch 000] Global Step 000002 =>> LR :: 0.000020 - Loss :: 1.4491:   2%|‚ñè         | 3/200 [35:13<26:51:30, 490.81s/it]
=>> [Epoch 000] Global Step 000003 =>> LR :: 0.000020 - Loss :: 0.9524:   2%|‚ñè         | 3/200 [35:13<26:51:30, 490.81s/it]
=>> [Epoch 000] Global Step 000003 =>> LR :: 0.000020 - Loss :: 0.9524:   2%|‚ñè         | 4/200 [36:09<17:22:41, 319.19s/it]
=>> [Epoch 000] Global Step 000004 =>> LR :: 0.000020 - Loss :: 0.8238:   2%|‚ñè         | 4/200 [36:09<17:22:41, 319.19s/it]
=>> [Epoch 000] Global Step 000004 =>> LR :: 0.000020 - Loss :: 0.8238:   2%|‚ñé         | 5/200 [37:06<12:10:31, 224.77s/it]
=>> [Epoch 000] Global Step 000005 =>> LR :: 0.000020 - Loss :: 0.7091:   2%|‚ñé         | 5/200 [37:06<12:10:31, 224.77s/it]
=>> [Epoch 000] Global Step 000005 =>> LR :: 0.000020 - Loss :: 0.7091:   3%|‚ñé         | 6/200 [41:52<13:14:07, 245.60s/it]
=>> [Epoch 000] Global Step 000006 =>> LR :: 0.000020 - Loss :: 0.5890:   3%|‚ñé         | 6/200 [41:52<13:14:07, 245.60s/it]
=>> [Epoch 000] Global Step 000006 =>> LR :: 0.000020 - Loss :: 0.5890:   4%|‚ñé         | 7/200 [42:46<9:48:02, 182.81s/it] 
=>> [Epoch 000] Global Step 000007 =>> LR :: 0.000020 - Loss :: 0.4666:   4%|‚ñé         | 7/200 [42:46<9:48:02, 182.81s/it]
=>> [Epoch 000] Global Step 000007 =>> LR :: 0.000020 - Loss :: 0.4666:   4%|‚ñç         | 8/200 [46:46<10:43:49, 201.19s/it]
=>> [Epoch 000] Global Step 000008 =>> LR :: 0.000020 - Loss :: 0.4459:   4%|‚ñç         | 8/200 [46:46<10:43:49, 201.19s/it]
=>> [Epoch 000] Global Step 000008 =>> LR :: 0.000020 - Loss :: 0.4459:   4%|‚ñç         | 9/200 [47:43<8:16:46, 156.05s/it] 
=>> [Epoch 000] Global Step 000009 =>> LR :: 0.000020 - Loss :: 0.3646:   4%|‚ñç         | 9/200 [47:43<8:16:46, 156.05s/it]
=>> [Epoch 000] Global Step 000009 =>> LR :: 0.000020 - Loss :: 0.3646:   5%|‚ñå         | 10/200 [48:37<6:34:34, 124.60s/it]
=>> [Epoch 000] Global Step 000010 =>> LR :: 0.000020 - Loss :: 0.3455:   5%|‚ñå         | 10/200 [48:37<6:34:34, 124.60s/it]
=>> [Epoch 000] Global Step 000010 =>> LR :: 0.000020 - Loss :: 0.3455:   6%|‚ñå         | 11/200 [51:13<7:02:39, 134.18s/it]
=>> [Epoch 000] Global Step 000011 =>> LR :: 0.000020 - Loss :: 0.3256:   6%|‚ñå         | 11/200 [51:13<7:02:39, 134.18s/it]
=>> [Epoch 000] Global Step 000011 =>> LR :: 0.000020 - Loss :: 0.3256:   6%|‚ñå         | 12/200 [52:08<5:44:55, 110.08s/it]
=>> [Epoch 000] Global Step 000012 =>> LR :: 0.000020 - Loss :: 0.2725:   6%|‚ñå         | 12/200 [52:08<5:44:55, 110.08s/it]
=>> [Epoch 000] Global Step 000012 =>> LR :: 0.000020 - Loss :: 0.2725:   6%|‚ñã         | 13/200 [55:17<6:57:28, 133.95s/it]
=>> [Epoch 000] Global Step 000013 =>> LR :: 0.000020 - Loss :: 0.2576:   6%|‚ñã         | 13/200 [55:17<6:57:28, 133.95s/it]
=>> [Epoch 000] Global Step 000013 =>> LR :: 0.000020 - Loss :: 0.2576:   7%|‚ñã         | 14/200 [56:11<5:40:18, 109.78s/it]
=>> [Epoch 000] Global Step 000014 =>> LR :: 0.000020 - Loss :: 0.2194:   7%|‚ñã         | 14/200 [56:11<5:40:18, 109.78s/it]
=>> [Epoch 000] Global Step 000014 =>> LR :: 0.000020 - Loss :: 0.2194:   8%|‚ñä         | 15/200 [1:00:42<8:08:56, 158.57s/it]
=>> [Epoch 000] Global Step 000015 =>> LR :: 0.000020 - Loss :: 0.1842:   8%|‚ñä         | 15/200 [1:00:42<8:08:56, 158.57s/it]
=>> [Epoch 000] Global Step 000015 =>> LR :: 0.000020 - Loss :: 0.1842:   8%|‚ñä         | 16/200 [1:01:35<6:28:04, 126.55s/it]
=>> [Epoch 000] Global Step 000016 =>> LR :: 0.000020 - Loss :: 0.1401:   8%|‚ñä         | 16/200 [1:01:35<6:28:04, 126.55s/it]
=>> [Epoch 000] Global Step 000016 =>> LR :: 0.000020 - Loss :: 0.1401:   8%|‚ñä         | 17/200 [1:02:30<5:20:27, 105.07s/it]
=>> [Epoch 000] Global Step 000017 =>> LR :: 0.000020 - Loss :: 0.1496:   8%|‚ñä         | 17/200 [1:02:30<5:20:27, 105.07s/it]
=>> [Epoch 000] Global Step 000017 =>> LR :: 0.000020 - Loss :: 0.1496:   9%|‚ñâ         | 18/200 [1:06:49<7:39:23, 151.45s/it]
=>> [Epoch 000] Global Step 000018 =>> LR :: 0.000020 - Loss :: 0.1252:   9%|‚ñâ         | 18/200 [1:06:49<7:39:23, 151.45s/it]
=>> [Epoch 000] Global Step 000018 =>> LR :: 0.000020 - Loss :: 0.1252:  10%|‚ñâ         | 19/200 [1:07:44<6:09:27, 122.47s/it]
=>> [Epoch 000] Global Step 000019 =>> LR :: 0.000020 - Loss :: 0.1086:  10%|‚ñâ         | 19/200 [1:07:44<6:09:27, 122.47s/it]
=>> [Epoch 000] Global Step 000019 =>> LR :: 0.000020 - Loss :: 0.1086:  10%|‚ñà         | 20/200 [1:12:48<8:51:00, 177.00s/it]
=>> [Epoch 000] Global Step 000020 =>> LR :: 0.000020 - Loss :: 0.1099:  10%|‚ñà         | 20/200 [1:12:48<8:51:00, 177.00s/it]
=>> [Epoch 000] Global Step 000020 =>> LR :: 0.000020 - Loss :: 0.1099:  10%|‚ñà         | 21/200 [1:13:47<7:02:24, 141.59s/it]
=>> [Epoch 000] Global Step 000021 =>> LR :: 0.000020 - Loss :: 0.0722:  10%|‚ñà         | 21/200 [1:13:47<7:02:24, 141.59s/it]
=>> [Epoch 000] Global Step 000021 =>> LR :: 0.000020 - Loss :: 0.0722:  11%|‚ñà         | 22/200 [1:17:44<8:25:07, 170.27s/it]
=>> [Epoch 000] Global Step 000022 =>> LR :: 0.000020 - Loss :: 0.0673:  11%|‚ñà         | 22/200 [1:17:44<8:25:07, 170.27s/it]
=>> [Epoch 000] Global Step 000022 =>> LR :: 0.000020 - Loss :: 0.0673:  12%|‚ñà‚ñè        | 23/200 [1:18:42<6:42:15, 136.36s/it]
=>> [Epoch 000] Global Step 000023 =>> LR :: 0.000020 - Loss :: 0.0625:  12%|‚ñà‚ñè        | 23/200 [1:18:42<6:42:15, 136.36s/it]
=>> [Epoch 000] Global Step 000023 =>> LR :: 0.000020 - Loss :: 0.0625:  12%|‚ñà‚ñè        | 24/200 [1:22:05<7:39:19, 156.59s/it]
=>> [Epoch 000] Global Step 000024 =>> LR :: 0.000020 - Loss :: 0.0529:  12%|‚ñà‚ñè        | 24/200 [1:22:05<7:39:19, 156.59s/it]
=>> [Epoch 000] Global Step 000024 =>> LR :: 0.000020 - Loss :: 0.0529:  12%|‚ñà‚ñé        | 25/200 [1:23:02<6:09:37, 126.73s/it]
=>> [Epoch 000] Global Step 000025 =>> LR :: 0.000020 - Loss :: 0.0412:  12%|‚ñà‚ñé        | 25/200 [1:23:02<6:09:37, 126.73s/it]
=>> [Epoch 000] Global Step 000025 =>> LR :: 0.000020 - Loss :: 0.0412:  13%|‚ñà‚ñé        | 26/200 [1:23:59<5:06:42, 105.76s/it]
=>> [Epoch 000] Global Step 000026 =>> LR :: 0.000020 - Loss :: 0.0721:  13%|‚ñà‚ñé        | 26/200 [1:23:59<5:06:42, 105.76s/it]
=>> [Epoch 000] Global Step 000026 =>> LR :: 0.000020 - Loss :: 0.0721:  14%|‚ñà‚ñé        | 27/200 [1:29:48<8:34:43, 178.52s/it]
=>> [Epoch 000] Global Step 000027 =>> LR :: 0.000020 - Loss :: 0.0476:  14%|‚ñà‚ñé        | 27/200 [1:29:48<8:34:43, 178.52s/it]
=>> [Epoch 000] Global Step 000027 =>> LR :: 0.000020 - Loss :: 0.0476:  14%|‚ñà‚ñç        | 28/200 [1:30:43<6:46:03, 141.65s/it]
=>> [Epoch 000] Global Step 000028 =>> LR :: 0.000020 - Loss :: 0.0455:  14%|‚ñà‚ñç        | 28/200 [1:30:43<6:46:03, 141.65s/it]
=>> [Epoch 000] Global Step 000028 =>> LR :: 0.000020 - Loss :: 0.0455:  14%|‚ñà‚ñç        | 29/200 [1:33:53<7:24:41, 156.03s/it]
=>> [Epoch 000] Global Step 000029 =>> LR :: 0.000020 - Loss :: 0.0323:  14%|‚ñà‚ñç        | 29/200 [1:33:53<7:24:41, 156.03s/it]
=>> [Epoch 000] Global Step 000029 =>> LR :: 0.000020 - Loss :: 0.0323:  15%|‚ñà‚ñå        | 30/200 [1:34:49<5:57:04, 126.03s/it]
=>> [Epoch 000] Global Step 000030 =>> LR :: 0.000020 - Loss :: 0.0312:  15%|‚ñà‚ñå        | 30/200 [1:34:49<5:57:04, 126.03s/it]
=>> [Epoch 000] Global Step 000030 =>> LR :: 0.000020 - Loss :: 0.0312:  16%|‚ñà‚ñå        | 31/200 [1:38:41<7:24:57, 157.97s/it]
=>> [Epoch 000] Global Step 000031 =>> LR :: 0.000020 - Loss :: 0.0228:  16%|‚ñà‚ñå        | 31/200 [1:38:41<7:24:57, 157.97s/it]
=>> [Epoch 000] Global Step 000031 =>> LR :: 0.000020 - Loss :: 0.0228:  16%|‚ñà‚ñå        | 32/200 [1:39:33<5:52:39, 125.95s/it]
=>> [Epoch 000] Global Step 000032 =>> LR :: 0.000020 - Loss :: 0.0176:  16%|‚ñà‚ñå        | 32/200 [1:39:33<5:52:39, 125.95s/it]
=>> [Epoch 000] Global Step 000032 =>> LR :: 0.000020 - Loss :: 0.0176:  16%|‚ñà‚ñã        | 33/200 [1:40:24<4:48:11, 103.54s/it]
=>> [Epoch 000] Global Step 000033 =>> LR :: 0.000020 - Loss :: 0.0113:  16%|‚ñà‚ñã        | 33/200 [1:40:24<4:48:11, 103.54s/it]
=>> [Epoch 000] Global Step 000033 =>> LR :: 0.000020 - Loss :: 0.0113:  17%|‚ñà‚ñã        | 34/200 [1:44:00<6:19:42, 137.24s/it]
=>> [Epoch 000] Global Step 000034 =>> LR :: 0.000020 - Loss :: 0.0092:  17%|‚ñà‚ñã        | 34/200 [1:44:00<6:19:42, 137.24s/it]
=>> [Epoch 000] Global Step 000034 =>> LR :: 0.000020 - Loss :: 0.0092:  18%|‚ñà‚ñä        | 35/200 [1:44:56<5:10:53, 113.05s/it]
=>> [Epoch 000] Global Step 000035 =>> LR :: 0.000020 - Loss :: 0.0131:  18%|‚ñà‚ñä        | 35/200 [1:44:56<5:10:53, 113.05s/it]
=>> [Epoch 000] Global Step 000035 =>> LR :: 0.000020 - Loss :: 0.0131:  18%|‚ñà‚ñä        | 36/200 [1:49:53<7:39:27, 168.10s/it]
=>> [Epoch 000] Global Step 000036 =>> LR :: 0.000020 - Loss :: 0.0116:  18%|‚ñà‚ñä        | 36/200 [1:49:53<7:39:27, 168.10s/it]
=>> [Epoch 000] Global Step 000036 =>> LR :: 0.000020 - Loss :: 0.0116:  18%|‚ñà‚ñä        | 37/200 [1:50:45<6:02:09, 133.31s/it]
=>> [Epoch 000] Global Step 000037 =>> LR :: 0.000020 - Loss :: 0.0173:  18%|‚ñà‚ñä        | 37/200 [1:50:45<6:02:09, 133.31s/it]
=>> [Epoch 000] Global Step 000037 =>> LR :: 0.000020 - Loss :: 0.0173:  19%|‚ñà‚ñâ        | 38/200 [1:54:08<6:56:09, 154.14s/it]
=>> [Epoch 000] Global Step 000038 =>> LR :: 0.000020 - Loss :: 0.0092:  19%|‚ñà‚ñâ        | 38/200 [1:54:08<6:56:09, 154.14s/it]
=>> [Epoch 000] Global Step 000038 =>> LR :: 0.000020 - Loss :: 0.0092:  20%|‚ñà‚ñâ        | 39/200 [1:55:04<5:35:00, 124.85s/it]
=>> [Epoch 000] Global Step 000039 =>> LR :: 0.000020 - Loss :: 0.0084:  20%|‚ñà‚ñâ        | 39/200 [1:55:04<5:35:00, 124.85s/it]
=>> [Epoch 000] Global Step 000039 =>> LR :: 0.000020 - Loss :: 0.0084:  20%|‚ñà‚ñà        | 40/200 [1:56:00<4:38:01, 104.26s/it]
=>> [Epoch 000] Global Step 000040 =>> LR :: 0.000020 - Loss :: 0.0132:  20%|‚ñà‚ñà        | 40/200 [1:56:00<4:38:01, 104.26s/it]
=>> [Epoch 000] Global Step 000040 =>> LR :: 0.000020 - Loss :: 0.0132:  20%|‚ñà‚ñà        | 41/200 [2:00:01<6:24:44, 145.18s/it]
=>> [Epoch 000] Global Step 000041 =>> LR :: 0.000020 - Loss :: 0.0059:  20%|‚ñà‚ñà        | 41/200 [2:00:01<6:24:44, 145.18s/it]
=>> [Epoch 000] Global Step 000041 =>> LR :: 0.000020 - Loss :: 0.0059:  21%|‚ñà‚ñà        | 42/200 [2:00:52<5:08:11, 117.03s/it]
=>> [Epoch 000] Global Step 000042 =>> LR :: 0.000020 - Loss :: 0.0086:  21%|‚ñà‚ñà        | 42/200 [2:00:52<5:08:11, 117.03s/it]
=>> [Epoch 000] Global Step 000042 =>> LR :: 0.000020 - Loss :: 0.0086:  22%|‚ñà‚ñà‚ñè       | 43/200 [2:04:14<6:12:12, 142.25s/it]
=>> [Epoch 000] Global Step 000043 =>> LR :: 0.000020 - Loss :: 0.0078:  22%|‚ñà‚ñà‚ñè       | 43/200 [2:04:14<6:12:12, 142.25s/it]
=>> [Epoch 000] Global Step 000043 =>> LR :: 0.000020 - Loss :: 0.0078:  22%|‚ñà‚ñà‚ñè       | 44/200 [2:05:07<5:00:52, 115.72s/it]
=>> [Epoch 000] Global Step 000044 =>> LR :: 0.000020 - Loss :: 0.0039:  22%|‚ñà‚ñà‚ñè       | 44/200 [2:05:07<5:00:52, 115.72s/it]
=>> [Epoch 000] Global Step 000044 =>> LR :: 0.000020 - Loss :: 0.0039:  22%|‚ñà‚ñà‚ñé       | 45/200 [2:11:15<8:14:23, 191.38s/it]
=>> [Epoch 000] Global Step 000045 =>> LR :: 0.000020 - Loss :: 0.0059:  22%|‚ñà‚ñà‚ñé       | 45/200 [2:11:15<8:14:23, 191.38s/it]
=>> [Epoch 000] Global Step 000045 =>> LR :: 0.000020 - Loss :: 0.0059:  23%|‚ñà‚ñà‚ñé       | 46/200 [2:12:07<6:23:39, 149.48s/it]
=>> [Epoch 000] Global Step 000046 =>> LR :: 0.000020 - Loss :: 0.0082:  23%|‚ñà‚ñà‚ñé       | 46/200 [2:12:07<6:23:39, 149.48s/it]
=>> [Epoch 000] Global Step 000046 =>> LR :: 0.000020 - Loss :: 0.0082:  24%|‚ñà‚ñà‚ñé       | 47/200 [2:13:01<5:07:55, 120.75s/it]
=>> [Epoch 000] Global Step 000047 =>> LR :: 0.000020 - Loss :: 0.0145:  24%|‚ñà‚ñà‚ñé       | 47/200 [2:13:01<5:07:55, 120.75s/it]
=>> [Epoch 000] Global Step 000047 =>> LR :: 0.000020 - Loss :: 0.0145:  24%|‚ñà‚ñà‚ñç       | 48/200 [2:16:02<5:51:38, 138.80s/it]
=>> [Epoch 000] Global Step 000048 =>> LR :: 0.000020 - Loss :: 0.0233:  24%|‚ñà‚ñà‚ñç       | 48/200 [2:16:02<5:51:38, 138.80s/it]
=>> [Epoch 000] Global Step 000048 =>> LR :: 0.000020 - Loss :: 0.0233:  24%|‚ñà‚ñà‚ñç       | 49/200 [2:16:55<4:44:58, 113.23s/it]
=>> [Epoch 000] Global Step 000049 =>> LR :: 0.000020 - Loss :: 0.0167:  24%|‚ñà‚ñà‚ñç       | 49/200 [2:16:55<4:44:58, 113.23s/it]
=>> [Epoch 000] Global Step 000049 =>> LR :: 0.000020 - Loss :: 0.0167:  25%|‚ñà‚ñà‚ñå       | 50/200 [2:20:16<5:48:39, 139.46s/it]
=>> [Epoch 000] Global Step 000050 =>> LR :: 0.000020 - Loss :: 0.0084:  25%|‚ñà‚ñà‚ñå       | 50/200 [2:20:16<5:48:39, 139.46s/it]
=>> [Epoch 000] Global Step 000050 =>> LR :: 0.000020 - Loss :: 0.0084:  26%|‚ñà‚ñà‚ñå       | 51/200 [2:21:08<4:41:04, 113.18s/it]
=>> [Epoch 000] Global Step 000051 =>> LR :: 0.000020 - Loss :: 0.0137:  26%|‚ñà‚ñà‚ñå       | 51/200 [2:21:08<4:41:04, 113.18s/it]
=>> [Epoch 000] Global Step 000051 =>> LR :: 0.000020 - Loss :: 0.0137:  26%|‚ñà‚ñà‚ñå       | 52/200 [2:27:20<7:50:37, 190.79s/it]
=>> [Epoch 000] Global Step 000052 =>> LR :: 0.000020 - Loss :: 0.0120:  26%|‚ñà‚ñà‚ñå       | 52/200 [2:27:20<7:50:37, 190.79s/it]
=>> [Epoch 000] Global Step 000052 =>> LR :: 0.000020 - Loss :: 0.0120:  26%|‚ñà‚ñà‚ñã       | 53/200 [2:28:13<6:06:41, 149.67s/it]
=>> [Epoch 000] Global Step 000053 =>> LR :: 0.000020 - Loss :: 0.0103:  26%|‚ñà‚ñà‚ñã       | 53/200 [2:28:13<6:06:41, 149.67s/it]
=>> [Epoch 000] Global Step 000053 =>> LR :: 0.000020 - Loss :: 0.0103:  27%|‚ñà‚ñà‚ñã       | 54/200 [2:29:06<4:53:27, 120.60s/it]
=>> [Epoch 000] Global Step 000054 =>> LR :: 0.000020 - Loss :: 0.0075:  27%|‚ñà‚ñà‚ñã       | 54/200 [2:29:06<4:53:27, 120.60s/it]
=>> [Epoch 000] Global Step 000054 =>> LR :: 0.000020 - Loss :: 0.0075:  28%|‚ñà‚ñà‚ñä       | 55/200 [2:32:08<5:35:42, 138.91s/it]
=>> [Epoch 000] Global Step 000055 =>> LR :: 0.000020 - Loss :: 0.0047:  28%|‚ñà‚ñà‚ñä       | 55/200 [2:32:08<5:35:42, 138.91s/it]
=>> [Epoch 000] Global Step 000055 =>> LR :: 0.000020 - Loss :: 0.0047:  28%|‚ñà‚ñà‚ñä       | 56/200 [2:33:02<4:32:42, 113.63s/it]
=>> [Epoch 000] Global Step 000056 =>> LR :: 0.000020 - Loss :: 0.0063:  28%|‚ñà‚ñà‚ñä       | 56/200 [2:33:02<4:32:42, 113.63s/it]
=>> [Epoch 000] Global Step 000056 =>> LR :: 0.000020 - Loss :: 0.0063:  28%|‚ñà‚ñà‚ñä       | 57/200 [2:34:01<3:51:50, 97.27s/it] 
=>> [Epoch 000] Global Step 000057 =>> LR :: 0.000020 - Loss :: 0.0122:  28%|‚ñà‚ñà‚ñä       | 57/200 [2:34:01<3:51:50, 97.27s/it]
=>> [Epoch 000] Global Step 000057 =>> LR :: 0.000020 - Loss :: 0.0122:  29%|‚ñà‚ñà‚ñâ       | 58/200 [2:34:59<3:21:57, 85.34s/it]
=>> [Epoch 000] Global Step 000058 =>> LR :: 0.000020 - Loss :: 0.0062:  29%|‚ñà‚ñà‚ñâ       | 58/200 [2:34:59<3:21:57, 85.34s/it]
=>> [Epoch 000] Global Step 000058 =>> LR :: 0.000020 - Loss :: 0.0062:  30%|‚ñà‚ñà‚ñâ       | 59/200 [2:41:52<7:11:50, 183.77s/it]
=>> [Epoch 000] Global Step 000059 =>> LR :: 0.000020 - Loss :: 0.0044:  30%|‚ñà‚ñà‚ñâ       | 59/200 [2:41:52<7:11:50, 183.77s/it]
=>> [Epoch 000] Global Step 000059 =>> LR :: 0.000020 - Loss :: 0.0044:  30%|‚ñà‚ñà‚ñà       | 60/200 [2:42:49<5:39:50, 145.64s/it]
=>> [Epoch 000] Global Step 000060 =>> LR :: 0.000020 - Loss :: 0.0038:  30%|‚ñà‚ñà‚ñà       | 60/200 [2:42:49<5:39:50, 145.64s/it]
=>> [Epoch 000] Global Step 000060 =>> LR :: 0.000020 - Loss :: 0.0038:  30%|‚ñà‚ñà‚ñà       | 61/200 [2:48:26<7:50:37, 203.15s/it]
=>> [Epoch 000] Global Step 000061 =>> LR :: 0.000020 - Loss :: 0.0042:  30%|‚ñà‚ñà‚ñà       | 61/200 [2:48:26<7:50:37, 203.15s/it]
=>> [Epoch 000] Global Step 000061 =>> LR :: 0.000020 - Loss :: 0.0042:  31%|‚ñà‚ñà‚ñà       | 62/200 [2:49:18<6:02:58, 157.81s/it]
=>> [Epoch 000] Global Step 000062 =>> LR :: 0.000020 - Loss :: 0.0047:  31%|‚ñà‚ñà‚ñà       | 62/200 [2:49:18<6:02:58, 157.81s/it]
=>> [Epoch 000] Global Step 000062 =>> LR :: 0.000020 - Loss :: 0.0047:  32%|‚ñà‚ñà‚ñà‚ñè      | 63/200 [2:50:18<4:52:49, 128.25s/it]
=>> [Epoch 000] Global Step 000063 =>> LR :: 0.000020 - Loss :: 0.0058:  32%|‚ñà‚ñà‚ñà‚ñè      | 63/200 [2:50:18<4:52:49, 128.25s/it]
=>> [Epoch 000] Global Step 000063 =>> LR :: 0.000020 - Loss :: 0.0058:  32%|‚ñà‚ñà‚ñà‚ñè      | 64/200 [2:51:35<4:15:54, 112.90s/it]
=>> [Epoch 000] Global Step 000064 =>> LR :: 0.000020 - Loss :: 0.0030:  32%|‚ñà‚ñà‚ñà‚ñè      | 64/200 [2:51:35<4:15:54, 112.90s/it]
=>> [Epoch 000] Global Step 000064 =>> LR :: 0.000020 - Loss :: 0.0030:  32%|‚ñà‚ñà‚ñà‚ñé      | 65/200 [2:52:33<3:36:56, 96.42s/it] 
=>> [Epoch 000] Global Step 000065 =>> LR :: 0.000020 - Loss :: 0.0032:  32%|‚ñà‚ñà‚ñà‚ñé      | 65/200 [2:52:33<3:36:56, 96.42s/it]
=>> [Epoch 000] Global Step 000065 =>> LR :: 0.000020 - Loss :: 0.0032:  33%|‚ñà‚ñà‚ñà‚ñé      | 66/200 [2:59:15<7:00:16, 188.18s/it]
=>> [Epoch 000] Global Step 000066 =>> LR :: 0.000020 - Loss :: 0.0021:  33%|‚ñà‚ñà‚ñà‚ñé      | 66/200 [2:59:15<7:00:16, 188.18s/it]
=>> [Epoch 000] Global Step 000066 =>> LR :: 0.000020 - Loss :: 0.0021:  34%|‚ñà‚ñà‚ñà‚ñé      | 67/200 [3:00:11<5:29:13, 148.52s/it]
=>> [Epoch 000] Global Step 000067 =>> LR :: 0.000020 - Loss :: 0.0017:  34%|‚ñà‚ñà‚ñà‚ñé      | 67/200 [3:00:11<5:29:13, 148.52s/it]
=>> [Epoch 000] Global Step 000067 =>> LR :: 0.000020 - Loss :: 0.0017:  34%|‚ñà‚ñà‚ñà‚ñç      | 68/200 [3:03:18<5:52:24, 160.18s/it]
=>> [Epoch 000] Global Step 000068 =>> LR :: 0.000020 - Loss :: 0.0012:  34%|‚ñà‚ñà‚ñà‚ñç      | 68/200 [3:03:18<5:52:24, 160.18s/it]
=>> [Epoch 000] Global Step 000068 =>> LR :: 0.000020 - Loss :: 0.0012:  34%|‚ñà‚ñà‚ñà‚ñç      | 69/200 [3:04:15<4:41:44, 129.04s/it]
=>> [Epoch 000] Global Step 000069 =>> LR :: 0.000020 - Loss :: 0.0008:  34%|‚ñà‚ñà‚ñà‚ñç      | 69/200 [3:04:15<4:41:44, 129.04s/it]
=>> [Epoch 000] Global Step 000069 =>> LR :: 0.000020 - Loss :: 0.0008:  35%|‚ñà‚ñà‚ñà‚ñå      | 70/200 [3:05:06<3:49:08, 105.76s/it]
=>> [Epoch 000] Global Step 000070 =>> LR :: 0.000020 - Loss :: 0.0005:  35%|‚ñà‚ñà‚ñà‚ñå      | 70/200 [3:05:06<3:49:08, 105.76s/it]
=>> [Epoch 000] Global Step 000070 =>> LR :: 0.000020 - Loss :: 0.0005:  36%|‚ñà‚ñà‚ñà‚ñå      | 71/200 [3:06:36<3:37:13, 101.04s/it]
=>> [Epoch 000] Global Step 000071 =>> LR :: 0.000020 - Loss :: 0.0002:  36%|‚ñà‚ñà‚ñà‚ñå      | 71/200 [3:06:36<3:37:13, 101.04s/it]
=>> [Epoch 000] Global Step 000071 =>> LR :: 0.000020 - Loss :: 0.0002:  36%|‚ñà‚ñà‚ñà‚ñå      | 72/200 [3:07:31<3:06:11, 87.28s/it] 
=>> [Epoch 000] Global Step 000072 =>> LR :: 0.000020 - Loss :: 0.0002:  36%|‚ñà‚ñà‚ñà‚ñå      | 72/200 [3:07:31<3:06:11, 87.28s/it]
=>> [Epoch 000] Global Step 000072 =>> LR :: 0.000020 - Loss :: 0.0002:  36%|‚ñà‚ñà‚ñà‚ñã      | 73/200 [3:15:34<7:15:55, 205.95s/it]
=>> [Epoch 000] Global Step 000073 =>> LR :: 0.000020 - Loss :: 0.0001:  36%|‚ñà‚ñà‚ñà‚ñã      | 73/200 [3:15:34<7:15:55, 205.95s/it]
=>> [Epoch 000] Global Step 000073 =>> LR :: 0.000020 - Loss :: 0.0001:  37%|‚ñà‚ñà‚ñà‚ñã      | 74/200 [3:16:29<5:37:07, 160.54s/it]
=>> [Epoch 000] Global Step 000074 =>> LR :: 0.000020 - Loss :: 0.0002:  37%|‚ñà‚ñà‚ñà‚ñã      | 74/200 [3:16:29<5:37:07, 160.54s/it]
=>> [Epoch 000] Global Step 000074 =>> LR :: 0.000020 - Loss :: 0.0002:  38%|‚ñà‚ñà‚ñà‚ñä      | 75/200 [3:18:54<5:24:38, 155.83s/it]
=>> [Epoch 000] Global Step 000075 =>> LR :: 0.000020 - Loss :: 0.0001:  38%|‚ñà‚ñà‚ñà‚ñä      | 75/200 [3:18:54<5:24:38, 155.83s/it]
=>> [Epoch 000] Global Step 000075 =>> LR :: 0.000020 - Loss :: 0.0001:  38%|‚ñà‚ñà‚ñà‚ñä      | 76/200 [3:19:51<4:21:02, 126.31s/it]
=>> [Epoch 000] Global Step 000076 =>> LR :: 0.000020 - Loss :: 0.00:47<3:35:33, 105.15s/it]=>> [Epoch 000] Global Step 000077 =>> LR :: 0.000020 - Loss :: 0.0001:  39%|‚ñà‚ñà‚ñà‚ñâ      | 78/200 [3:24:20<4:39:45, 137.59s/it]=>> [Epoch 000] Global Step 000078 =>> LR :: 0.000020 - Loss :: 0.0001:  39%|‚ñà‚ñà‚ñà‚ñâ      | 78/200 [3:24:20<4:39:45, 137.59s/it]=>> [Epoch 000] Global Step 000078 =>> LR :: 0.000020 - Loss :: 0.0001:  40%|‚ñà‚ñà‚ñà‚ñâ      | 79/200 [3:25:16<3:48:10, 113.15s/it]=>> [Epoch 000] Global Step 000079 =>> LR :: 0.000020 - Loss :: 0.0001:  40%|‚ñà‚ñà‚ñà‚ñâ      | 79/200 [3:25:16<3:48:10, 113.15s/it]                                                                                                                                                                                                                                                                                                                           =>> [Epoch 000] Global Step 000079 =>> LR :: 0.000020 - Loss :: 0.0001:  40%|‚ñà‚ñà‚ñà‚ñà      | 80/200 [3:31:16<6:14:10, 187.08s/it]=>> [Epoch 000] Global Step 000080 =>> LR :: 0.000020 - Loss :: 0.0001:  40%|‚ñà‚ñà‚ñà‚ñà      | 80/200 [3:31:16<6:14:10, 187.08s/it]=>> [Epoch 000] Global Step 000080 =>> LR :: 0.000020 - Loss :: 0.0001:  40%|‚ñà‚ñà‚ñà‚ñà      | 81/200 [3:32:07<4:50:13, 146.33s/it]=>> [Epoch 000] Global Step 000081 =>> LR :: 0.000020 - Loss :: 0.0000:  40%|‚ñà‚ñà‚ñà‚ñà      | 81/200 [3:32:07<4:50:13, 146.33s/it]=>> [Epoch 000] Global Step 000081 =>> LR :: 0.000020 - Loss :: 0.0000:  41%|‚ñà‚ñà‚ñà‚ñà      | 82/200 [3:33:46<4:19:43, 132.06s/it]=>> [Epoch 000] Global Step 000082 =>> LR :: 0.000020 - Loss :: 0.0000:  41%|‚ñà‚ñà‚ñà‚ñà      | 82/200 [3:33:46<4:19:43, 132.06s/it]=>> [Epoch 000] Global Step 000082 =>> LR :: 0.000020 - Loss :: 0.0000:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 83/200 [3:34:40<3:31:42, 108.57s/it]=>> [Epoch 000] Global Step 000083 =>> LR :: 0.000020 - Loss :: 0.0000:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 83/200 [3:34:40<3:31:42, 108.57s/it]=>> [Epoch 000] Global Step 000083 =>> LR :: 0.000020 - Loss :: 0.0000:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 84/200 [3:35:34<2:58:16, 92.21s/it] =>> [Epoch 000] Global Step 000084 =>> LR :: 0.000020 - Loss :: 0.0000:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 84/200 [3:35:34<2:58:16, 92.21s/it]=>> [Epoch 000] Global Step 000084 =>> LR :: 0.000020 - Loss :: 0.0000:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 85/200 [3:40:16<4:45:50, 149.13s/it]=>> [Epoch 000] Global Step 000085 =>> LR :: 0.000020 - Loss :: 0.0000:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 85/200 [3:40:16<4:45:50, 149.13s/it]=>> [Epoch 000] Global Step 000085 =>> LR :: 0.000020 - Loss :: 0.0000:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 86/200 [3:41:08<3:48:00, 120.01s/it]=>> [Epoch 000] Global Step 000086 =>> LR :: 0.000020 - Loss :: 0.0000:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 86/200 [3:41:08<3:48:00, 120.01s/it]=>> [Epoch 000] Global Step 000086 =>> LR :: 0.000020 - Loss :: 0.0000:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 87/200 [3:45:13<4:56:34, 157.47s/it]=>> [Epoch 000] Global Step 000087 =>> LR :: 0.000020 - Loss :: 0.0000:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 87/200 [3:45:13<4:56:34, 157.47s/it]=>> [Epoch 000] Global Step 000087 =>> LR :: 0.000020 - Loss :: 0.0000:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 88/200 [3:46:10<3:57:41, 127.34s/it]=>> [Epoch 000] Global Step 000088 =>> LR :: 0.000020 - Loss :: 0.0000:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 88/200 [3:46:10<3:57:41, 127.34s/it]=>> [Epoch 000] Global Step 000088 =>> LR :: 0.000020 - Loss :: 0.0000:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 89/200 [3:48:20<3:57:26, 128.35s/it]=>> [Epoch 000] Global Step 000089 =>> LR :: 0.000020 - Loss :: 0.0000:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 89/200 [3:48:20<3:57:26, 128.35s/it]=>> [Epoch 000] Global Step 000089 =>> LR :: 0.000020 - Loss :: 0.0000:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 90/200 [3:49:13<3:13:42, 105.66s/it]=>> [Epoch 000] Global Step 000090 =>> LR :: 0.000020 - Loss :: 0.0000:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 90/200 [3:49:13<3:13:42, 105.66s/it]=>> [Epoch 000] Global Step 000090 =>> LR :: 0.000020 - Loss :: 0.0000:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 91/200 [3:50:09<2:44:55, 90.78s/it] =>> [Epoch 000] Global Step 000091 =>> LR :: 0.000020 - Loss :: 0.0000:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 91/200 [3:50:09<2:44:55, 90.78s/it]=>> [Epoch 000] Global Step 000091 =>> LR :: 0.000020 - Loss :: 0.0000:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 92/200 [3:56:11<5:09:52, 172.15s/it]=>> [Epoch 000] Global Step 000092 =>> LR :: 0.000020 - Loss :: 0.0000:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 92/200 [3:56:11<5:09:52, 172.15s/it]=>> [Epoch 000] Global Step 000092 =>> LR :: 0.000020 - Loss :: 0.0000:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 93/200 [3:57:04<4:03:14, 136.40s/it]=>> [Epoch 000] Global Step 000093 =>> LR :: 0.000020 - Loss :: 0.0000:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 93/200 [3:57:04<4:03:14, 136.40s/it]=>> [Epoch 000] Global Step 000093 =>> LR :: 0.000020 - Loss :: 0.0000:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 94/200 [4:00:14<4:29:03, 152.30s/it]=>> [Epoch 000] Global Step 000094 =>> LR :: 0.000020 - Loss :: 0.0000:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 94/200 [4:00:14<4:29:03, 152.30s/it]=>> [Epoch 000] Global Step 000094 =>> LR :: 0.000020 - Loss :: 0.0000:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 95/200 [4:01:07<3:34:47, 122.74s/it]=>> [Epoch 000] Global Step 000095 =>> LR :: 0.000020 - Loss :: 0.0000:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 95/200 [4:01:07<3:34:47, 122.74s/it]=>> [Epoch 000] Global Step 000095 =>> LR :: 0.000020 - Loss :: 0.0000:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 96/200 [4:06:39<5:21:13, 185.33s/it]=>> [Epoch 000] Global Step 000096 =>> LR :: 0.000020 - Loss :: 0.0000:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 96/200 [4:06:39<5:21:13, 185.33s/it]=>> [Epoch 000] Global Step 000096 =>> LR :: 0.000020 - Loss :: 0.0000:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 97/200 [4:07:29<4:08:52, 144.98s/it]=>> [Epoch 000] Global Step 000097 =>> LR :: 0.000020 - Loss :: 0.0000:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 97/200 [4:07:29<4:08:52, 144.98s/it]=>> [Epoch 000] Global Step 000097 =>> LR :: 0.000020 - Loss :: 0.0000:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 98/200 [4:10:41<4:29:57, 158.80s/it]=>> [Epoch 000] Global Step 000098 =>> LR :: 0.000020 - Loss :: 0.0000:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 98/200 [4:10:41<4:29:57, 158.80s/it]=>> [Epoch 000] Global Step 000098 =>> LR :: 0.000020 - Loss :: 0.0000:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 99/200 [4:11:37<3:35:33, 128.05s/it]=>> [Epoch 000] Global Step 000099 =>> LR :: 0.000020 - Loss :: 0.0000:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 99/200 [4:11:37<3:35:33, 128.05s/it]=>> [Epoch 000] Global Step 000099 =>> LR :: 0.000020 - Loss :: 0.0000:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 100/200 [4:12:37<2:59:34, 107.75s/it]=>> [Epoch 000] Global Step 000100 =>> LR :: 0.000020 - Loss :: 0.0000:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 100/200 [4:12:37<2:59:34, 107.75s/it]=>> [Epoch 000] Global Step 000100 =>> LR :: 0.000020 - Loss :: 0.0000:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 101/200 [4:17:33<4:30:55, 164.19s/it]=>> [Epoch 000] Global Step 000101 =>> LR :: 0.000020 - Loss :: 0.0000:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 101/200 [4:17:33<4:30:55, 164.19s/it]=>> [Epoch 000] Global Step 000101 =>> LR :: 0.000020 - Loss :: 0.0000:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 102/200 [4:18:23<3:32:07, 129.87s/it]=>> [Epoch 000] Global Step 000102 =>> LR :: 0.000020 - Loss :: 0.0000:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 102/200 [4:18:23<3:32:07, 129.87s/it]=>> [Epoch 000] Global Step 000102 =>> LR :: 0.000020 - Loss :: 0.0000:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 103/200 [4:22:08<4:16:04, 158.40s/it]=>> [Epoch 000] Global Step 000103 =>> LR :: 0.000020 - Loss :: 0.0000:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 103/200 [4:22:08<4:16:04, 158.40s/it]=>> [Epoch 000] Global Step 000103 =>> LR :: 0.000020 - Loss :: 0.0000:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 104/200 [4:23:06<3:25:24, 128.38s/it]=>> [Epoch 000] Global Step 000104 =>> LR :: 0.000020 - Loss :: 0.0000:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 104/200 [4:23:06<3:25:24, 128.38s/it]=>> [Epoch 000] Global Step 000104 =>> LR :: 0.000020 - Loss :: 0.0000:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 105/200 [4:26:20<3:54:19, 148.00s/it]=>> [Epoch 000] Global Step 000105 =>> LR :: 0.000020 - Loss :: 0.0000:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 105/200 [4:26:20<3:54:19, 148.00s/it]=>> [Epoch 000] Global Step 000105 =>> LR :: 0.000020 - Loss :: 0.0000:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 106/200 [4:27:25<3:12:50, 123.09s/it]=>> [Epoch 000] Global Step 000106 =>> LR :: 0.000020 - Loss :: 0.0000:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 106/200 [4:27:25<3:12:50, 123.09s/it]=>> [Epoch 000] Global Step 000106 =>> LR :: 0.000020 - Loss :: 0.0000:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 107/200 [4:28:27<2:42:30, 104.84s/it]=>> [Epoch 000] Global Step 000107 =>> LR :: 0.000020 - Loss :: 0.0000:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 107/200 [4:28:27<2:42:30, 104.84s/it]=>> [Epoch 000] Global Step 000107 =>> LR :: 0.000020 - Loss :: 0.0000:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 108/200 [4:31:45<3:23:30, 132.73s/it]=>> [Epoch 000] Global Step 000108 =>> LR :: 0.000020 - Loss :: 0.0000:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 108/200 [4:31:45<3:23:30, 132.73s/it]=>> [Epoch 000] Global Step 000108 =>> LR :: 0.000020 - Loss :: 0.0000:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 109/200 [4:32:45<2:48:00, 110.78s/it]=>> [Epoch 000] Global Step 000109 =>> LR :: 0.000020 - Loss :: 0.0000:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 109/200 [4:32:45<2:48:00, 110.78s/it]=>> [Epoch 000] Global Step 000109 =>> LR :: 0.000020 - Loss :: 0.0000:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 110/200 [4:36:56<3:49:36, 153.08s/it]=>> [Epoch 000] Global Step 000110 =>> LR :: 0.000020 - Loss :: 0.0000:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 110/200 [4:36:56<3:49:36, 153.08s/it]=>> [Epoch 000] Global Step 000110 =>> LR :: 0.000020 - Loss :: 0.0000:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 111/200 [4:37:48<3:01:56, 122.65s/it]=>> [Epoch 000] Global Step 000111 =>> LR :: 0.000020 - Loss :: 0.0000:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 111/200 [4:37:48<3:01:56, 122.65s/it]=>> [Epoch 000] Global Step 000111 =>> LR :: 0.000020 - Loss :: 0.0000:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 112/200 [4:41:39<3:47:30, 155.12s/it]=>> [Epoch 000] Global Step 000112 =>> LR :: 0.000020 - Loss :: 0.0000:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 112/200 [4:41:39<3:47:30, 155.12s/it]=>> [Epoch 000] Global Step 000112 =>> LR :: 0.000020 - Loss :: 0.0000:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 113/200 [4:42:32<3:00:26, 124.44s/it]=>> [Epoch 000] Global Step 000113 =>> LR :: 0.000020 - Loss :: 0.0000:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 113/200 [4:42:32<3:00:26, 124.44s/it]=>> [Epoch 000] Global Step 000113 =>> LR :: 0.000020 - Loss :: 0.0000:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 114/200 [4:43:24<2:27:08, 102.65s/it]=>> [Epoch 000] Global Step 000114 =>> LR :: 0.000020 - Loss :: 0.0000:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 114/200 [4:43:24<2:27:08, 102.65s/it]=>> [Epoch 000] Global Step 000114 =>> LR :: 0.000020 - Loss :: 0.0000:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 115/200 [4:46:31<3:01:36, 128.20s/it]=>> [Epoch 000] Global Step 000115 =>> LR :: 0.000020 - Loss :: 0.0000:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 115/200 [4:46:31<3:01:36, 128.20s/it]=>> [Epoch 000] Global Step 000115 =>> LR :: 0.000020 - Loss :: 0.0000:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 116/200 [4:47:25<2:28:15, 105.90s/it]=>> [Epoch 000] Global Step 000116 =>> LR :: 0.000020 - Loss :: 0.0000:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 116/200 [4:47:25<2:28:15, 105.90s/it]=>> [Epoch 000] Global Step 000116 =>> LR :: 0.000020 - Loss :: 0.0000:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 117/200 [4:52:23<3:46:06, 163.45s/it]=>> [Epoch 000] Global Step 000117 =>> LR :: 0.000020 - Loss :: 0.0000:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 117/200 [4:52:23<3:46:06, 163.45s/it]=>> [Epoch 000] Global Step 000117 =>> LR :: 0.000020 - Loss :: 0.0000:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 118/200 [4:53:21<3:00:17, 131.92s/it]=>> [Epoch 000] Global Step 000118 =>> LR :: 0.000020 - Loss :: 0.0000:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 118/200 [4:53:21<3:00:17, 131.92s/it]=>> [Epoch 000] Global Step 000118 =>> LR :: 0.000020 - Loss :: 0.0000:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 119/200 [4:55:55<3:06:47, 138.36s/it]=>> [Epoch 000] Global Step 000119 =>> LR :: 0.000020 - Loss :: 0.0000:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 119/200 [4:55:55<3:06:47, 138.36s/it]=>> [Epoch 000] Global Step 000119 =>> LR :: 0.000020 - Loss :: 0.0000:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 120/200 [4:56:53<2:32:24, 114.30s/it]=>> [Epoch 000] Global Step 000120 =>> LR :: 0.000020 - Loss :: 0.0000:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 120/200 [4:56:53<2:32:24, 114.30s/it]=>> [Epoch 000] Global Step 000120 =>> LR :: 0.000020 - Loss :: 0.0000:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 121/200 [4:57:46<2:06:11, 95.84s/it] =>> [Epoch 000] Global Step 000121 =>> LR :: 0.000020 - Loss :: 0.0000:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 121/200 [4:57:46<2:06:11, 95.84s/it]=>> [Epoch 000] Global Step 000121 =>> LR :: 0.000020 - Loss :: 0.0000:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 122/200 [5:02:11<3:10:31, 146.56s/it]=>> [Epoch 000] Global Step 000122 =>> LR :: 0.000020 - Loss :: 0.0000:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 122/200 [5:02:11<3:10:31, 146.56s/it]=>> [Epoch 000] Global Step 000122 =>> LR :: 0.000020 - Loss :: 0.0000:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 123/200 [5:03:04<2:32:10, 118.58s/it]=>> [Epoch 000] Global Step 000123 =>> LR :: 0.000020 - Loss :: 0.0000:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 123/200 [5:03:04<2:32:10, 118.58s/it]=>> [Epoch 000] Global Step 000123 =>> LR :: 0.000020 - Loss :: 0.0000:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 124/200 [5:07:56<3:36:19, 170.78s/it]=>> [Epoch 000] Global Step 000124 =>> LR :: 0.000020 - Loss :: 0.0000:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 124/200 [5:07:56<3:36:19, 170.78s/it]=>> [Epoch 000] Global Step 000124 =>> LR :: 0.000020 - Loss :: 0.0000:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 125/200 [5:08:54<2:51:06, 136.88s/it]=>> [Epoch 000] Global Step 000125 =>> LR :: 0.000020 - Loss :: 0.0000:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 125/200 [5:08:54<2:51:06, 136.88s/it]=>> [Epoch 000] Global Step 000125 =>> LR :: 0.000020 - Loss :: 0.0000:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 126/200 [5:13:54<3:49:11, 185.83s/it]=>> [Epoch 000] Global Step 000126 =>> LR :: 0.000020 - Loss :: 0.0000:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 126/200 [5:13:54<3:49:11, 185.83s/it]=>> [Epoch 000] Global Step 000126 =>> LR :: 0.000020 - Loss :: 0.0000:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 127/200 [5:14:51<2:58:48, 146.96s/it]=>> [Epoch 000] Global Step 000127 =>> LR :: 0.000020 - Loss :: 0.0000:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 127/200 [5:14:51<2:58:48, 146.96s/it]=>> [Epoch 000] Global Step 000127 =>> LR :: 0.000020 - Loss :: 0.0000:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 128/200 [5:15:39<2:21:01, 117.53s/it]=>> [Epoch 000] Global Step 000128 =>> LR :: 0.000020 - Loss :: 0.0000:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 128/200 [5:15:39<2:21:01, 117.53s/it]=>> [Epoch 000] Global Step 000128 =>> LR :: 0.000020 - Loss :: 0.0000:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 129/200 [5:19:13<2:53:05, 146.27s/it]=>> [Epoch 000] Global Step 000129 =>> LR :: 0.000020 - Loss :: 0.0000:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 129/200 [5:19:13<2:53:05, 146.27s/it]=>> [Epoch 000] Global Step 000129 =>> LR :: 0.000020 - Loss :: 0.0000:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 130/200 [5:20:11<2:20:01, 120.02s/it]=>> [Epoch 000] Global Step 000130 =>> LR :: 0.000020 - Loss :: 0.0000:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 130/200 [5:20:12<2:20:01, 120.02s/it]=>> [Epoch 000] Global Step 000130 =>> LR :: 0.000020 - Loss :: 0.0000:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 131/200 [5:22:06<2:16:17, 118.52s/it]=>> [Epoch 000] Global Step 000131 =>> LR :: 0.000020 - Loss :: 0.0000:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 131/200 [5:22:06<2:16:17, 118.52s/it]=>> [Epoch 000] Global Step 000131 =>> LR :: 0.000020 - Loss :: 0.0000:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 132/200 [5:23:02<1:53:02, 99.74s/it] =>> [Epoch 000] Global Step 000132 =>> LR :: 0.000020 - Loss :: 0.0000:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 132/200 [5:23:02<1:53:02, 99.74s/it]=>> [Epoch 000] Global Step 000132 =>> LR :: 0.000020 - Loss :: 0.0000:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 133/200 [5:28:55<3:16:07, 175.64s/it]=>> [Epoch 000] Global Step 000133 =>> LR :: 0.000020 - Loss :: 0.0000:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 133/200 [5:28:55<3:16:07, 175.64s/it]=>> [Epoch 000] Global Step 000133 =>> LR :: 0.000020 - Loss :: 0.0000:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 134/200 [5:29:50<2:33:18, 139.37s/it]=>> [Epoch 000] Global Step 000134 =>> LR :: 0.000020 - Loss :: 0.0000:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 134/200 [5:29:50<2:33:18, 139.37s/it]=>> [Epoch 000] Global Step 000134 =>> LR :: 0.000020 - Loss :: 0.0000:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 135/200 [5:33:25<2:55:34, 162.07s/it]=>> [Epoch 000] Global Step 000135 =>> LR :: 0.000020 - Loss :: 0.0000:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 135/200 [5:33:25<2:55:34, 162.07s/it]=>> [Epoch 000] Global Step 000135 =>> LR :: 0.000020 - Loss :: 0.0000:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 136/200 [5:34:16<2:17:18, 128.72s/it]=>> [Epoch 000] Global Step 000136 =>> LR :: 0.000020 - Loss :: 0.0000:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 136/200 [5:34:16<2:17:18, 128.72s/it]=>> [Epoch 000] Global Step 000136 =>> LR :: 0.000020 - Loss :: 0.0000:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 137/200 [5:35:10<1:51:34, 106.26s/it]=>> [Epoch 000] Global Step 000137 =>> LR :: 0.000020 - Loss :: 0.0000:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 137/200 [5:35:10<1:51:34, 106.26s/it]=>> [Epoch 000] Global Step 000137 =>> LR :: 0.000020 - Loss :: 0.0000:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 138/200 [5:37:00<1:50:57, 107.38s/it]=>> [Epoch 000] Global Step 000138 =>> LR :: 0.000020 - Loss :: 0.0000:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 138/200 [5:37:00<1:50:57, 107.38s/it]=>> [Epoch 000] Global Step 000138 =>> LR :: 0.000020 - Loss :: 0.0000:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 139/200 [5:37:51<1:32:06, 90.60s/it] =>> [Epoch 000] Global Step 000139 =>> LR :: 0.000020 - Loss :: 0.0000:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 139/200 [5:37:51<1:32:06, 90.60s/it]=>> [Epoch 000] Global Step 000139 =>> LR :: 0.000020 - Loss :: 0.0000:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 140/200 [5:41:41<2:12:18, 132.30s/it]=>> [Epoch 000] Global Step 000140 =>> LR :: 0.000020 - Loss :: 0.0000:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 140/200 [5:41:41<2:12:18, 132.30s/it]=>> [Epoch 000] Global Step 000140 =>> LR :: 0.000020 - Loss :: 0.0000:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 141/200 [5:42:33<1:46:25, 108.23s/it]=>> [Epoch 000] Global Step 000141 =>> LR :: 0.000020 - Loss :: 0.0000:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 141/200 [5:42:33<1:46:25, 108.23s/it]=>> [Epoch 000] Global Step 000141 =>> LR :: 0.000020 - Loss :: 0.0000:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 142/200 [5:46:29<2:21:35, 146.47s/it]=>> [Epoch 000] Global Step 000142 =>> LR :: 0.000020 - Loss :: 0.0000:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 142/200 [5:46:29<2:21:35, 146.47s/it]=>> [Epoch 000] Global Step 000142 =>> LR :: 0.000020 - Loss :: 0.0000:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 143/200 [5:47:20<1:52:13, 118.13s/it]=>> [Epoch 000] Global Step 000143 =>> LR :: 0.000020 - Loss :: 0.0000:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 143/200 [5:47:20<1:52:13, 118.13s/it]=>> [Epoch 000] Global Step 000143 =>> LR :: 0.000020 - Loss :: 0.0000:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 144/200 [5:48:13<1:31:56, 98.51s/it] =>> [Epoch 000] Global Step 000144 =>> LR :: 0.000020 - Loss :: 0.0000:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 144/200 [5:48:13<1:31:56, 98.51s/it]=>> [Epoch 000] Global Step 000144 =>> LR :: 0.000020 - Loss :: 0.0000:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 145/200 [5:54:49<2:52:02, 187.68s/it]=>> [Epoch 000] Global Step 000145 =>> LR :: 0.000020 - Loss :: 0.0000:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 145/200 [5:54:49<2:52:02, 187.68s/it]=>> [Epoch 000] Global Step 000145 =>> LR :: 0.000020 - Loss :: 0.0000:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 146/200 [5:55:43<2:12:50, 147.60s/it]=>> [Epoch 000] Global Step 000146 =>> LR :: 0.000020 - Loss :: 0.0000:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 146/200 [5:55:43<2:12:50, 147.60s/it]=>> [Epoch 000] Global Step 000146 =>> LR :: 0.000020 - Loss :: 0.0000:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 147/200 [5:58:57<2:22:37, 161.46s/it]=>> [Epoch 000] Global Step 000147 =>> LR :: 0.000020 - Loss :: 0.0000:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 147/200 [5:58:57<2:22:37, 161.46s/it]=>> [Epoch 000] Global Step 000147 =>> LR :: 0.000020 - Loss :: 0.0000:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 148/200 [5:59:53<1:52:28, 129.78s/it]=>> [Epoch 000] Global Step 000148 =>> LR :: 0.000020 - Loss :: 0.0000:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 148/200 [5:59:53<1:52:28, 129.78s/it]=>> [Epoch 000] Global Step 000148 =>> LR :: 0.000020 - Loss :: 0.0000:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 149/200 [6:03:14<2:08:25, 151.10s/it]=>> [Epoch 000] Global Step 000149 =>> LR :: 0.000020 - Loss :: 0.0000:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 149/200 [6:03:14<2:08:25, 151.10s/it]=>> [Epoch 000] Global Step 000149 =>> LR :: 0.000020 - Loss :: 0.0000:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 150/200 [6:04:12<1:42:40, 123.20s/it]=>> [Epoch 000] Global Step 000150 =>> LR :: 0.000020 - Loss :: 0.0000:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 150/200 [6:04:12<1:42:40, 123.20s/it]=>> [Epoch 000] Global Step 000150 =>> LR :: 0.000020 - Loss :: 0.0000:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 151/200 [6:05:12<1:25:17, 104.44s/it]=>> [Epoch 000] Global Step 000151 =>> LR :: 0.000020 - Loss :: 0.0000:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 151/200 [6:05:12<1:25:17, 104.44s/it]=>> [Epoch 000] Global Step 000151 =>> LR :: 0.000020 - Loss :: 0.0000:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 152/200 [6:09:33<2:00:56, 151.18s/it]=>> [Epoch 000] Global Step 000152 =>> LR :: 0.000020 - Loss :: 0.0000:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 152/200 [6:09:33<2:00:56, 151.18s/it]=>> [Epoch 000] Global Step 000152 =>> LR :: 0.000020 - Loss :: 0.0000:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 153/200 [6:10:30<1:36:29, 123.19s/it]=>> [Epoch 000] Global Step 000153 =>> LR :: 0.000020 - Loss :: 0.0000:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 153/200 [6:10:30<1:36:29, 123.19s/it]=>> [Epoch 000] Global Step 000153 =>> LR :: 0.000020 - Loss :: 0.0000:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 154/200 [6:14:17<1:58:07, 154.07s/it]=>> [Epoch 000] Global Step 000154 =>> LR :: 0.000020 - Loss :: 0.0000:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 154/200 [6:14:17<1:58:07, 154.07s/it]=>> [Epoch 000] Global Step 000154 =>> LR :: 0.000020 - Loss :: 0.0000:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 155/200 [6:15:14<1:33:52, 125.16s/it]=>> [Epoch 000] Global Step 000155 =>> LR :: 0.000020 - Loss :: 0.0000:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 155/200 [6:15:14<1:33:52, 125.16s/it]=>> [Epoch 000] Global Step 000155 =>> LR :: 0.000020 - Loss :: 0.0000:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 156/200 [6:19:00<1:53:59, 155.43s/it]=>> [Epoch 000] Global Step 000156 =>> LR :: 0.000020 - Loss :: 0.0000:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 156/200 [6:19:00<1:53:59, 155.43s/it]=>> [Epoch 000] Global Step 000156 =>> LR :: 0.000020 - Loss :: 0.0000:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 157/200 [6:19:56<1:29:54, 125.46s/it]=>> [Epoch 000] Global Step 000157 =>> LR :: 0.000020 - Loss :: 0.0000:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 157/200 [6:19:56<1:29:54, 125.46s/it]=>> [Epoch 000] Global Step 000157 =>> LR :: 0.000020 - Loss :: 0.0000:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 158/200 [6:20:53<1:13:22, 104.83s/it]=>> [Epoch 000] Global Step 000158 =>> LR :: 0.000020 - Loss :: 0.0000:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 158/200 [6:20:53<1:13:22, 104.83s/it]=>> [Epoch 000] Global Step 000158 =>> LR :: 0.000020 - Loss :: 0.0000:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 159/200 [6:25:39<1:48:47, 159.21s/it]=>> [Epoch 000] Global Step 000159 =>> LR :: 0.000020 - Loss :: 0.0000:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 159/200 [6:25:39<1:48:47, 159.21s/it]=>> [Epoch 000] Global Step 000159 =>> LR :: 0.000020 - Loss :: 0.0000:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 160/200 [6:26:32<1:24:53, 127.34s/it]=>> [Epoch 000] Global Step 000160 =>> LR :: 0.000020 - Loss :: 0.0000:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 160/200 [6:26:32<1:24:53, 127.34s/it]=>> [Epoch 000] Global Step 000160 =>> LR :: 0.000020 - Loss :: 0.0000:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 161/200 [6:30:35<1:45:26, 162.22s/it]=>> [Epoch 000] Global Step 000161 =>> LR :: 0.000020 - Loss :: 0.0000:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 161/200 [6:30:35<1:45:26, 162.22s/it]=>> [Epoch 000] Global Step 000161 =>> LR :: 0.000020 - Loss :: 0.0000:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 162/200 [6:31:27<1:21:48, 129.18s/it]=>> [Epoch 000] Global Step 000162 =>> LR :: 0.000020 - Loss :: 0.0000:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 162/200 [6:31:27<1:21:48, 129.18s/it]=>> [Epoch 000] Global Step 000162 =>> LR :: 0.000020 - Loss :: 0.0000:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 163/200 [6:34:59<1:34:50, 153.78s/it]=>> [Epoch 000] Global Step 000163 =>> LR :: 0.000020 - Loss :: 0.0000:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 163/200 [6:34:59<1:34:50, 153.78s/it]=>> [Epoch 000] Global Step 000163 =>> LR :: 0.000020 - Loss :: 0.0000:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 164/200 [6:35:54<1:14:32, 124.23s/it]=>> [Epoch 000] Global Step 000164 =>> LR :: 0.000020 - Loss :: 0.0000:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 164/200 [6:35:54<1:14:32, 124.23s/it]=>> [Epoch 000] Global Step 000164 =>> LR :: 0.000020 - Loss :: 0.0000:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 165/200 [6:36:49<1:00:24, 103.55s/it]=>> [Epoch 000] Global Step 000165 =>> LR :: 0.000020 - Loss :: 0.0000:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 165/200 [6:36:49<1:00:24, 103.55s/it]=>> [Epoch 000] Global Step 000165 =>> LR :: 0.000020 - Loss :: 0.0000:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 166/200 [6:42:04<1:34:34, 166.89s/it]=>> [Epoch 000] Global Step 000166 =>> LR :: 0.000020 - Loss :: 0.0000:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 166/200 [6:42:04<1:34:34, 166.89s/it]=>> [Epoch 000] Global Step 000166 =>> LR :: 0.000020 - Loss :: 0.0000:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 167/200 [6:43:03<1:13:59, 134.54s/it]=>> [Epoch 000] Global Step 000167 =>> LR :: 0.000020 - Loss :: 0.0000:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 167/200 [6:43:03<1:13:59, 134.54s/it]=>> [Epoch 000] Global Step 000167 =>> LR :: 0.000020 - Loss :: 0.0000:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 168/200 [6:45:36<1:14:45, 140.18s/it]=>> [Epoch 000] Global Step 000168 =>> LR :: 0.000020 - Loss :: 0.0000:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 168/200 [6:45:36<1:14:45, 140.18s/it]=>> [Epoch 000] Global Step 000168 =>> LR :: 0.000020 - Loss :: 0.0000:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 169/200 [6:46:37<1:00:09, 116.43s/it]=>> [Epoch 000] Global Step 000169 =>> LR :: 0.000020 - Loss :: 0.0000:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 169/200 [6:46:37<1:00:09, 116.43s/it]=>> [Epoch 000] Global Step 000169 =>> LR :: 0.000020 - Loss :: 0.0000:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 170/200 [6:52:16<1:31:31, 183.05s/it]=>> [Epoch 000] Global Step 000170 =>> LR :: 0.000020 - Loss :: 0.0000:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 170/200 [6:52:16<1:31:31, 183.05s/it]=>> [Epoch 000] Global Step 000170 =>> LR :: 0.000020 - Loss :: 0.0000:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 171/200 [6:53:08<1:09:34, 143.95s/it]=>> [Epoch 000] Global Step 000171 =>> LR :: 0.000020 - Loss :: 0.0000:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 171/200 [6:53:08<1:09:34, 143.95s/it]=>> [Epoch 000] Global Step 000171 =>> LR :: 0.000020 - Loss :: 0.0000:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 172/200 [6:55:44<1:08:50, 147.52s/it]=>> [Epoch 000] Global Step 000172 =>> LR :: 0.000020 - Loss :: 0.0000:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 172/200 [6:55:44<1:08:50, 147.52s/it]=>> [Epoch 000] Global Step 000172 =>> LR :: 0.000020 - Loss :: 0.0000:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 173/200 [6:56:40<53:58, 119.94s/it]  =>> [Epoch 000] Global Step 000173 =>> LR :: 0.000020 - Loss :: 0.0000:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 173/200 [6:56:40<53:58, 119.94s/it]=>> [Epoch 000] Global Step 000173 =>> LR :: 0.000020 - Loss :: 0.0000:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 174/200 [6:57:39<44:02, 101.64s/it]=>> [Epoch 000] Global Step 000174 =>> LR :: 0.000020 - Loss :: 0.0000:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 174/200 [6:57:39<44:02, 101.64s/it]=>> [Epoch 000] Global Step 000174 =>> LR :: 0.000020 - Loss :: 0.0000:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 175/200 [7:01:05<55:21, 132.87s/it]=>> [Epoch 000] Global Step 000175 =>> LR :: 0.000020 - Loss :: 0.0000:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 175/200 [7:01:05<55:21, 132.87s/it]=>> [Epoch 000] Global Step 000175 =>> LR :: 0.000020 - Loss :: 0.0000:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 176/200 [7:01:59<43:41, 109.24s/it]=>> [Epoch 000] Global Step 000176 =>> LR :: 0.000020 - Loss :: 0.0000:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 176/200 [7:01:59<43:41, 109.24s/it]=>> [Epoch 000] Global Step 000176 =>> LR :: 0.000020 - Loss :: 0.0000:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 177/200 [7:04:40<47:48, 124.74s/it]=>> [Epoch 000] Global Step 000177 =>> LR :: 0.000020 - Loss :: 0.0000:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 177/200 [7:04:40<47:48, 124.74s/it]=>> [Epoch 000] Global Step 000177 =>> LR :: 0.000020 - Loss :: 0.0000:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 178/200 [7:05:37<38:17, 104.45s/it]=>> [Epoch 000] Global Step 000178 =>> LR :: 0.000020 - Loss :: 0.0000:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 178/200 [7:05:37<38:17, 104.45s/it]=>> [Epoch 000] Global Step 000178 =>> LR :: 0.000020 - Loss :: 0.0000:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 179/200 [7:11:39<1:03:39, 181.89s/it]=>> [Epoch 000] Global Step 000179 =>> LR :: 0.000020 - Loss :: 0.0000:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 179/200 [7:11:39<1:03:39, 181.89s/it]=>> [Epoch 000] Global Step 000179 =>> LR :: 0.000020 - Loss :: 0.0000:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 180/200 [7:12:32<47:43, 143.16s/it]  =>> [Epoch 000] Global Step 000180 =>> LR :: 0.000020 - Loss :: 0.0000:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 180/200 [7:12:32<47:43, 143.16s/it]=>> [Epoch 000] Global Step 000180 =>> LR :: 0.000020 - Loss :: 0.0000:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 181/200 [7:13:25<36:47, 116.19s/it]=>> [Epoch 000] Global Step 000181 =>> LR :: 0.000020 - Loss :: 0.0000:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 181/200 [7:13:25<36:47, 116.19s/it]=>> [Epoch 000] Global Step 000181 =>> LR :: 0.000020 - Loss :: 0.0000:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 182/200 [7:15:43<36:46, 122.58s/it]=>> [Epoch 000] Global Step 000182 =>> LR :: 0.000020 - Loss :: 0.0000:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 182/200 [7:15:43<36:46, 122.58s/it]=>> [Epoch 000] Global Step 000182 =>> LR :: 0.000020 - Loss :: 0.0000:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 183/200 [7:16:35<28:45, 101.50s/it]=>> [Epoch 000] Global Step 000183 =>> LR :: 0.000020 - Loss :: 0.0000:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 183/200 [7:16:35<28:45, 101.50s/it]=>> [Epoch 000] Global Step 000183 =>> LR :: 0.000020 - Loss :: 0.0000:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 184/200 [7:20:39<38:27, 144.20s/it]=>> [Epoch 000] Global Step 000184 =>> LR :: 0.000020 - Loss :: 0.0000:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 184/200 [7:20:39<38:27, 144.20s/it]=>> [Epoch 000] Global Step 000184 =>> LR :: 0.000020 - Loss :: 0.0000:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 185/200 [7:21:31<29:07, 116.51s/it]=>> [Epoch 000] Global Step 000185 =>> LR :: 0.000020 - Loss :: 0.0000:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 185/200 [7:21:31<29:07, 116.51s/it]=>> [Epoch 000] Global Step 000185 =>> LR :: 0.000020 - Loss :: 0.0000:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 186/200 [7:26:19<39:12, 168.03s/it]=>> [Epoch 000] Global Step 000186 =>> LR :: 0.000020 - Loss :: 0.0000:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 186/200 [7:26:19<39:12, 168.03s/it]=>> [Epoch 000] Global Step 000186 =>> LR :: 0.000020 - Loss :: 0.0000:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 187/200 [7:27:14<29:01, 133.98s/it]=>> [Epoch 000] Global Step 000187 =>> LR :: 0.000020 - Loss :: 0.0000:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 187/200 [7:27:14<29:01, 133.98s/it]=>> [Epoch 000] Global Step 000187 =>> LR :: 0.000020 - Loss :: 0.0000:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 188/200 [7:28:10<22:07, 110.64s/it]=>> [Epoch 000] Global Step 000188 =>> LR :: 0.000020 - Loss :: 0.0000:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 188/200 [7:28:10<22:07, 110.64s/it]=>> [Epoch 000] Global Step 000188 =>> LR :: 0.000020 - Loss :: 0.0000:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 189/200 [7:31:13<24:16, 132.45s/it]=>> [Epoch 000] Global Step 000189 =>> LR :: 0.000020 - Loss :: 0.0000:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 189/200 [7:31:13<24:16, 132.45s/it]=>> [Epoch 000] Global Step 000189 =>> LR :: 0.000020 - Loss :: 0.0000:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 190/200 [7:32:07<18:08, 108.86s/it]=>> [Epoch 000] Global Step 000190 =>> LR :: 0.000020 - Loss :: 0.0000:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 190/200 [7:32:07<18:08, 108.86s/it]=>> [Epoch 000] Global Step 000190 =>> LR :: 0.000020 - Loss :: 0.0000:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 191/200 [7:35:18<20:02, 133.58s/it]=>> [Epoch 000] Global Step 000191 =>> LR :: 0.000020 - Loss :: 0.0000:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 191/200 [7:35:18<20:02, 133.58s/it]=>> [Epoch 000] Global Step 000191 =>> LR :: 0.000020 - Loss :: 0.0000:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 192/200 [7:36:15<14:45, 110.65s/it]=>> [Epoch 000] Global Step 000192 =>> LR :: 0.000020 - Loss :: 0.0000:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 192/200 [7:36:15<14:45, 110.65s/it]=>> [Epoch 000] Global Step 000192 =>> LR :: 0.000020 - Loss :: 0.0000:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 193/200 [7:39:55<16:43, 143.40s/it]=>> [Epoch 000] Global Step 000193 =>> LR :: 0.000020 - Loss :: 0.0000:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 193/200 [7:39:55<16:43, 143.40s/it]=>> [Epoch 000] Global Step 000193 =>> LR :: 0.000020 - Loss :: 0.0000:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 194/200 [7:40:51<11:41, 117.00s/it]=>> [Epoch 000] Global Step 000194 =>> LR :: 0.000020 - Loss :: 0.0000:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 194/200 [7:40:51<11:41, 117.00s/it]=>> [Epoch 000] Global Step 000194 =>> LR :: 0.000020 - Loss :: 0.0000:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 195/200 [7:41:57<08:29, 101.88s/it]=>> [Epoch 000] Global Step 000195 =>> LR :: 0.000020 - Loss :: 0.0000:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 195/200 [7:41:57<08:29, 101.88s/it]=>> [Epoch 000] Global Step 000195 =>> LR :: 0.000020 - Loss :: 0.0000:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 196/200 [7:46:55<10:43, 160.77s/it]=>> [Epoch 000] Global Step 000196 =>> LR :: 0.000020 - Loss :: 0.0000:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 196/200 [7:46:55<10:43, 160.77s/it]=>> [Epoch 000] Global Step 000196 =>> LR :: 0.000020 - Loss :: 0.0000:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 197/200 [7:47:48<06:25, 128.44s/it]=>> [Epoch 000] Global Step 000197 =>> LR :: 0.000020 - Loss :: 0.0000:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 197/200 [7:47:48<06:25, 128.44s/it]=>> [Epoch 000] Global Step 000197 =>> LR :: 0.000020 - Loss :: 0.0000:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 198/200 [7:51:23<05:08, 154.22s/it]=>> [Epoch 000] Global Step 000198 =>> LR :: 0.000020 - Loss :: 0.0000:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 198/200 [7:51:23<05:08, 154.22s/it]=>> [Epoch 000] Global Step 000198 =>> LR :: 0.000020 - Loss :: 0.0000: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 199/200 [7:52:14<02:03, 123.47s/it]=>> [Epoch 000] Global Step 000199 =>> LR :: 0.000020 - Loss :: 0.0000: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 199/200 [7:52:14<02:03, 123.47s/it]